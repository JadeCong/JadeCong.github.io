---
layout: page
title: Projects
description: >
  Archive list for awesome projects.
# logo:
# theme_color:
# accent_color:
accent_image:
  background: url('/assets/images/resources/sidebar-resources.jpg') center/cover
  overlay: true
# image:
#   path:
#   srcset:
#     1024w:
#     512w:
#     256w:

# permalink: /contents/resources/
# show_collection: resources
# selected_projects:
# projects_page:
# selected_posts:
# posts_page:
# related_posts:
# redirect_from:
# excerpt_separator:
last_modified_at: 2024-11-18

hide_description: true
hide_image: false
hide_last_modified: false
invert_sidebar: false
cover: false
no_groups: true
no_link_title: false
no_excerpt: false
no_third_column: true
sitemap: true
comments: true
featured: false
---

- Table of Contents
{:toc}

# Artificial Intelligence

## (1) LMs

> 1. **The Rhythm In Anything**: <https://oreillyp.github.io/tria/>
> 2. **DriveDreamer4D**: <https://drivedreamer4d.github.io/>
> 3. **TokenFormer**: <https://haiyang-w.github.io/tokenformer.github.io/>
> 4. **hertz-dev**: <https://si.inc/hertz-dev/>
> 5. **PointLLM**: <https://runsenxu.com/projects/PointLLM/>
> 6. **Aria**: <https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model>
> 7. **SiT**: <https://scalable-interpolant.github.io/>
> 8. **Rhymes AI**: <https://rhymes.ai/>
> 9. **nemotron-4-340b-instruct**: <https://build.nvidia.com/nvidia/nemotron-4-340b-instruct>
> 10. **EgoLM**: <https://hongfz16.github.io/projects/EgoLM>
> 11. **Small Language Models (SLM)**: <https://www.jetson-ai-lab.com/tutorial_slm.html>
> 12. **MiniMind**: <https://jingyaogong.github.io/minimind/>
> 13. **ell**: <https://docs.ell.so/#>
> 14. **CogVLM2-Video**: <https://cogvlm2-video.github.io/>
> 15. **InternVL**: <https://internvl.github.io/>
> 16. **Chroma**: <https://generatebiomedicines.com/chroma>
> 17. **GameNGen**: <https://gamengen.github.io/>
> 18. **Husky**: <https://agent-husky.github.io/>

## (2) Agents

> 1. **UFO**: <https://github.com/microsoft/UFO>
> 2. **ell**: <https://github.com/MadcowD/ell>
> 3. **SWE-agent**: <https://github.com/princeton-nlp/SWE-agent>
> 4. **autogen**: <https://github.com/microsoft/autogen>
> 5. **CharacterGen**: <https://github.com/zjp-shadow/CharacterGen>
> 6. **Qwen2.5-Coder**: <https://qwenlm.github.io/zh/blog/qwen2.5-coder/>
> 7. **Skyvern**: <https://www.skyvern.com/>
> 8. **Ichigo**: <https://github.com/homebrewltd/ichigo>
> 9. **AI for Grant Writing**: <https://www.lizseckel.com/ai-for-grant-writing/>
> 10. **Large Language Model Agents**: <https://llmagents-learning.org/f24>
> 11. **OpenAGI**: <https://openagi.aiplanet.com/>
> 12. **HyperWrite**: <https://www.hyperwriteai.com/>
> 13. **FastGPT**: <https://tryfastgpt.ai/>
> 14. **ADAS**: <https://www.shengranhu.com/ADAS/>
> 15. **AI Town**: <https://www.convex.dev/ai-town>
> 16. **Comflowy**: <https://www.comflowy.com/>
> 17. **Altera**: <https://altera.al/>
> 18. **Firecrawl**: <https://www.firecrawl.dev/>
> 19. **Artificial LIfe ENvironment (ALIEN)**: <https://www.alien-project.org/index.html>
> 20. **AutoGen Studio 2.0**: <https://autogen-studio.com/>
> 21. **SuperCraft**: <https://supercraft.ai/>

## (3) AIGC

> 1. **MikuDance**: <https://kebii.github.io/MikuDance/>
> 2. **DanceFusion**: <https://th-mlab.github.io/DanceFusion/>
> 3. **StdGEN**: <https://stdgen.github.io/>
> 4. **URAvatar**: <https://junxuan-li.github.io/urgca-website/>
> 5. **Towards High-fidelity Head Blending with Chroma Keying for Industrial Applications**: <https://hahminlew.github.io/changer/>
> 6. **ReCapture**: <https://generative-video-camera-controls.github.io/>
> 7. **DimensionX**: <https://chenshuo20.github.io/DimensionX/>
> 8. **Fashion-VDM**: <https://johannakarras.github.io/Fashion-VDM/>
> 9. **X-Portrait 2**: <https://byteaigc.github.io/X-Portrait2/>
> 10. **GameGen-X**: <https://gamegen-x.github.io/>
> 11. **HelloMeme**: <https://songkey.github.io/hellomeme/>
> 12. **DreamVideo-2**: <https://dreamvideo2.github.io/>
> 13. **VidPanos**: <https://vidpanos.github.io/>
> 14. **DAWN**: <https://hanbo-cheng.github.io/DAWN/>
> 15. **Interstice**: <https://www.interstice.cloud/>
> 16. **LongVU**: <https://vision-cair.github.io/LongVU/>
> 17. **DreamCraft3D++**: <https://dreamcraft3dplus.github.io/>
> 18. **Hallo2**: <https://fudan-generative-vision.github.io/hallo2/#/>
> 19. **MVideo**: <https://mvideo-v1.github.io/>
> 20. **UniMuMo**: <https://hanyangclarence.github.io/unimumo_demo/>
> 21. **TextToon**: <https://songluchuan.github.io/TextToon/>
> 22. **eye-contact-correction**: <https://www.sievedata.com/functions/sieve/eye-contact-correction>
> 23. **TANGO**: <https://pantomatrix.github.io/TANGO/>
> 24. **Animate-X**: <https://lucaria-academy.github.io/Animate-X/>
> 25. **ACE**: <https://ali-vilab.github.io/ace-page/>
> 26. **PhysGen**: <https://stevenlsw.github.io/physgen/>
> 27. **EdgeRunner**: <https://research.nvidia.com/labs/dir/edgerunner/>
> 28. **Movie Gen**: <https://ai.meta.com/research/movie-gen/>
> 29. **Inverse Painting**: <https://inversepainting.github.io/>
> 30. **Disco4D**: <https://disco-4d.github.io/>
> 31. **MimicTalk**: <https://mimictalk.github.io/>
> 32. **DIAMOND**: <https://diamond-wm.github.io/>
> 33. **JoyHallo**: <https://jdh-algo.github.io/JoyHallo/>
> 34. **SF3D**: <https://stable-fast-3d.github.io/>
> 35. **GaussianCube**: <https://gaussiancube.github.io/>
> 36. **Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing**: <https://pei-xu.github.io/guitar>
> 37. **SPARK**: <https://kelianb.github.io/SPARK/>
> 38. **LVCD**: <https://luckyhzt.github.io/lvcd>
> 39. **PortraitGen**: <https://ustc3dv.github.io/PortraitGen/>
> 40. **NeRF**: <https://www.matthewtancik.com/nerf>
> 41. **HIT**: <https://hit.is.tue.mpg.de/#video>
> 42. **3DTopia-XL**: <https://3dtopia.github.io/3DTopia-XL/>
> 43. **DrawingSpinUp**: <https://lordliang.github.io/DrawingSpinUp/>
> 44. **Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos**: <https://nowheretrix.github.io/DualGS/>
> 45. **PoseTalk**: <https://junleen.github.io/projects/posetalk/>
> 46. **GVHMR**: <https://zju3dv.github.io/gvhmr/>
> 47. **PersonaTalk**: <https://grisoon.github.io/PersonaTalk/>
> 48. **EscherNet**: <https://kxhit.github.io/EscherNet>
> 49. **Gaussian Garments**: <https://ribosome-rbx.github.io/Gaussian-Garments/>
> 50. **CyberHost**: <https://cyberhost.github.io/>
> 51. **Draw an Audio**: <https://yannqi.github.io/Draw-an-Audio/>
> 52. **3DGRT**: <https://gaussiantracer.github.io/>
> 53. **ViewCrafter**: <https://drexubery.github.io/ViewCrafter/>
> 54. **ReconX**: <https://liuff19.github.io/ReconX/>
> 55. **FaceSwap**: <https://faceswap.so/>
> 56. **Civitai**: <https://civitai.com/>
> 57. **Loopy**: <https://loopyavatar.github.io/>
> 58. **PhotoMaker**: <https://huggingface.co/spaces/TencentARC/PhotoMaker>
> 59. **InterTrack**: <https://virtualhumans.mpi-inf.mpg.de/InterTrack/>
> 60. **Build-A-Scene**: <https://abdo-eldesokey.github.io/build-a-scene/>
> 61. **MagicMan**: <https://thuhcsi.github.io/MagicMan/>
> 62. **LayerPano3D**: <https://ys-imtech.github.io/projects/LayerPano3D/>
> 63. **DreamCinema**: <https://liuff19.github.io/DreamCinema/>

# Robotics

## (1) Hardware

> 1. **Zeroth**: <https://docs.zeroth.bot/>
> 2. **Power-over-Skin**: <https://www.figlab.com/research/2024/poweroverskin>
> 3. **RoboDuet**: <https://locomanip-duet.github.io/>
> 4. **The snake that saves lives**: <https://ethz.ch/en/news-and-events/eth-news/news/2024/11/the-snake-that-saves-lives.html>
> 5. **XGO-Rider**: <https://www.kickstarter.com/projects/xgorobot/xgo-rider-desktop-two-wheel-legged-robot-with-ai>
> 6. **7X**: <https://7xr.tech/>

## (2) Software

> 1. **ROS2**: <https://docs.ros.org/en/jazzy/index.html>
> 2. **SAFER-Splat**: <https://chengine.github.io/safer-splat/>
> 3. **Neural MP**: <https://mihdalal.github.io/neuralmotionplanner/>

# AIRobotics

## (1) Robot Learning

> 1. **UMI(Universal Manipulation Interface)**: <https://umi-gripper.github.io/>
> 2. **PSAG**: <https://www.jianrenw.com/PSAG/>
> 3. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 4. **RGBManip**: <https://rgbmanip.github.io/>
> 5. **RoboStudio**: <https://robostudioapp.com/>
> 6. **ReKep**: <https://rekep-robot.github.io/>
> 7. **DeformGS**: <https://deformgs.github.io/>
> 8. **NeuralFeels**: <https://suddhu.github.io/neural-feels/>
> 9. **ALOHA**: <https://tonyzhaozh.github.io/aloha/>
> 10. **LucidSim**: <https://lucidsim.github.io/>
> 11. **RoPotter**: <https://robot-pottery.github.io/>
> 12. **HOVER**: <https://hover-versatile-humanoid.github.io/>
> 13. **DexMimicGen**: <https://dexmimicgen.github.io/>
> 14. **Eurekaverse**: <https://eureka-research.github.io/eurekaverse/>
> 15. **HIL-SERL**: <https://hil-serl.github.io/>
> 16. **OrbitGrasp**: <https://orbitgrasp.github.io/>
> 17. **3D-ViTac**: <https://binghao-huang.github.io/3D-ViTac/>
> 18. **Physical Intelligence**: <https://www.physicalintelligence.company/blog/pi0>
> 19. **HuDOR**: <https://object-rewards.github.io/>
> 20. **LAPA**: <https://latentactionpretraining.github.io/>
> 21. **ManipGen**: <https://mihdalal.github.io/manipgen/>
> 22. **Robots Pre-Train Robots**: <https://robots-pretrain-robots.github.io/>
> 23. **ARNOLD**: <https://arnold-benchmark.github.io/>
> 24. **GPT-4V(ision) for Robotics**: <https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/>
> 25. **VoxAct-B**: <https://voxact-b.github.io/>
> 26. **ARCap**: <https://stanford-tml.github.io/ARCap/>
> 27. **Harmon**: <https://ut-austin-rpl.github.io/Harmon/>
> 28. **Data Scaling Laws**: <https://data-scaling-laws.github.io/>
> 29. **Dynamic 3D Gaussian Tracking**: <https://gs-dynamics.github.io/>
> 30. **OKAMI**: <https://ut-austin-rpl.github.io/OKAMI/>
> 31. **UniHSI**: <https://xizaoqu.github.io/unihsi/>
> 32. **SDS**: <https://rpl-cs-ucl.github.io/SDSweb/>
> 33. **EgoAllo**: <https://egoallo.github.io/>
> 34. **DART**: <https://zkf1997.github.io/DART/>
> 35. **FürElise**: <https://for-elise.github.io/>
> 36. **PourIt**: <https://hetolin.github.io/PourIt/>
> 37. **Cherrybot**: <https://goodcherrybot.github.io/>
> 38. **AnyCar to Anywhere**: <https://lecar-lab.github.io/anycar/>
> 39. **Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies**: <https://lipschitz-constrained-policy.github.io/>
> 40. **HumanoidOlympics**: <https://humanoidolympics.github.io/>
> 41. **OmniH2O**: <https://omni.human2humanoid.com/>
> 42. **Continuously Improving Mobile Manipulation with Autonomous Real-World RL**: <https://continual-mobile-manip.github.io/>
> 43. **MotIF**: <https://motif-1k.github.io/>
> 44. **Helpful DoggyBot**: <https://helpful-doggybot.github.io/>
> 45. **Blox-Net**: <https://bloxnet.org/>
> 46. **GR-MG**: <https://gr-mg.github.io/>
> 47. **Real-World Cooking Robot System from Recipes**: <https://kanazawanaoaki.github.io/cook-from-recipe-pddl/>
> 48. **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**: <https://gr1-manipulation.github.io/>
> 49. **GR-2**: <https://gr2-manipulation.github.io/>
> 50. **CLoSD**: <https://guytevet.github.io/CLoSD-page/>
> 51. **RDT-1B**: <https://rdt-robotics.github.io/rdt-robotics/>
> 52. **Agile Continuous Jumping in Discontinuous Terrains**: <https://yxyang.github.io/jumping_cod/>
> 53. **Humanoid Manipulation**: <https://humanoid-manipulation.github.io/>
> 54. **Diff-Control**: <https://diff-control.github.io/>
> 55. **Catch It**: <https://mobile-dex-catch.github.io/>
> 56. **Robot See Robot Do**: <https://robot-see-robot-do.github.io/>
> 57. **Gen2Act**: <https://homangab.github.io/gen2act/>
> 58. **Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing**: <https://lecar-lab.github.io/dial-mpc/>
> 59. **ReMEmbR**: <https://nvidia-ai-iot.github.io/remembr/>
> 60. **ReMEmbR**: <https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/>
> 61. **MaskedMimic**: <https://research.nvidia.com/labs/par/maskedmimic/>
> 62. **Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation**: <https://human2humanoid.com/>
> 63. **Theia**: <https://theia.theaiinstitute.com/>
> 64. **Robot Motion Diffusion Model: Motion Generation for Robotic Characters**: <https://la.disneyresearch.com/publication/robot-motion-diffusion-model-motion-generation-for-robotic-characters/>
> 65. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 66. **One-shot Video Imitation via Parameterized Symbolic Abstraction Graphs**: <https://www.jianrenw.com/PSAG/>
> 67. **RGBManip**: <https://rgbmanip.github.io/>
> 68. **ALOHA Unleashed**: <https://aloha-unleashed.github.io/>
> 69. **PianoMime**: <https://pianomime.github.io/>
> 70. **Robot Utility Models**: <https://robotutilitymodels.com/#>
> 71. **Polaris**: <https://star-uu-wang.github.io/Polaris/>
> 72. **RoboStudio**: <https://robostudioapp.com/>
> 73. **ICRT**: <https://icrt.dev/>
> 74. **SkillMimic**: <https://ingrid789.github.io/SkillMimic/>
> 75. **VoicePilot**: <https://sites.google.com/andrew.cmu.edu/voicepilot/>
> 76. **ReKep**: <https://rekep-robot.github.io/>
> 77. **DeformGS**: <https://deformgs.github.io/>

## (2) Autonomous Driving

## (3) Embodied Intelligence

# Metaverse

## (1) Omniverse

## (2) Digital Twin

# Utilities

## (1) Computer Vision

> 1. **OpenCV**: <https://opencv.org/>
> 2. **roboflow**: <https://roboflow.com/>
> 3. **MoGe**: <https://wangrc.site/MoGePage/>
> 4. **Cloth-Splatting**: <https://kth-rpl.github.io/cloth-splatting/>
> 5. **SpectroMotion**: <https://cdfan0627.github.io/spectromotion/>
> 6. **SMITE**: <https://segment-me-in-time.github.io/>
> 7. **VistaDream**: <https://vistadream-project-page.github.io/index.html>
> 8. **InterMask**: <https://gohar-malik.github.io/intermask/>
> 9. **Dessie**: <https://celiali.github.io/Dessie/>
> 10. **CoTracker3**: <https://cotracker3.github.io/>
> 11. **PointCloud Conditioned Mesh Generation**: <https://research.nvidia.com/labs/dir/edgerunner/gallery/point_cond_4.html>
> 12. **Depth Any Video with Scalable Synthetic Data**: <https://depthanyvideo.github.io/>
> 13. **MonST3R**: <https://monst3r-project.github.io/>
> 14. **EVER**: <https://half-potato.gitlab.io/posts/ever/>
> 15. **CoTracker**: <https://co-tracker.github.io/>
> 16. **WiLoR**: <https://rolpotamias.github.io/WiLoR/>
> 17. **MIMO**: <https://menyifang.github.io/projects/MIMO/index.html>
> 18. **M2Mapping**: <https://jianhengliu.github.io/Projects/M2Mapping/>
> 19. **3D Gaussian Splatting for Real-Time Radiance Field Rendering**: <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>
> 20. **StableNormal**: <https://stable-x.github.io/StableNormal/>
> 21. **EmbodiedSAM**: <https://xuxw98.github.io/ESAM/>
> 22. **Large Étendue 3D Holographic Display with Content-adpative Dynamic Fourier Modulation**: <https://bchao1.github.io/holo_dfm/>
> 23. **Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos**: <https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/>
> 24. **EgoHDM**: <https://handiyin.github.io/EgoHDM/>
> 25. **DepthCrafter**: <https://depthcrafter.github.io/>
> 26. **Spann3R**: <https://hengyiwang.github.io/projects/spanner>
> 27. **OpenIns3D**: <https://zheninghuang.github.io/OpenIns3D/>

Continue reading [Papers](Papers.md){:.heading.flip-title}
{:.read-more}
