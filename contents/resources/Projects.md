---
layout: page
title: Projects
description: >
  Archive list for awesome projects.
# logo:
# theme_color:
# accent_color:
accent_image:
  background: url('/assets/images/resources/sidebar-resources.jpg') center/cover
  overlay: true
# image:
#   path:
#   srcset:
#     1024w:
#     512w:
#     256w:

# permalink: /contents/resources/
# show_collection: resources
# selected_projects:
# projects_page:
# selected_posts:
# posts_page:
# related_posts:
# redirect_from:
# excerpt_separator:
last_modified_at: 2026-02-27

hide_description: true
hide_image: false
hide_last_modified: false
invert_sidebar: false
cover: false
no_groups: true
no_link_title: false
no_excerpt: false
no_third_column: true
sitemap: true
comments: true
featured: false
---

- Table of Contents
{:toc}

# Artificial Intelligence

## (1) LMs

> 1. **The Rhythm In Anything**: <https://oreillyp.github.io/tria/>
> 2. **DriveDreamer4D**: <https://drivedreamer4d.github.io/>
> 3. **TokenFormer**: <https://haiyang-w.github.io/tokenformer.github.io/>
> 4. **hertz-dev**: <https://si.inc/hertz-dev/>
> 5. **PointLLM**: <https://runsenxu.com/projects/PointLLM/>
> 6. **Aria**: <https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model>
> 7. **SiT**: <https://scalable-interpolant.github.io/>
> 8. **Rhymes AI**: <https://rhymes.ai/>
> 9. **nemotron-4-340b-instruct**: <https://build.nvidia.com/nvidia/nemotron-4-340b-instruct>
> 10. **EgoLM**: <https://hongfz16.github.io/projects/EgoLM>
> 11. **Small Language Models (SLM)**: <https://www.jetson-ai-lab.com/tutorial_slm.html>
> 12. **MiniMind**: <https://jingyaogong.github.io/minimind/>
> 13. **ell**: <https://docs.ell.so/#>
> 14. **CogVLM2-Video**: <https://cogvlm2-video.github.io/>
> 15. **InternVL**: <https://internvl.github.io/>
> 16. **Chroma**: <https://generatebiomedicines.com/chroma>
> 17. **GameNGen**: <https://gamengen.github.io/>
> 18. **Husky**: <https://agent-husky.github.io/>
> 19. **Sapiens**: <https://about.meta.com/realitylabs/codecavatars/sapiens>
> 20. **MeshFormer**: <https://meshformer3d.github.io/>
> 21. **Genie**: <https://sites.google.com/view/genie-2024/home>
> 22. **Llama Tour**: <https://llamatutor.together.ai/>
> 23. **CogVideo**: <https://cogvideo.pka.moe/>
> 24. **Awesome ChatGPT Prompts**: <https://prompts.chat/>
> 25. **Opening up ChatGPT**: <https://opening-up-chatgpt.github.io/>
> 26. **AI Home Tab**: <https://aihometab.com/>
> 27. **Groq**: <https://groq.com/>
> 28. **NextChat**: <https://nextchat.dev/>
> 29. **MaxKB**: <https://maxkb.cn/>
> 30. **OpenDatalab**: <https://opendatalab.com/OpenSourceTools>
> 31. **Segment Anything Model 2 (SAM 2)**: <https://ai.meta.com/sam2/>
> 32. **LLaMA-Factory**: <https://llamafactory.readthedocs.io/zh-cn/latest/>
> 33. **FunAudioLLM**: <https://funaudiollm.github.io/>
> 34. **KLING**: <https://kling.kuaishou.com/>
> 35. **AlphaGeometry**: <https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/>
> 36. **Prompt Engineering Guide**: <https://www.promptingguide.ai/>
> 37. **AskManyAI**: <https://askmanyai.cn/login>
> 38. **Meet PathChat 2**: <https://www.modella.ai/intro.html>
> 39. **PaintsUndo**: <https://lllyasviel.github.io/pages/paints_undo/>
> 40. **Artificial Analysis**: <https://artificialanalysis.ai/>
> 41. **Transformer Explainer**: <https://poloclub.github.io/transformer-explainer/>
> 42. **Moshi**: <https://moshi.chat/?queue_id=talktomoshi>
> 43. **AI Graveyard**: <https://dang.ai/ai-graveyard>
> 44. **GraphRAG**: <https://microsoft.github.io/graphrag/>
> 45. **Luma Dream Machine**: <https://lumalabs.ai/dream-machine>
> 46. **Open-Sora**: <https://hpcaitech.github.io/Open-Sora/>
> 47. **Cambrian-1**: <https://cambrian-mllm.github.io/>
> 48. **PaLM-E**: <https://palm-e.github.io/>
> 49. **Meta Prompting for AI Systems**: <https://meta-prompting.github.io/>
> 50. **SITUATIONAL AWARENESS**: <https://situational-awareness.ai/>
> 51. **ChatTTS**: <https://chattts.com/>
> 52. **KAN**: <https://kindxiaoming.github.io/pykan/>
> 53. **LLM Visualization**: <https://bbycroft.net/llm>
> 54. **MLX**: <https://ml-explore.github.io/mlx/build/html/index.html>
> 55. **COSTAR Prompt Engineering**: <https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875>
> 56. **Cohere**: <https://cohere.com/>
> 57. **DeepSpeed**: <https://www.deepspeed.ai/>
> 58. **AI Mind**: <https://www.aimind.so/>
> 59. **flowith**: <https://flowith.net/blank>
> 60. **LeanDojo**: <https://leandojo.org/>
> 61. **GPT4All**: <https://www.nomic.ai/gpt4all>
> 62. **MiniGPT4-Video**: <https://vision-cair.github.io/MiniGPT4-video/>
> 63. **Mira**: <https://mira-space.github.io/>
> 64. **ModelScope**: <https://www.modelscope.cn/home>
> 65. **Osmo**: <https://www.osmo.ai/>
> 66. **Hume**: <https://www.hume.ai/>
> 67. **Reka**: <https://www.reka.ai/>
> 68. **Suno**: <https://suno.com/>
> 69. **Kimi**: <https://kimi.moonshot.cn/>
> 70. **SeamlessM4T**: <https://ai.meta.com/blog/seamless-m4t/>
> 71. **ElevenLabs**: <https://elevenlabs.io/>
> 72. **ChatLaw**: <https://chatlaw.cloud/>
> 73. **Gemma**: <https://ai.google.dev/gemma?hl=zh-cn>
> 74. **Stable Diffusion Art**: <https://stable-diffusion-art.com/comfyui/>
> 75. **OpenRouter**: <https://openrouter.ai/>
> 76. **ChatRTX**: <https://www.nvidia.com/en-us/ai-on-rtx/chatrtx/>
> 77. **Runway AI**: <https://runwayml.com/>
> 78. **Stable Video**: <https://www.stablevideo.com/welcome>
> 79. **Stability AI**: <https://stability.ai/>
> 80. **Multi-Agent Transformer**: <https://sites.google.com/view/multi-agent-transformer>
> 81. **DreamerV3**: <https://danijar.com/project/dreamerv3/>
> 82. **Imagen 2**: <https://deepmind.google/technologies/imagen-2/>
> 83. **Gemini Models**: <https://deepmind.google/technologies/gemini/#introduction>
> 84. **Anthropic AI**: <https://www.anthropic.com/>
> 85. **Grok**: <https://x.ai/>
> 86. **LLaVA**: <https://llava-vl.github.io/>
> 87. **Speaking AI**: <https://speaking.ai/>
> 88. **GPT-4 Is Too Smart To Be Safe**: <https://llmcipherchat.github.io/>
> 89. **Character AI**: <https://character.ai/>
> 90. **Whisper**: <https://openai.com/index/whisper/>
> 91. **Imagica AI**: <https://www.imagica.ai/>
> 92. **LangGPT**: <https://community.openai.com/t/langgpt-empowering-everyone-to-become-a-prompt-expert/207880>
> 93. **vLLM**: <https://docs.vllm.ai/en/latest/>
> 94. **DeepAI**: <https://deepai.org/>
> 95. **Midjourney**: <https://www.midjourney.com/home>
> 96. **Beautiful AI**: <https://www.beautiful.ai/>
> 97. **Qwen2.5-Turbo**: <https://qwen2.org/qwen2-5-turbo/>
> 98. **DINO-X**: <https://deepdataspace.com/home>
> 99. **ChatExcel**: <https://chatexcel.com/#/>
> 100. **World Labs**: <https://www.worldlabs.ai/blog>
> 101. **Prime Intellect**: <https://www.primeintellect.ai/>
> 102. **DeepSeek**: <https://www.deepseek.com/>
> 103. **HunyuanVideo**: <https://aivideo.hunyuan.tencent.com/>
> 104. **Navigation World Models**: <https://www.amirbar.net/nwm/>
> 105. **Polymathic**: <https://polymathic-ai.org/>
> 106. **Gemini 2.0**: <https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message>
> 107. **AlphaFold Server**: <https://alphafoldserver.com/welcome>
> 108. **Veo 2**: <https://deepmind.google/technologies/veo/veo-2/>
> 109. **Odyssey**: <https://odyssey.systems/introducing-explorer>
> 110. **Jina AI**: <https://jina.ai/reader/>
> 111. **Thinking in Space**: <https://vision-x-nyu.github.io/thinking-in-space.github.io/>
> 112. **Automating the Search for Artificial Life with Foundation Models**: <https://pub.sakana.ai/asal/>
> 113. **GPTsCopilot**: <https://chat.openai-now.com/zh>
> 114. **Unstract**: <https://unstract.com/>
> 115. **Decomposing Predictions by Modeling Model Computation**: <https://gradientscience.org/modelcomponents/>
> 116. **Hands-On Large Language Models**: <https://www.llm-book.com/>
> 117. **ZeroGPT**: <https://www.zerogpt.com/>
> 118. **SCENIC**: <https://virtualhumans.mpi-inf.mpg.de/scenic/>
> 119. **LobeChat**: <https://chat-preview.lobehub.com/chat>
> 120. **Farfalle**: <https://www.farfalle.dev/>
> 121. **TypingMind**: <https://www.typingmind.com/?ref=trackbes>
> 122. **LangGPT**: <https://langgptai.feishu.cn/wiki/RXdbwRyASiShtDky381ciwFEnpe>
> 123. **Sa2VA**: <https://lxtgh.github.io/project/sa2va/>
> 124. **Blackbox AI**: <https://www.blackbox.ai/>
> 125. **Sky-T1**: <https://novasky-ai.github.io/posts/sky-t1/>
> 126. **Instructor**: <https://useinstructor.com/>
> 127. **VILA**: <https://vila.mit.edu/>
> 128. **VideoLingo**: <https://videolingo.io/zh>
> 129. **Omni-RGPT**: <https://miranheo.github.io/omni-rgpt/>
> 130. **unwind ai**: <https://www.theunwindai.com/>
> 131. **ReaderLM v2**: <https://jina.ai/news/readerlm-v2-frontier-small-language-model-for-html-to-markdown-and-json/>
> 132. **cyanpuppets**: <https://cyanpuppets.com/>
> 133. **MatterGen**: <https://www.microsoft.com/en-us/research/articles/mattergen-a-generative-model-for-materials-design/>
> 134. **DrivingWorld**: <https://huxiaotaostasy.github.io/DrivingWorld/index.html>
> 135. **GAN Lab**: <https://poloclub.github.io/ganlab/>
> 136. **PoloClub**: <https://poloclub.github.io/>
> 137. **3D Shape Tokenization**: <https://machinelearning.apple.com/research/3d-shape-tokenization>
> 138. **Go-with-the-Flow**: <https://eyeline-research.github.io/Go-with-the-Flow/>
> 139. **unsloth**: <https://unsloth.ai/>
> 140. **Goku**: <https://saiyan-world.github.io/goku/>
> 141. **Inception**: <https://www.inceptionlabs.ai/>
> 142. **Magma**: <https://microsoft.github.io/Magma/>
> 143. **WorldModelBench**: <https://worldmodelbench-team.github.io/>
> 144. **OLMo Trace**: <https://allenai.org/>
> 145. **Model Context Protocol**: <https://www.anthropic.com/news/model-context-protocol>
> 146. **Awesome MCP Servers**: <https://mcpservers.org/>
> 147. **Glama**: <https://glama.ai/mcp/servers>
> 148. **MiniMax**:  <https://chat.minimaxi.com/>
> 149. **SpatialLM**: <https://manycore-research.github.io/SpatialLM/>
> 150. **Wanzhi**: <https://www.wanzhi.com/>
> 151. **Zapier MCP**: <https://zapier.com/mcp>
> 152. **MCP.so**: <https://mcp.so/zh>
> 153. **Qwen2.5 Omni**: <https://qwenlm.github.io/zh/blog/qwen2.5-omni/>
> 154. **Mureka**: <https://www.mureka.ai/home>
> 155. **Google AI Studio**: <https://aistudio.google.com/prompts/new_chat>
> 156. **MGX**: <https://mgx.dev/>
> 157. **Spark TTS**: <https://sparktts.io/zh>
> 158. **Llama Models**: <https://www.llama.com/docs/model-cards-and-prompt-formats/>
> 159. **WorldScore**: <https://haoyi-duan.github.io/WorldScore/>
> 160. **Agent2Agent Protocol**: <https://google.github.io/A2A/#/>
> 161. **Lyria**: <https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/>
> 162. **Magi-1**: <https://sand.ai/>
> 163. **InternVL**: <https://internvl.readthedocs.io/en/latest/>
> 164. **Niantic**: <https://www.nianticlabs.com/>
> 165. **Lemon Slice Live**: <https://lemonslice.com/live>
> 166. **OpenXLA**: <https://openxla.org/?hl=zh-cn>
> 167. **Veo**: <https://deepmind.google/models/veo/>
> 168. **Fast-dLLM**: <https://nvlabs.github.io/Fast-dLLM/>
> 169. **ThetaWave AI**: <https://thetawave.ai/>
> 170. **V-JEPA 2**: <https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/>
> 171. **AdaWorld**: <https://adaptable-world-model.github.io/>
> 172. **Alpaca**: <https://crfm.stanford.edu/2023/03/13/alpaca.html>
> 173. **Spatial-MLLM**: <https://diankun-wu.github.io/Spatial-MLLM/>
> 174. **Gemma 3n**: <https://deepmind.google/models/gemma/gemma-3n/>
> 175. **AlphaGenome**: <https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/>
> 176. **Happy-LLM**: <https://datawhalechina.github.io/happy-llm/#/>
> 177. **Chai-2**: <https://www.chaidiscovery.com/>
> 178. **Context7**: <https://context7.com/>
> 179. **MemOS**: <https://memos.openmem.net/>
> 180. **Meshcapade**: <https://meshcapade.com/>
> 181. **SceneScript**: <https://www.projectaria.com/scenescript/>
> 182. **NVIDIA TensorRT**: <https://developer.nvidia.com/tensorrt#>
> 183. **OpenAI Open Models**: <https://openai.com/zh-Hans-CN/open-models/>
> 184. **LMArena**: <https://lmarena.ai/>
> 185. **Genie 3**: <https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/>
> 186. **OLMo 2**: <https://allenai.org/olmo>
> 187. **DINOv3**: <https://ai.meta.com/dinov3/>
> 188. **Getting Good Results from Claude Code**: <https://www.dzombak.com/blog/2025/08/getting-good-results-from-claude-code/>
> 189. **VibeVoice**: <https://microsoft.github.io/VibeVoice/>
> 190. **verl**: <https://verl.readthedocs.io/en/latest/index.html>
> 191. **Open Source LLM Development Landscape**: <https://antoss-landscape.my.canva.site/>
> 192. **Unsloth**: <https://docs.unsloth.ai/>
> 193. **DiffusionNFT**: <https://research.nvidia.com/labs/dir/DiffusionNFT/>
> 194. **OpenRLHF**: <https://openrlhf.readthedocs.io/en/latest/>
> 195. **How To AI (Almost) Anything**: <https://mit-mi.github.io/how2ai-course/spring2025/>
> 196. **HUMAN 3.0**: <https://thedankoe.com/letters/human-3-0-a-map-to-reach-the-top-1/>
> 197. **Nof1**: <https://nof1.ai/>
> 198. **Google Skills**: <https://www.skills.google/>
> 199. **AirScape**: <https://embodiedcity.github.io/AirScape/>
> 200. **Segment Anything Model 3 (SAM 3)**: <https://ai.meta.com/sam3/>
> 201. **GLM**: <https://www.zhipuai.cn/zh>

## (2) Agents

> 1. **UFO**: <https://github.com/microsoft/UFO>
> 2. **ell**: <https://github.com/MadcowD/ell>
> 3. **SWE-agent**: <https://github.com/princeton-nlp/SWE-agent>
> 4. **autogen**: <https://github.com/microsoft/autogen>
> 5. **CharacterGen**: <https://github.com/zjp-shadow/CharacterGen>
> 6. **Qwen2.5-Coder**: <https://qwenlm.github.io/zh/blog/qwen2.5-coder/>
> 7. **Skyvern**: <https://www.skyvern.com/>
> 8. **Ichigo**: <https://github.com/homebrewltd/ichigo>
> 9. **AI for Grant Writing**: <https://www.lizseckel.com/ai-for-grant-writing/>
> 10. **Large Language Model Agents**: <https://llmagents-learning.org/f24>
> 11. **OpenAGI**: <https://openagi.aiplanet.com/>
> 12. **HyperWrite**: <https://www.hyperwriteai.com/>
> 13. **FastGPT**: <https://tryfastgpt.ai/>
> 14. **ADAS**: <https://www.shengranhu.com/ADAS/>
> 15. **AI Town**: <https://www.convex.dev/ai-town>
> 16. **Comflowy**: <https://www.comflowy.com/>
> 17. **Altera**: <https://altera.al/>
> 18. **Firecrawl**: <https://www.firecrawl.dev/>
> 19. **Artificial LIfe ENvironment (ALIEN)**: <https://www.alien-project.org/index.html>
> 20. **AutoGen Studio 2.0**: <https://autogen-studio.com/>
> 21. **SuperCraft**: <https://supercraft.ai/>
> 22. **Pipecat**: <https://www.pipecat.ai/>
> 23. **Neo4j**: <https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/>
> 24. **Lumina**: <https://www.lumina.sh/c5bbe32b-4fb7-476a-81aa-fe269f67f283?ref=www.lumina-chat.com>
> 25. **RAGFlow**: <https://ragflow.io/>
> 26. **OmniParse**: <https://docs.cognitivelab.in/>
> 27. **Supermemory**: <https://supermemory.ai/>
> 28. **MindSearch**: <https://mindsearch.netlify.app/>
> 29. **ChatDev**: <https://chatdev.toscl.com/>
> 30. **LlamaCoder**: <https://llamacoder.together.ai/>
> 31. **GPTs Works**: <https://gpts.works/>
> 32. **V2A-Mapper**: <https://v2a-mapper.github.io/>
> 33. **MultiOn**: <https://www.multion.ai/>
> 34. **ThinkAny**: <https://thinkany.ai/zh>
> 35. **Mem0**: <https://docs.mem0.ai/overview>
> 36. **Cradle**: <https://baai-agents.github.io/Cradle/>
> 37. **Devv**: <https://devv.ai/zh>
> 38. **Co-STORM**: <https://storm.genie.stanford.edu/>
> 39. **Bubble**: <https://bubble.io/>
> 40. **Humanize AI text**: <https://www.humanizeai.pro/>
> 41. **Aider**: <https://aider.chat/>
> 42. **Agently AI**: <https://agently.tech/>
> 43. **DeepSeek Coder**: <https://deepseekcoder.github.io/>
> 44. **AgentScope**: <https://doc.agentscope.io/en/index.html>
> 45. **CrewAI**: <https://docs.crewai.com/introduction>
> 46. **Humaan AI**: <https://humaan.ai/>
> 47. **FlowiseAI**: <https://flowiseai.com/>
> 48. **Chainlit**: <https://docs.chainlit.io/get-started/overview>
> 49. **Phidata**: <https://docs.phidata.com/agents>
> 50. **Lepton AI**: <https://www.lepton.ai/>
> 51. **AutoGPT**: <https://agpt.co/>
> 52. **MetaGPT**: <https://www.deepwisdom.ai/>
> 53. **LangGraph**: <https://langchain-ai.github.io/langgraph/>
> 54. **HyperWrite**: <https://www.hyperwriteai.com/>
> 55. **QAnything**: <https://qanything.ai/>
> 56. **Synthflow AI Voice Assistants**: <https://synthflow.ai/>
> 57. **Tavily**: <https://tavily.com/>
> 58. **Dify**: <https://dify.ai/>
> 59. **LangChain**: <https://www.langchain.com/>
> 60. **LlamaIndex**: <https://www.llamaindex.ai/>
> 61. **SWE-agent**: <https://swe-agent.com/>
> 62. **MemGPT**: <https://memgpt.ai/>
> 63. **SIMA**: <https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/>
> 64. **Durable**: <https://durable.co/>
> 65. **Cognition**: <https://www.cognition.ai/>
> 66. **LTX Studio**: <https://ltx.studio/>
> 67. **vellum**: <https://www.vellum.ai/>
> 68. **DetectGPT**: <https://detectgpt.ai/index.html>
> 69. **Dora AI**: <https://www.dora.run/ai>
> 70. **An Embodied Generalist Agent in 3D World**: <https://embodied-generalist.github.io/>
> 71. **Voyager**: <https://voyager.minedojo.org/>
> 72. **XAgent**: <https://xagent-doc.readthedocs.io/en/latest/>
> 73. **SuperAGI**: <https://superagi.com/>
> 74. **ima.copilot**: <https://ima.qq.com/>
> 75. **Supermaven**: <https://supermaven.com/>
> 76. **Accio**: <https://www.accio.com/>
> 77. **excalidraw**: <https://excalidraw.com/>
> 78. **Tencent Yuanqi**: <https://yuanqi.tencent.com/agent-shop>
> 79. **PicMenu**: <https://www.picmenu.co/>
> 80. **AgentBench**: <https://llmbench.ai/agent>
> 81. **SafetyBench**: <https://llmbench.ai/safety>
> 82. **AlignBench**: <https://llmbench.ai/align>
> 83. **AutoGLM**: <https://xiao9905.github.io/AutoGLM/>
> 84. **OmniParser**: <https://microsoft.github.io/OmniParser/>
> 85. **ResearchFlow**: <https://rflow.ai/>
> 86. **Semantic Scholar**: <https://www.semanticscholar.org/>
> 87. **Ant Design X**: <https://x.ant.design/index-cn>
> 88. **HAKE**: <http://hake-mvig.cn/home/>
> 89. **AIOS Foundation**: <https://aios.foundation/>
> 90. **CosmOS**: <https://humane.com/cosmos>
> 91. **letta**: <https://www.letta.com/>
> 92. **aisuite**: <https://github.com/andrewyng/aisuite>
> 93. **FlyFlow**: <https://www.flyflow.cc/>
> 94. **Building effective agents**: <https://www.anthropic.com/research/building-effective-agents>
> 95. **Automa**: <https://www.automa.site/>
> 96. **AG2**: <https://ag2.ai/>
> 97. **Running Llama on Windows 98**: <https://blog.exolabs.net/day-4/>
> 98. **n8n**: <https://n8n.io/>
> 99. **Project IDX**: <https://idx.dev/>
> 100. **Diagramming AI**: <https://diagrammingai.com/>
> 101. **Windsurf**: <https://windsurfai.org/zh>
> 102. **OpenSPG**: <https://spg.openkg.cn/>
> 103. **WrenAI**: <https://getwren.ai/oss>
> 104. **DB-GPT**: <http://docs.dbgpt.cn/docs/overview>
> 105. **Magentic-One**: <https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/>
> 106. **Micro Agent**: <https://www.builder.io/blog/micro-agent>
> 107. **Browserbase**: <https://www.browserbase.com/>
> 108. **Browser Use**: <https://browser-use.com/>
> 109. **TXYZ**: <https://app.txyz.ai/library>
> 110. **Agent Laboratory**: <https://agentlaboratory.github.io/>
> 111. **WhisperTyping**: <https://whispertyping.com/>
> 112. **Whisper Keyboard**: <https://wkd.market/>
> 113. **Eko**: <https://eko.fellou.ai/>
> 114. **ResearchFlow**: <https://rflow.ai/zh>
> 115. **PDF Guru Anki**: <https://guru.kevin2li.top/>
> 116. **RuleOS**: <https://ruleos.com/console/home>
> 117. **AI Agent**: <https://www.cnblogs.com/lusuo/p/18663007>
> 118. **Appwrite**: <https://appwrite.io/>
> 119. **Litmaps**: <https://www.litmaps.com/>
> 120. **PaSa**: <https://pasa-agent.ai/>
> 121. **TinyEngine**: <https://opentiny.design/tiny-engine#/home>
> 122. **AppAgent**: <https://appagent-official.github.io/>
> 123. **PC Agent**: <https://gair-nlp.github.io/PC-Agent/>
> 124. **ChatPaper**: <https://chatwithpaper.org/>
> 125. **DesktopGPT**: <https://desktopgpt.hix.ai/>
> 126. **Scholarcy**: <https://www.scholarcy.com/>
> 127. **Open Interpreter**: <https://www.openinterpreter.com/>
> 128. **Lumina**: <https://www.lumina.sh/c5bbe32b-4fb7-476a-81aa-fe269f67f283>
> 129. **Humata**: <https://app.humata.ai/login>
> 130. **Softgen**: <https://softgen.ai/>
> 131. **Talo**: <https://www.taloai.com/zh>
> 132. **Cofounder**: <https://cofounder.openinterface.ai/>
> 133. **Potpie AI**: <https://potpie.ai/>
> 134. **UndatasIO**: <https://undatas.io/>
> 135. **Sakana AI**: <https://sakana.ai/ai-cuda-engineer/>
> 136. **Docmost**: <https://docmost.com/>
> 137. **Convergence**: <https://convergence.ai/welcome>
> 138. **PyGPT**: <https://pygpt.net/>
> 139. **PySpur**: <https://www.pyspur.com/>
> 140. **Mobile-Agent-E**: <https://x-plug.github.io/MobileAgent/>
> 141. **OpenHands**: <https://www.all-hands.dev/>
> 142. **OctoTools**: <https://octotools.github.io/>
> 143. **Power Automate**: <https://www.microsoft.com/en-us/power-platform/products/power-automate>
> 144. **PySpur**: <https://www.pyspur.dev/>
> 145. **Manus**: <https://manus.monica.cn/>
> 146. **Glean**: <https://www.glean.com/>
> 147. **AppAgentX**: <https://appagentx.github.io/>
> 148. **LangBot**: <https://docs.langbot.app/insight/guide>
> 149. **TheySaid**: <https://www.theysaid.io/>
> 150. **Relume**: <https://www.relume.io/>
> 151. **ChatTTS**: <https://www.chattts.co/zh>
> 152. **Flourish**: <https://flourish.studio/>
> 153. **CopyWeb**: <https://copyweb.ai/>
> 154. ***AFFiNE**: <https://affine.pro/>
> 155. **Agent TARS**: <https://agent-tars.com/>
> 156. **TalkMe**: <https://www.talkme.ai/>
> 157. **Anysphere**: <https://anysphere.inc/>
> 158. **screenpipe**: <https://screenpi.pe/>
> 159. **Genspark**: <https://www.genspark.ai/>
> 160. **Agno**: <https://www.agno.com/>
> 161. **Khoj AI**: <https://khoj.dev/>
> 162. **Crawl4AI**: <https://docs.crawl4ai.com/>
> 163. **Drawatoon**: <https://drawatoon.com/>
> 164. **Cline**: <https://cline.bot/>
> 165. **Roo Code**: <https://roocode.com/>
> 166. **PandaAI**: <https://getpanda.ai/>
> 167. **Agent Development Kit**: <https://google.github.io/adk-docs/>
> 168. **Industrial Copilots**: <https://www.siemens.com/global/en/products/automation/topic-areas/industrial-ai/industrial-copilot.html#DiscoverourIndustrialCopilots>
> 169. **meetily**: <https://meetily.zackriya.com/>
> 170. **Liam ERD**: <https://liambx.com/>
> 171. **Droidrun**: <https://www.droidrun.ai/>
> 172. **AingDesk**: <https://www.aingdesk.com/>
> 173. **MindsDB**: <https://mindsdb.com/>
> 174. **Vanna.AI**: <https://vanna.ai/>
> 175. **Outrank**: <https://www.outrank.so/>
> 176. **Maxun**: <https://www.maxun.dev/>
> 177. **Simular AI**: <https://www.simular.ai/>
> 178. **FunASR**: <https://www.funasr.com/#/>
> 179. **Graphiti**: <https://help.getzep.com/graphiti/graphiti/overview>
> 180. **Suna**: <https://www.suna.so/>
> 181. **Chat2DB**: <https://chat2db-ai.com/>
> 182. **OpenBB**: <https://openbb.co/>
> 183. **SigLens**: <https://www.siglens.com/>
> 184. **Readdy**: <https://readdy.ai/>
> 185. **LegoGPT**: <https://avalovelace1.github.io/LegoGPT/>
> 186. **DeerFlow**: <https://deerflow.tech/>
> 187. **OpenMemory MCP**: <https://mem0.ai/openmemory-mcp>
> 188. **Camera Interaction App**: <https://github.ngxson.com/smolvlm-realtime-webcam/>
> 189. **CodeRabbit**: <https://www.coderabbit.ai/>
> 190. **AlphaEvolve**: <https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/>
> 191. **AG-UI Protocol**: <https://docs.ag-ui.com/introduction>
> 192. **ListenHub**: <https://listenhub.ai/zh>
> 193. **Comet**: <https://www.perplexity.ai/comet>
> 194. **Microsoft Copilot Studio**: <https://www.microsoft.com/zh-cn/microsoft-copilot/microsoft-copilot-studio>
> 195. **WildDoc**: <https://bytedance.github.io/WildDoc/>
> 196. **Emergent Vibe Coding**: <https://app.emergent.sh/>
> 197. **Stitch**: <https://stitch.withgoogle.com/?pli=1>
> 198. **VideoTutor**: <https://videotutor.io/>
> 199. **hyperbrowser**: <https://www.hyperbrowser.ai/>
> 200. **Auto-MCP**: <https://auto-mcp.com/>
> 201. **Parakeet TDT**: <https://parakeettdt.com/>
> 202. **Onlook**: <https://onlook.com/>
> 203. **AgenticSeek**: <https://fosowl.github.io/agenticSeek.html>
> 204. **Void**: <https://voideditor.com/>
> 205. **Cool-Admin**: <https://admin.cool-js.com/>
> 206. **Pocket Flow**: <https://the-pocket.github.io/PocketFlow/>
> 207. **Gemini CLI**: <https://gemini-cli.dev/>
> 208. **OpenManus**: <https://openmanus.github.io/>
> 209. **Strands Agents**: <https://strandsagents.com/latest/>
> 210. **Augment Code**: <https://www.augmentcode.com/>
> 211. **Claude Code**: <https://www.claudecode.io/>
> 212. **NoteGen**: <https://notegen.top/zh/>
> 213. **Skywork**: <https://skywork.ai>
> 214. **Opencode**: <https://opencode.ai/>
> 215. **AmpCode**: <https://ampcode.com/>
> 216. **openai-codex**: <https://openai.com/index/openai-codex/>
> 217. **Magic Agent**: <https://21st.dev/magic>
> 218. **Memos**: <https://www.usememos.com/>
> 219. **PandaWiki**: <https://pandawiki.docs.baizhi.cloud/welcome>
> 220. **Stagewise**: <https://stagewise.io/>
> 221. **Zread**: <https://zread.ai/>
> 222. **MiroMind**: <https://miromind.ai/>
> 223. **Qoder**: <https://qoder.com/>
> 224. **Sim**: <https://www.sim.ai/>
> 225. **MCP**: <https://modelcontextprotocol.io/docs/getting-started/intro>
> 226. **Project AIRI**: <https://airi.moeru.ai/docs/zh-Hans/>
> 227. **bytebot**: <https://www.bytebot.ai/>
> 228. **Hyprnote**: <https://hyprnote.com/>
> 229. **Visual Story-Writing**: <https://damienmasson.com/VisualStoryWriting/>
> 230. **IndexTTS 2**: <https://indextts.org/zh>
> 231. **DeepMCPAgent**: <https://cryxnet.github.io/DeepMCPAgent/>
> 232. **Zed**: <https://zed.dev/>
> 233. **JeecgBoot**: <https://www.jeecg.com/>
> 234. **FastbuildAI**: <https://www.fastbuildai.com/>
> 235. **TEN Agent**: <https://agent.theten.ai/>
> 236. **ROMA**: <https://blog.sentient.xyz/posts/recursive-open-meta-agent>
> 237. **Super Magic**: <https://www.letsmagic.ai/>
> 238. **Agent Zero**: <https://agent-zero.ai/#hero>
> 239. **Dreamer 4**: <https://danijar.com/project/dreamer4/>
> 240. **Jaaz**: <https://jaaz.app/>
> 241. **Haystack**: <https://haystack.deepset.ai/>
> 242. **Cherry Studio**: <https://www.cherry-ai.com/>
> 243. **Supabase**: <https://supabase.com/>
> 244. **Warp**: <https://www.warp.dev/>
> 245. **ChopperBot**: <https://geniusay.github.io/ChopperBot-Doc/>
> 246. **Paper2Video**: <https://showlab.github.io/Paper2Video/>
> 247. **Kirara AI**: <https://kirara-docs.app.lss233.com/>
> 248. **iFlow**: <https://iflow.cn/>
> 249. **Color Fuse AI**: <https://colorfuseai.com/>
> 250. **Openspec**: <https://openspec.dev/>
> 251. **fast-agent**: <https://fast-agent.ai/>
> 252. **MCP Playground**: <https://mcpsplayground.com/chat>
> 253. **Claude Agent Skills**: <https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview>
> 254. **Gambo**: <https://www.gambo.ai/>
> 255. **CodeWiki**: <https://codewiki.ai/>
> 256. **OmniBox**: <https://www.omnibox.pro/zh-cn/>
> 257. **Atlas Research**: <https://atlas-research.io/>
> 258. **Curiosity**: <https://curiosity.ai/>
> 259. **GELab-Zero**: <https://opengelab.github.io/>
> 260. **Agentic AI**: <https://www.deeplearning.ai/courses/agentic-ai/>
> 261. **Google Labs**: <https://labs.google/>
> 262. **Ralv**: <https://ralv.ai/>
> 263. **Moltbot**: <https://clawd.bot/>
> 264. **OpenCode**: <https://opencode.ai/zh>
> 265. **OpenWork**: <https://openwork.software/>
> 266. **WebMCP**: <https://webmachinelearning.github.io/webmcp/>
> 267. **NanoClaw**: <https://nanoclaw.dev/>
> 268. **ZeroClaw**: <https://zeroclawlabs.ai/>
> 269. **PicoClaw**: <https://picoclaw.io/>
> 270. **IronClaw**: <https://www.ironclaw.com/>

## (3) AIGC

> 1. **MikuDance**: <https://kebii.github.io/MikuDance/>
> 2. **DanceFusion**: <https://th-mlab.github.io/DanceFusion/>
> 3. **StdGEN**: <https://stdgen.github.io/>
> 4. **URAvatar**: <https://junxuan-li.github.io/urgca-website/>
> 5. **Towards High-fidelity Head Blending with Chroma Keying for Industrial Applications**: <https://hahminlew.github.io/changer/>
> 6. **ReCapture**: <https://generative-video-camera-controls.github.io/>
> 7. **DimensionX**: <https://chenshuo20.github.io/DimensionX/>
> 8. **Fashion-VDM**: <https://johannakarras.github.io/Fashion-VDM/>
> 9. **X-Portrait 2**: <https://byteaigc.github.io/X-Portrait2/>
> 10. **GameGen-X**: <https://gamegen-x.github.io/>
> 11. **HelloMeme**: <https://songkey.github.io/hellomeme/>
> 12. **DreamVideo-2**: <https://dreamvideo2.github.io/>
> 13. **VidPanos**: <https://vidpanos.github.io/>
> 14. **DAWN**: <https://hanbo-cheng.github.io/DAWN/>
> 15. **Interstice**: <https://www.interstice.cloud/>
> 16. **LongVU**: <https://vision-cair.github.io/LongVU/>
> 17. **DreamCraft3D++**: <https://dreamcraft3dplus.github.io/>
> 18. **Hallo2**: <https://fudan-generative-vision.github.io/hallo2/#/>
> 19. **MVideo**: <https://mvideo-v1.github.io/>
> 20. **UniMuMo**: <https://hanyangclarence.github.io/unimumo_demo/>
> 21. **TextToon**: <https://songluchuan.github.io/TextToon/>
> 22. **eye-contact-correction**: <https://www.sievedata.com/functions/sieve/eye-contact-correction>
> 23. **TANGO**: <https://pantomatrix.github.io/TANGO/>
> 24. **Animate-X**: <https://lucaria-academy.github.io/Animate-X/>
> 25. **ACE**: <https://ali-vilab.github.io/ace-page/>
> 26. **PhysGen**: <https://stevenlsw.github.io/physgen/>
> 27. **EdgeRunner**: <https://research.nvidia.com/labs/dir/edgerunner/>
> 28. **Movie Gen**: <https://ai.meta.com/research/movie-gen/>
> 29. **Inverse Painting**: <https://inversepainting.github.io/>
> 30. **Disco4D**: <https://disco-4d.github.io/>
> 31. **MimicTalk**: <https://mimictalk.github.io/>
> 32. **DIAMOND**: <https://diamond-wm.github.io/>
> 33. **JoyHallo**: <https://jdh-algo.github.io/JoyHallo/>
> 34. **SF3D**: <https://stable-fast-3d.github.io/>
> 35. **GaussianCube**: <https://gaussiancube.github.io/>
> 36. **Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing**: <https://pei-xu.github.io/guitar>
> 37. **SPARK**: <https://kelianb.github.io/SPARK/>
> 38. **LVCD**: <https://luckyhzt.github.io/lvcd>
> 39. **PortraitGen**: <https://ustc3dv.github.io/PortraitGen/>
> 40. **NeRF**: <https://www.matthewtancik.com/nerf>
> 41. **HIT**: <https://hit.is.tue.mpg.de/#video>
> 42. **3DTopia-XL**: <https://3dtopia.github.io/3DTopia-XL/>
> 43. **DrawingSpinUp**: <https://lordliang.github.io/DrawingSpinUp/>
> 44. **Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos**: <https://nowheretrix.github.io/DualGS/>
> 45. **PoseTalk**: <https://junleen.github.io/projects/posetalk/>
> 46. **GVHMR**: <https://zju3dv.github.io/gvhmr/>
> 47. **PersonaTalk**: <https://grisoon.github.io/PersonaTalk/>
> 48. **EscherNet**: <https://kxhit.github.io/EscherNet>
> 49. **Gaussian Garments**: <https://ribosome-rbx.github.io/Gaussian-Garments/>
> 50. **CyberHost**: <https://cyberhost.github.io/>
> 51. **Draw an Audio**: <https://yannqi.github.io/Draw-an-Audio/>
> 52. **3DGRT**: <https://gaussiantracer.github.io/>
> 53. **ViewCrafter**: <https://drexubery.github.io/ViewCrafter/>
> 54. **ReconX**: <https://liuff19.github.io/ReconX/>
> 55. **FaceSwap**: <https://faceswap.so/>
> 56. **Civitai**: <https://civitai.com/>
> 57. **Loopy**: <https://loopyavatar.github.io/>
> 58. **PhotoMaker**: <https://huggingface.co/spaces/TencentARC/PhotoMaker>
> 59. **InterTrack**: <https://virtualhumans.mpi-inf.mpg.de/InterTrack/>
> 60. **Build-A-Scene**: <https://abdo-eldesokey.github.io/build-a-scene/>
> 61. **MagicMan**: <https://thuhcsi.github.io/MagicMan/>
> 62. **LayerPano3D**: <https://ys-imtech.github.io/projects/LayerPano3D/>
> 63. **DreamCinema**: <https://liuff19.github.io/DreamCinema/>
> 64. **TurboEdit**: <https://betterze.github.io/TurboEdit/>
> 65. **Scaling Up Dynamic Human-Scene Interaction Modeling**: <https://jnnan.github.io/trumans/>
> 66. **DiPIR**: <https://research.nvidia.com/labs/toronto-ai/DiPIR/>
> 67. **TurboEdit**: <https://turboedit-paper.github.io/>
> 68. **DEGAS**: <https://initialneil.github.io/DEGAS>
> 69. **Audio Match Cutting**: <https://denfed.github.io/audiomatchcut/>
> 70. **Subsurface Scattering for Gaussian Splatting**: <https://sss.jdihlmann.com/>
> 71. **Tavus**: <https://www.tavus.io/>
> 72. **Media2Face**: <https://sites.google.com/view/media2face>
> 73. **FruitNeRF**: <https://meyerls.github.io/fruit_nerf/>
> 74. **Puppet-Master**: <https://vgg-puppetmaster.github.io/>
> 75. **ernerf**: <https://zhuanlan.zhihu.com/p/675131165>
> 76. **An Object is Worth 64x64 Pixels**: <https://omages.github.io/>
> 77. **VideoDoodles**: <https://em-yu.github.io/research/videodoodles/>
> 78. **ReSyncer**: <https://guanjz20.github.io/projects/ReSyncer/>
> 79. **KEEP**: <https://jnjaby.github.io/projects/KEEP/>
> 80. **MoMask**: <https://ericguo5513.github.io/momask/>
> 81. **Tora**: <https://ali-videoai.github.io/tora_video/>
> 82. **EmoTalk3D**: <https://nju-3dv.github.io/projects/EmoTalk3D/>
> 83. **Cycle3D**: <https://pku-yuangroup.github.io/Cycle3D/>
> 84. **Swapface**: <https://www.swapface.org/#/home>
> 85. **ExAvatar**: <https://mks0601.github.io/ExAvatar/>
> 86. **MotionClone**: <https://bujiazi.github.io/motionclone.github.io/>
> 87. **Outfit Anyone**: <https://humanaigc.github.io/outfit-anyone/>
> 88. **Vidu**: <https://www.vidu.studio/zh>
> 89. **Temporal Residual Jacobians for Rig-free Motion Transfer**: <https://temporaljacobians.github.io/>
> 90. **Cinemo**: <https://maxin-cn.github.io/cinemo_project/>
> 91. **HumanVid**: <https://humanvid.github.io/>
> 92. **Diffree**: <https://opengvlab.github.io/Diffree/>
> 93. **SMooDi**: <https://neu-vi.github.io/SMooDi/>
> 94. **Lite2Relight**: <https://vcai.mpi-inf.mpg.de/projects/Lite2Relight/>
> 95. ***Noise Calibration**: <https://yangqy1110.github.io/NC-SDEdit/>
> 96. **Diff-Foley**: <https://diff-foley.github.io/>
> 97. **Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity**: <https://maskvat.github.io/>
> 98. **vozo**: <https://www.vozo.ai/>
> 99. **MoA**: <https://snap-research.github.io/mixture-of-attention/>
> 100. **Magic Insert**: <https://magicinsert.github.io/>
> 101. **CharacterGen**: <https://charactergen.github.io/>
> 102. **Live2Diff**: <https://live2diff.github.io/>
> 103. **RodinHD**: <https://rodinhd.github.io/>
> 104. **StickerBaker**: <https://stickerbaker.com/>
> 105. **Still-Moving**: <https://still-moving.github.io/>
> 106. **Tripo3D**: <https://www.tripo3d.ai/>
> 107. **Hedra**: <https://www.hedra.com/>
> 108. **RenderNet**: <https://rendernet.ai/index.html>
> 109. **LivePortrait**: <https://liveportrait.github.io/>
> 110. **Image Conductor**: <https://liyaowei-stu.github.io/project/ImageConductor/>
> 111. **MOTIA**: <https://be-your-outpainter.github.io/>
> 112. **Meta 3D AssetGen**: <https://assetgen.github.io/>
> 113. **Portrait3D**: <https://jinkun-hao.github.io/Portrait3D/>
> 114. **GaussianDreamerPro**: <https://taoranyi.com/gaussiandreamerpro/>
> 115. **MimicMotion**: <https://tencent.github.io/MimicMotion/>
> 116. **Text-Animator**: <https://laulampaul.github.io/text-animator.html>
> 117. **YouDream**: <https://youdream3d.github.io/>
> 118. **FoleyCrafter**: <https://foleycrafter.github.io/>
> 119. **Wonder Studio**: <https://wonderdynamics.com/>
> 120. **TripoSR**: <https://stability.ai/news/triposr-3d-generation>
> 121. **MeshAnything**: <https://buaacyw.github.io/mesh-anything/>
> 122. **EvTexture**: <https://dachunkai.github.io/evtexture.github.io/>
> 123. **ScoreHypo**: <https://xy02-05.github.io/ScoreHypo/>
> 124. **AniFusion**: <https://anifusion.ai/>
> 125. **Style-NeRF2NeRF**: <https://haruolabs.github.io/style-n2n/>
> 126. **4K4DGen**: <https://4k4dgen.github.io/>
> 127. **ExVideo**: <https://ecnu-cilab.github.io/ExVideoProjectPage/>
> 128. **Diffutoon**: <https://ecnu-cilab.github.io/DiffutoonProjectPage/>
> 129. **Holistic-Motion2D**: <https://holistic-motion2d.github.io/>
> 130. **FaceFusion**: <https://docs.facefusion.io/>
> 131. **AnyFit**: <https://colorful-liyu.github.io/anyfit-page/>
> 132. **PuzzleFusion++**: <https://puzzlefusion-plusplus.github.io/>
> 133. **UniAnimate**: <https://unianimate.github.io/>
> 134. **ChronoDepth**: <https://jhaoshao.github.io/ChronoDepth/>
> 135. **Unique3D**: <https://wukailu.github.io/Unique3D/>
> 136. **GECO**: <https://cwchenwang.github.io/geco/>
> 137. **T2V-Turbo**: <https://t2v-turbo.github.io/>
> 138. **EasyAnimate**: <https://easyanimate.github.io/>
> 139. **ZeroSmooth**: <https://ssyang2020.github.io/zerosmooth.github.io/>
> 140. **MVSGaussian**: <https://mvsgaussian.github.io/>
> 141. **CityGaussian**: <https://dekuliutesla.github.io/citygs/>
> 142. **MOFA-Video**: <https://myniuuu.github.io/MOFA_Video/>
> 143. **VividDream**: <https://vivid-dream-4d.github.io/>
> 144. **Motion2VecSets**: <https://vveicao.github.io/projects/Motion2VecSets/>
> 145. **MultiPly**: <https://eth-ait.github.io/MultiPly/>
> 146. **Neural Gaffer**: <https://neural-gaffer.github.io/>
> 147. **I4VGen**: <https://xiefan-guo.github.io/i4vgen/>
> 148. **ToonCrafter**: <https://doubiiu.github.io/projects/ToonCrafter/>
> 149. **2DGS**: <https://surfsplatting.github.io/>
> 150. **Collaborative Video Diffusion**: <https://collaborativevideodiffusion.github.io/>
> 151. **Looking Backward**: <https://jeff-liangf.github.io/projects/streamv2v/>
> 152. **SadTalker**: <https://sadtalker.github.io/>
> 153. **VividTalk**: <https://humanaigc.github.io/vivid-talk/>
> 154. **I2VEdit**: <https://i2vedit.github.io/>
> 155. **MagicPose4D**: <https://boese0601.github.io/magicpose4d/>
> 156. **Generative Camera Dolly**: <https://gcd.cs.columbia.edu/>
> 157. **ReVideo**: <https://mc-e.github.io/project/ReVideo/>
> 158. **Text-to-Vector Generation with Neural Path Representation**: <https://intchous.github.io/T2V-NPR/>
> 159. **CAT3D**: <https://cat3d.github.io/>
> 160. **StructLDM**: <https://taohuumd.github.io/projects/StructLDM/>
> 161. **AniTalker**: <https://x-lance.github.io/AniTalker/>
> 162. **Dual3D**: <https://dual3d.github.io/>
> 163. **X-Oscar**: <https://xmu-xiaoma666.github.io/Projects/X-Oscar/>
> 164. **HiDiffusion**: <https://hidiffusion.github.io/>
> 165. **Tunnel Try-on**: <https://mengtingchen.github.io/tunnel-try-on-page/>
> 166. **STAG4D**: <https://nju-3dv.github.io/projects/STAG4D/>
> 167. **GS-LRM**: <https://sai-bi.github.io/project/gs-lrm/>
> 168. **GScream**: <https://w-ted.github.io/publications/gscream/>
> 169. **MotionMaster**: <https://sjtuplayer.github.io/projects/MotionMaster/>
> 170. **PhysDreamer**: <https://physdreamer.github.io/>
> 171. **SwapAnything**: <https://swap-anything.github.io/>
> 172. **MagicPose**: <https://boese0601.github.io/magicdance/>
> 173. **ZeST**: <https://ttchengab.github.io/zest/>
> 174. **EMOPortraits**: <https://neeek2303.github.io/EMOPortraits/>
> 175. **StoryDiffusion**: <https://storydiffusion.github.io/>
> 176. **Automatic Controllable Colorization via Imagination**: <https://xy-cong.github.io/imagine-colorization/>
> 177. **MaPa**: <https://zhanghe3z.github.io/MaPa/>
> 178. **EMO**: <https://humanaigc.github.io/emote-portrait-alive/>
> 179. **StreamingT2V**: <https://streamingt2v.github.io/>
> 180. **IDM-VTON**: <https://idm-vton.github.io/>
> 181. **IntrinsicAnything**: <https://zju3dv.github.io/IntrinsicAnything/>
> 182. **Interactive3D**: <https://interactive-3d.github.io/>
> 183. **in2IN**: <https://pabloruizponce.github.io/in2IN/>
> 184. **synthesia**: <https://www.synthesia.io/>
> 185. **DreamWalk**: <https://mshu1.github.io/dreamwalk.github.io/>
> 186. **MagicTime**: <https://pku-yuangroup.github.io/MagicTime/>
> 187. **Gaussian Head Avatar**: <https://yuelangx.github.io/gaussianheadavatar/>
> 188. **Champ**: <https://fudan-generative-vision.github.io/champ/#/>
> 189. **ObjectDrop**: <https://objectdrop.github.io/>
> 190. **HeyGen**: <https://www.heygen.com/>
> 191. **DomoAI**: <https://domoai.app/>
> 192. **Pebblely**: <https://pebblely.com/>
> 193. **Photorealistic Video Generation with Diffusion Models**: <https://walt-video-diffusion.github.io/>
> 194. **3D-GPT**: <https://chuny1.github.io/3DGPT/3dgpt.html>
> 195. **SynthID**: <https://deepmind.google/technologies/synthid/>
> 196. **Palette**: <https://palette.fm/>
> 197. **Upscayl**: <https://upscayl.org/>
> 198. **CLIP-NeRF**: <https://cassiepython.github.io/clipnerf/>
> 199. **Scaling up GANs for Text-to-Image Synthesis**: <https://mingukkang.github.io/GigaGAN/>
> 200. **CoDeF**: <https://qiuyu96.github.io/CoDeF/>
> 201. **MVideo**: <https://mvideo-v1.github.io/>
> 202. **edify-3d**: <https://build.nvidia.com/shutterstock/edify-3d>
> 203. **JoyVASA**: <https://jdh-algo.github.io/JoyVASA/>
> 204. **EchoMimicV1**: <https://antgroup.github.io/ai/echomimic/>
> 205. **EchoMimicV2**: <https://antgroup.github.io/ai/echomimic_v2/>
> 206. **DiffusionGS**: <https://caiyuanhao1998.github.io/project/DiffusionGS/>
> 207. **AnchorCrafter**: <https://cangcz.github.io/Anchor-Crafter/>
> 208. **Generative Omnimatte**: <https://gen-omnimatte.github.io/>
> 209. **MultiFoley**: <https://ificl.github.io/MultiFoley/>
> 210. **ConsisID**: <https://pku-yuangroup.github.io/ConsisID/>
> 211. **CAT4D**: <https://cat-4d.github.io/>
> 212. **Sonic**: <https://jixiaozhong.github.io/Sonic/>
> 213. **MyTimeMachine**: <https://mytimemachine.github.io/>
> 214. **FLOAT**: <https://deepbrainai-research.github.io/float/>
> 215. **SelfSplat**: <https://gynjn.github.io/selfsplat/>
> 216. **I2VControl**: <https://wanquanf.github.io/I2VControl>
> 217. **One Shot, One Talk**: <https://ustc3dv.github.io/OneShotOneTalk/>
> 218. **MEMO**: <https://memoavatar.github.io/>
> 219. **MIDI**: <https://huanngzh.github.io/MIDI-Page/>
> 220. **AnyDressing**: <https://crayon-shinchan.github.io/AnyDressing/>
> 221. **StableAnimator**: <https://francis-rings.github.io/StableAnimator/>
> 222. **MotionShop**: <https://motionshop-diffusion.github.io/>
> 223. **You See it,You Got it**: <https://vision.baai.ac.cn/see3d>
> 224. **StyleMaster**: <https://zixuan-ye.github.io/stylemaster/>
> 225. **DisPose**: <https://lihxxx.github.io/DisPose/>
> 226. **ClearerVoice-Studio**: <https://stable-learn.com/en/clearvoice-studio-tutorial/>
> 227. **Video Seal**: <https://aidemos.meta.com/videoseal>
> 228. **Animated Drawings**: <https://fairanimateddrawings.com/site/home>
> 229. **DiffGS**: <https://junshengzhou.github.io/DiffGS/>
> 230. **FlipSketch**: <https://hmrishavbandy.github.io/flipsketch-web/>
> 231. **Motion Prompting**: <https://motion-prompting.github.io/>
> 232. **Wonderland**: <https://snap-research.github.io/wonderland/>
> 233. **BrushEdit**: <https://liyaowei-stu.github.io/project/BrushEdit/index.html>
> 234. **Whisk**: <https://labs.google/fx/zh/tools/whisk/unsupported-country>
> 235. **AniDoc**: <https://yihao-meng.github.io/AniDoc_demo/>
> 236. **Stag-1**: <https://wzzheng.net/Stag/>
> 237. **PanoDreamer**: <https://people.engr.tamu.edu/nimak/Papers/PanoDreamer/index.html>
> 238. **DI-PCG**: <https://thuzhaowang.github.io/projects/DI-PCG/>
> 239. **Sketch2Sound**: <https://hugofloresgarcia.art/sketch2sound/>
> 240. **INFP**: <https://grisoon.github.io/INFP/>
> 241. **Feat2GS**: <https://fanegg.github.io/Feat2GS/>
> 242. **AI Sound Effect Generator**: <https://www.sound-effect-generator.com/>
> 243. **ElevenLabs**: <https://elevenlabs.io/sound-effects>
> 244. **MotiF**: <https://wang-sj16.github.io/motif/>
> 245. **DrivingForward**: <https://fangzhou2000.github.io/projects/drivingforward/>
> 246. **Birth and Death of a Rose**: <https://chen-geng.com/rose4d/>
> 247. **ZeroHSI**: <https://awfuact.github.io/zerohsi/>
> 248. **Dora**: <https://www.dora.run/ai/>
> 249. **FaceLift**: <https://www.wlyu.me/FaceLift/>
> 250. **SynCamMaster**: <https://jianhongbai.github.io/SynCamMaster/>
> 251. **PDP**: <https://stanford-tml.github.io/PDP.github.io/>
> 252. **TEXT-TO-CAD**: <https://zoo.dev/text-to-cad>
> 253. **Story-Adapter**: <https://jwmao1.github.io/storyadapter/>
> 254. **PERSE**: <https://hyunsoocha.github.io/perse/>
> 255. **SeedVR**: <https://iceclear.github.io/projects/seedvr/>
> 256. **VideoAnydoor**: <https://videoanydoor.github.io/>
> 257. **STAR**: <https://nju-pcalab.github.io/projects/STAR/>
> 258. **3DEnhancer**: <https://yihangluo.com/projects/3DEnhancer/>
> 259. **Hallo3**: <https://fudan-generative-vision.github.io/hallo3/#/>
> 260. **TransPixar**: <https://wileewang.github.io/TransPixar/>
> 261. **DaS**: <https://igl-hkust.github.io/das/>
> 262. **Sana**: <https://nvlabs.github.io/Sana/>
> 263. **InfiniCube**: <https://research.nvidia.com/labs/toronto-ai/infinicube/>
> 264. **apob**: <https://apob.ai/>
> 265. **Diffusion as Shader**: <https://igl-hkust.github.io/das/>
> 266. **SVFR**: <https://wangzhiyaoo.github.io/SVFR/>
> 267. **EnergyMoGen**: <https://jiro-zhang.github.io/EnergyMoGen/>
> 268. **Labelme**: <https://labelme.io/>
> 269. **Magic Mirror**: <https://julianjuaner.github.io/projects/MagicMirror/>
> 270. **Video Alchemist**: <https://snap-research.github.io/open-set-video-personalization/>
> 271. **SPAR3D**: <https://spar3d.github.io/>
> 272. **ConceptMaster**: <https://yuzhou914.github.io/ConceptMaster/>
> 273. **TOPVIEW**: <https://www.topview.ai/>
> 274. **MangaNinja**: <https://johanan528.github.io/MangaNinjia/>
> 275. **LeviTor**: <https://ppetrichor.github.io/levitor.github.io/>
> 276. **Turbo-GS**: <https://ivl.cs.brown.edu/research/turbo-gs.html>
> 277. **RAIN**: <https://pscgylotti.github.io/pages/RAIN/>
> 278. **SynthLight**: <https://vrroom.github.io/synthlight/>
> 279. **Piktochart**: <https://piktochart.com/>
> 280. **Medio.Cool**: <https://www.medio.cool/en/>
> 281. **VTON**: <https://ningshuliang.github.io/2023/Arxiv/index.html>
> 282. **FlexiClip**: <https://creative-gen.github.io/flexiclip.github.io/>
> 283. **CaPa**: <https://ncsoft.github.io/CaPa/>
> 284. **AniGS**: <https://lingtengqiu.github.io/2024/AniGS/>
> 285. **Arc2Avatar**: <https://arc2avatar.github.io/>
> 286. **GaussianAvatar-Editor**: <https://xiangyueliu.github.io/GaussianAvatar-Editor/>
> 287. **MoDec-GS**: <https://kaist-viclab.github.io/MoDecGS-site/>
> 288. **X-Dyna**: <https://x-dyna.github.io/xdyna.github.io/>
> 289. **Textoon**: <https://human3daigc.github.io/Textoon_webpage/>
> 290. **EMO2**: <https://humanaigc.github.io/emote-portrait-alive-2/>
> 291. **MatAnyone**: <https://pq-yang.github.io/projects/MatAnyone/>
> 292. **OmniHuman-1**: <https://omnihuman-lab.github.io/>
> 293. **Scaling Up 3D Gaussian Splatting Training**: <https://daohanlu.github.io/scaling-up-3dgs/>
> 294. **DynVFX**: <https://dynvfx.github.io/>
> 295. **AuraFusion360**: <https://kkennethwu.github.io/aurafusion360/>
> 296. **JOGG AI**: <https://www.jogg.ai/community/>
> 297. **Animate Anyone 2**: <https://humanaigc.github.io/animate-anyone-2/>
> 298. **Light-A-Video**: <https://bujiazi.github.io/light-a-video.github.io/>
> 299. **CineMaster**: <https://cinemaster-dev.github.io/>
> 300. **HumanDiT**: <https://agnjason.github.io/HumanDiT-page/>
> 301. **Pippo**: <https://yashkant.github.io/pippo/>
> 302. **MetaHuman**: <https://www.unrealengine.com/zh-CN/metahuman>
> 303. **Phantom**: <https://phantom-video.github.io/Phantom/>
> 304. **SkyReels**: <https://www.skyreels.ai/home>
> 305. **Stockimg AI**: <https://stockimg.ai/>
> 306. **FantasyID**: <https://fantasy-amap.github.io/fantasy-id/>
> 307. **CAST**: <https://sites.google.com/view/cast4>
> 308. **EmbodiedScan**: <https://tai-wang.github.io/embodiedscan/>
> 309. **Kiss3DGen**: <https://ltt-o.github.io/Kiss3dgen.github.io/>
> 310. **Avat3r**: <https://tobias-kirschstein.github.io/avat3r/>
> 311. **Raphael AI**: <https://raphael.app/zh>
> 312. **UniScene**: <https://arlo0o.github.io/uniscene/>
> 313. **MeshPad**: <https://derkleineli.github.io/meshpad/>
> 314. **Vid2Avatar-Pro**: <https://moygcc.github.io/vid2avatar-pro/>
> 315. **Heygem**: <https://github.com/GuijiAI/HeyGem.ai>
> 316. **TrajectoryCrafter**: <https://trajectorycrafter.github.io/>
> 317. **DreamRelation**: <https://dreamrelation.github.io/>
> 318. **ObjectMover**: <https://xinyu-andy.github.io/ObjMover/>
> 319. **Motion Anything**: <https://steve-zeyu-zhang.github.io/MotionAnything/>
> 320. **NotaGen**: <https://electricalexis.github.io/notagen-demo/>
> 321. **Phase**: <https://www.phase.com/>
> 322. **VACE**: <https://ali-vilab.github.io/VACE-Page/>
> 323. **Flux AI**: <https://flux-ai.io/cn/>
> 324. **Klingai**: <https://app.klingai.com/cn/>
> 325. **miaohua**: <https://miaohua.sensetime.com/>
> 326. **PIKA AI**: <https://pikaai.org/cn/>
> 327. **EchoMimic**: <https://badtobest.github.io/echomimic.html>
> 328. **EchoMimicV2**: <https://antgroup.github.io/ai/echomimic_v2/>
> 329. **Uthana**: <https://uthana.com/>
> 330. **jimeng**: <https://jimeng.jianying.com/ai-tool/home>
> 331. **chanjing**: <https://www.chanjing.cc/>
> 332. **AIPPT**: <https://www.aippt.cn/>
> 333. **ETCH**: <https://boqian-li.github.io/ETCH/>
> 334. **Bolt3D**: <https://szymanowiczs.github.io/bolt3d>
> 335. **MTV-Inpaint**: <https://mtv-inpaint.github.io/>
> 336. **Hunyuan3D**: <https://www.hunyuan-3d.com/>
> 337. **Gamma**: <https://gamma.app/zh-cn>
> 338. **DeepMesh**: <https://zhaorw02.github.io/DeepMesh/>
> 339. **SynCity**: <https://research.paulengstler.com/syncity/>
> 340. **MagicMotion**: <https://quanhaol.github.io/magicmotion-site/?ref=aiartweekly>
> 341. **RASA**: <https://alice01010101.github.io/RASA/>
> 342. **A Recipe for Generating 3D Worlds From a Single Image**: <https://katjaschwarz.github.io/worlds/>
> 343. **TaoAvatar**: <https://pixelai-team.github.io/TaoAvatar/> or <https://taoavatar.org/>
> 344. **MoDGS**: <https://modgs.github.io/>
> 345. **MoCha**: <https://congwei1230.github.io/MoCha/>
> 346. **PhysGen3D**: <https://by-luckk.github.io/PhysGen3D/>
> 347. **GroomLight**: <https://syntec-research.github.io/GroomLight/>
> 348. **ChatAnyone**: <https://humanaigc.github.io/chat-anyone/>
> 349. **DreamActor-M1**: <https://grisoon.github.io/DreamActor-M1/>
> 350. **InfiniteYou**: <https://bytedance.github.io/InfiniteYou/>
> 351. **ACTalker**: <https://harlanhong.github.io/publications/actalker/index.html>
> 352. **LAM**: <https://aigc3d.github.io/projects/LAM/>
> 353. **One-Minute Video Generation with Test-Time Training**: <https://test-time-training.github.io/video-dit/>
> 354. **Comprehensive Relighting**: <https://junyingw.github.io/paper/relighting/>
> 355. **OmniSVG**: <https://omnisvg.github.io/>
> 356. **FantasyTalking**: <https://fantasy-amap.github.io/fantasy-talking/>
> 357. **lipsync2**: <https://taneemishere.github.io/projects/lipsync2.html>
> 358. **Move AI**: <https://www.move.ai/>
> 359. **Uni3C**: <https://ewrfcas.github.io/Uni3C/>
> 360. **Event-Enhanced Blurry Video Super-Resolution**: <https://dachunkai.github.io/ev-deblurvsr.github.io/>
> 361. **Infinite Mobility**: <https://infinite-mobility.github.io/>
> 362. **DeepCAD**: <http://www.cs.columbia.edu/cg/deepcad/>
> 363. **3DV-TON**: <https://2y7c3.github.io/3DV-TON/>
> 364. **Insert Anything**: <https://song-wensong.github.io/insert-anything/>
> 365. **Varjo**: <https://varjo.com/company-news/varjo-launches-teleport-2-0-a-generational-leap-in-photorealistic-3d-capture-for-creators/>
> 366. **MotionCritic**: <https://motioncritic.github.io/>
> 367. **Pixel3DMM**: <https://simongiebenhain.github.io/pixel3dmm/>
> 368. **CycleGAN**: <https://junyanz.github.io/CycleGAN/>
> 369. **Semantic Image Synthesis with Spatially-Adaptive Normalization**: <https://nvlabs.github.io/SPADE/>
> 370. **DeepLiveCam**: <https://deeplivecam.net/>
> 371. **AnimateAnywhere**: <https://animateanywhere.github.io/>
> 372. **HunyuanCustom**: <https://hunyuancustom.github.io/>
> 373. **Step1X-3D**: <https://stepfun-ai.github.io/Step1X-3D/>
> 374. **Articulate AnyMesh**: <https://articulateanymesh.github.io/>
> 375. **LightLab**: <https://nadmag.github.io/LightLab/>
> 376. **SOAP**: <https://tingtingliao.github.io/soap/>
> 377. **DICE-Talk**: <https://toto222.github.io/DICE-Talk/>
> 378. **Sketch2Anim**: <https://zhongleilz.github.io/Sketch2Anim/>
> 379. **MTVCrafter**: <https://dingyanb.github.io/MTVCtafter/>
> 380. **ISA4D**: <https://dsaurus.github.io/isa4d/>
> 381. **JoyGen**: <https://joy-mm.github.io/JoyGen/>
> 382. **The Way of Code**: <https://www.thewayofcode.com/>
> 383. **Direct3D-S2**: <https://nju-3dv.github.io/projects/Direct3D-S2/>
> 384. **DUIX**: <https://duix.com/>
> 385. **MotionPro**: <https://zhw-zhang.github.io/MotionPro-page/>
> 386. **MAGREF**: <https://magref-video.github.io/magref.github.io/>
> 387. **ATI**: <https://anytraj.github.io/>
> 388. **Temporal In-Context Fine-Tuning**: <https://kinam0252.github.io/TIC-FT/>
> 389. **Modify Video**: <https://lumalabs.ai/blog/news/introducing-modify-video>
> 390. **MiniMax-Remover**: <https://minimax-remover.github.io/>
> 391. **PartCrafter**: <https://wgsxm.github.io/projects/partcrafter/>
> 392. **MonkeyOCR**: <http://vlrlabmonkey.xyz:7685/>
> 393. **OmniSync**: <https://ziqiaopeng.github.io/OmniSync/>
> 394. **DreamActor-H1**: <https://submit2025-dream.github.io/DreamActor-H1/>
> 395. **MagicTryOn**: <https://vivocameraresearch.github.io/magictryon/>
> 396. **ImmerseGen**: <https://immersegen.github.io/>
> 397. **OmniGen2**: <https://vectorspacelab.github.io/OmniGen2/>
> 398. **NexusGS**: <https://usmizuki.github.io/NexusGS/>
> 399. **SwapAnyHead**: <https://humanaigc.github.io/SwapAnyHead/>
> 400. **FairyGen**: <https://jayleejia.github.io/FairyGen/>
> 401. **Shape-for-Motion**: <https://shapeformotion.github.io/>
> 402. **LangScene-X**: <https://liuff19.github.io/LangScene-X/>
> 403. **PhysRig**: <https://physrig.github.io/>
> 404. **MOSPA**: <https://frank-zy-dou.github.io/projects/MOSPA/index.html>
> 405. **Diffuse-CLoC**: <https://diffusecloc.github.io/website/>
> 406. **SyncTalk**: <https://ziqiaopeng.github.io/synctalk/>
> 407. **EgoTwin**: <https://egotwin.pages.dev/>
> 408. **RenderFormer**: <https://microsoft.github.io/renderformer/>
> 409. **Wan**: <https://wan.video/>
> 410. **VolSplat**: <https://lhmd.top/volsplat/>
> 411. **Code2Video**: <https://showlab.github.io/Code2Video/>
> 412. **HuMo**: <https://phantom-video.github.io/HuMo/>
> 413. **WorldGrow**: <https://world-grow.github.io/>
> 414. **Motion 3-to-4**: <https://motion3-to-4.github.io/>
> 415. **AnyView**: <https://tri-ml.github.io/AnyView/>
> 416. **PhysX-Anything**: <https://physx-anything.github.io/>

# Robotics

## (1) Hardware

> 1. **Zeroth**: <https://docs.zeroth.bot/>
> 2. **Power-over-Skin**: <https://www.figlab.com/research/2024/poweroverskin>
> 3. **RoboDuet**: <https://locomanip-duet.github.io/>
> 4. **The snake that saves lives**: <https://ethz.ch/en/news-and-events/eth-news/news/2024/11/the-snake-that-saves-lives.html>
> 5. **XGO-Rider**: <https://www.kickstarter.com/projects/xgorobot/xgo-rider-desktop-two-wheel-legged-robot-with-ai>
> 6. **7X**: <https://7xr.tech/>
> 7. **Berkeley Humanoid**: <https://berkeley-humanoid.com/>
> 8. **Torobo**: <https://robotics.tokyo/products/torobo/>
> 9. **DexHand**: <https://www.dexhand.org/>
> 10. **NAVER LABS**: <https://www.naverlabs.com/>
> 11. **Surena Humanoid Robot**: <https://surenahumanoid.com/>
> 12. **NEO Home Humanoid**: <https://www.1x.tech/>
> 13. **Digit - Dexterous Manipulation and Touch Perception**: <https://digit.ml/>
> 14. **TidyBot**: <https://tidybot.cs.princeton.edu/>
> 15. **EyeSight Hand**: <https://eyesighthand.github.io/>
> 16. **Carpentopod:A walking table project**: <https://www.decarpentier.nl/carpentopod>
> 17. **MouthPad**: <https://www.augmental.tech/>
> 18. **MiniRHex**: <https://robomechanics.github.io/MiniRHex/>
> 19. **DRAGON**: <https://interestingengineering.com/videos/ep-13-how-dragons-and-insects-are-inspiring-drones-of-the-future>
> 20. **Velociraptor Robot**: <https://indiannerve.com/korean-velociraptor-robot-is-the-fastest-biped-that-can-outrun-usain-bolt-top-speed-46-kmhr-15584/>
> 21. **RoboPanoptes**: <https://robopanoptes.github.io/>
> 22. **ToddlerBot**: <https://toddlerbot.github.io/>
> 23. **DOGlove**: <https://do-glove.github.io/>
> 24. **Torobo Hand**: <https://robotics.tokyo/products/hand/>
> 25. **COVVI Robotics**: <https://www.covvi-robotics.com/>
> 26. **Hannes Hand**: <https://rehab.iit.it/hannes>
> 27. **Vega**: <https://www.dexmate.ai/vega>
> 28. **FIRST Tech Challenge**: <https://ftc-docs.firstinspires.org/en/latest/index.html>
> 29. **Sanctuary AI**: <https://www.sanctuary.ai/>
> 30. **Linkerbot**: <https://www.linkerbot.cn/index>
> 31. **DexHand**: <https://www.dex-robot.com/productionDexhand>
> 32. **Booster T1**: <https://www.boosterobotics.com/zh/>
> 33. **OpenArm**: <https://open-arm.org/>
> 34. **XGO-Rider**: <https://wiki.elecfreaks.com/en/microbit/robot/xgo-rider-kit/introduction/>
> 35. **BamBot**: <https://bambot.org/>
> 36. **ORCA Hand**: <https://www.orcahand.com/>
> 37. **DG-5F Hand**: <https://en.tesollo.com/dg-5f/>
> 38. **Poppy Humanoid**: <https://www.poppy-project.org/en/robots/poppy-humanoid/>
> 39. **agibot**: <http://agibot.cn/>
> 40. **X1-PDG**: <https://www.zhiyuan-robot.com/DOCS/OS/X1-PDG>
> 41. **Fourier-GRX-N1**: <http://support.fftai.cn/main/en/concepts/overview/>
> 42. **Tactile-Readtive Roller Grasper**: <https://yuanshenli.com/tactile_reactive_roller_grasper.html>
> 43. **PP-Tac**: <https://peilin-666.github.io/projects/PP-Tac/>
> 44. **HOMIE**: <https://homietele.github.io/>
> 45. **K-Bot**: <https://docs.kscale.dev/docs/kbot>
> 46. **Berkeley Humanoid Lite**: <https://lite.berkeley-humanoid.org/>
> 47. **Robot Studio**: <https://www.therobotstudio.com/>
> 48. **FastUMI**: <https://fastumi.com/>
> 49. **Vision Tactile Manipulation**: <https://vision-tactile-manip.github.io/teleop/>
> 50. **HOPEJr**: <https://github.com/TheRobotStudio/HOPEJr>
> 51. **Reachy 2**: <https://www.pollen-robotics.com/reachy/>
> 52. **Xiaozhi**: <https://xiaozhi.me/>
> 53. **Two-Stage Tentacle**: <https://hackaday.com/2016/10/05/two-stage-tentacle-mechanisms-part-ii-the-cable-controller/>
> 54. **Unitree Qmini**: <https://github.com/unitreerobotics/Qmini>
> 55. **lerobot**: <https://huggingface.co/docs/lerobot/index>
> 56. **XLeRobot**: <https://github.com/Vector-Wangel/XLeRobot> & <https://xlerobot.readthedocs.io/en/latest/>
> 57. **NuExo**: <https://nubot-nuexo.github.io/>
> 58. **DexUMI**: <https://dex-umi.github.io/>
> 59. **PolyTouch**: <https://polytouch.alanz.info/>
> 60. **RoTipBot**: <https://sites.google.com/view/rotipbot>
> 61. **eFlesh**: <https://e-flesh.com/>
> 62. **ALOHA 2**: <https://aloha-2.github.io/>
> 63. **Mobile ALOHA**: <https://mobile-aloha.github.io/cn.html>
> 64. **AirExo**: <https://airexo.github.io/https://airexo.github.io/>
> 65. **DexHub and DART**: <https://dexhub.ai/project>
> 66. **SharpaWave**: <https://www.sharpa.com/>
> 67. **FlexTail**: <https://minktec.com/>
> 68. **Metal Gel**: <https://www.liquidwire.com/metal-gel>
> 69. **DexWrist**: <https://dexwrist.csail.mit.edu/>
> 70. **Vicarious Surgical Robotic System**: <https://www.vicarioussurgical.com/>
> 71. **FORTE**: <https://merge-lab.github.io/FORTE/>
> 72. **PAPRAS**: <https://uiuckimlab.github.io/papras-pages/>
> 73. **AmazingHand**: <https://github.com/pollen-robotics/AmazingHand>
> 74. **Vision-based Proximity and Tactile Sensing for Robot Arms**: <https://quan-luu.github.io/protac-website/>
> 75. **Manus Quantum**: <https://www.cnbytool.com/productinfo/2836553.html>
> 76. **Humanoid Occupancy**: <https://humanoid-occupancy.github.io/>
> 77. **MEVITA**: <https://haraduka.github.io/mevita-hardware/>
> 78. **Morpheus**: <https://jiawenyang-ch.github.io/Morpheus-Hardware-Design/>
> 79. **Tactile beyond pixels**: <https://akashsharma02.github.io/sparsh-x-ssl/>
> 80. **DEXOP**: <https://dex-op.github.io/>
> 81. **TRLC**: <https://docs.robot-learning.co/>
> 82. **ByteWrist**:<https://bytewrist.github.io/>
> 83. **FlexiTac**: <https://flexitac.github.io/>
> 84. **WAVE**: <https://omron-sinicx.github.io/wave/>
> 85. **M3D-skin**: <https://ssk-yoshimura.github.io/M3D-skin/>
> 86. **Aero Hand Open**: <https://tetheria.github.io/aero-hand-open/>
> 87. **ONGO**: <https://ongolamp.com/>
> 88. **Scanford**: <https://scanford-robot.github.io/>
> 89. **Reachy Mini**: <https://reachymini.net/>
> 90. **Robots That Exist**: <https://robotsthatexist.com/>
> 91. **AlohaMini**: <https://www.alohamini.cn/>
> 92. **Pantograph**: <https://pantograph.com/blog/building-a-preschool-for-robots.html>
> 93. **allonic**: <https://allonic.co/>
> 94. **TacUMI**: <https://tac-umi.github.io/TacUMI/>

## (2) Software

> 1. **ROS2**: <https://docs.ros.org/en/jazzy/index.html>
> 2. **SAFER-Splat**: <https://chengine.github.io/safer-splat/>
> 3. **Neural MP**: <https://mihdalal.github.io/neuralmotionplanner/>
> 4. **AirSLAM**: <https://xukuanhit.github.io/airslam/>
> 5. **SimTK**: <https://simtk.org/>
> 6. **MyoSuite**: <https://sites.google.com/view/myosuite/myosuite>
> 7. **Hyfydy**: <https://hyfydy.com/>
> 8. **LVCP**: <https://sites.google.com/view/lvcp>
> 9. **Hello**: <https://www.hello-algo.com/>
> 10. **Skild AI**: <https://www.skild.ai/>
> 11. **NVIDIA Project GR00T**: <https://developer.nvidia.com/project-gr00t>
> 12. **OpenWorm**: <https://openworm.org/>
> 13. **NVIDIA Isaac ROS**: <https://nvidia-isaac-ros.github.io/>
> 14. **VehicleSim**: <https://www.carsim.com/>
> 15. **GraspNet**: <https://graspnet.net/>
> 16. **Quad-SDK**: <https://robomechanics.github.io/quad-sdk/>
> 17. **MASt3R-SLAM**: <https://edexheim.github.io/mast3r-slam/>
> 18. **Genesis**: <https://genesis-embodied-ai.github.io/>
> 19. **Exbody2**: <https://exbody2.github.io/>
> 20. **HO-Cap**: <https://irvlutd.github.io/HOCap/>
> 21. **ARMOR**: <https://daehwakim.com/armor/>
> 22. **HugWBC**: <https://hugwbc.github.io/>
> 23. **Dexterity Gen**: <https://zhaohengyin.github.io/dexteritygen/>
> 24. **MJINX**: <https://based-robotics.github.io/mjinx/index.html>
> 25. **Newton**: <https://developer.nvidia.com/blog/announcing-newton-an-open-source-physics-engine-for-robotics-simulation>
> 26. **RUKA**: <https://ruka-hand.github.io/>
> 27. **Genie Sim Benchmark**: <https://agibot-world.com/sim-evaluation>
> 28. **Geometric Retargeting**: <https://zhaohengyin.github.io/geort/#>
> 29. **LangWBC**: <https://langwbc.github.io/>
> 30. **Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design**: <https://dgdm-robot.github.io/>
> 31. **AMO**: <https://amo-humanoid.github.io/>
> 32. **PyRoki**: <https://pyroki-toolkit.github.io/>
> 33. **GELLO**: <https://wuphilipp.github.io/gello_site/>
> 34. **FACET**: <https://facet.pages.dev/#/>
> 35. **MPlib**: <https://motion-planning-lib.readthedocs.io/latest/>
> 36. **Newton Physics**: <https://newton-physics.github.io/newton/guide/overview.html>
> 37. **Holistic Mobile Manipulation**: <https://jhavl.github.io/holistic/>
> 38. **Judo**: <https://bdaiinstitute.github.io/judo/>
> 39. **AnimaX**: <https://anima-x.github.io/>
> 40. **CI-MPC**: <https://approximating-global-ci-mpc.github.io/>
> 41. **ros2-urdf-web-converter**: <https://ros2-urdf-web-converter.onrender.com/>
> 42. **Pyrender**: <https://pyrender.readthedocs.io/en/latest/>
> 43. **OSCBF**: <https://stanfordasl.github.io/oscbf/>
> 44. **Nullspace MPC**: <https://mizuhoaoki.github.io/projects/nullspace_mpc>
> 45. **NeuralSVCD**: <https://neuralsvcd.github.io/>
> 46. **Mink**: <https://kevinzakka.github.io/mink/>
> 47. **SLAM-Former**: <https://tsinghua-mars-lab.github.io/SLAM-Former/>
> 48. **Awesome Robotics Libraries**: <https://jslee02.github.io/awesome-robotics-libraries/>
> 49. **trac_ik**: <https://bitbucket.org/traclabs/trac_ik/src/rolling/>
> 50. **ProtoMotions**: <https://protomotions.github.io/>
> 51. **Tnkr**: <https://www.tnkr.ai/>
> 52. **Genie Sim**: <https://agibot-world.com/genie-sim>
> 53. **toppra**: <https://hungpham2511.github.io/toppra/index.html>
> 54. **Humanoid Policy Viewer**: <https://motion-tracking.axell.top/>
> 55. **CHILD**: <https://uiuckimlab.github.io/CHILD-pages/>

# AIRobotics

## (1) Robot Learning

> 1. **UMI(Universal Manipulation Interface)**: <https://umi-gripper.github.io/>
> 2. **PSAG**: <https://www.jianrenw.com/PSAG/>
> 3. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 4. **RGBManip**: <https://rgbmanip.github.io/>
> 5. **RoboStudio**: <https://robostudioapp.com/>
> 6. **ReKep**: <https://rekep-robot.github.io/>
> 7. **DeformGS**: <https://deformgs.github.io/>
> 8. **NeuralFeels**: <https://suddhu.github.io/neural-feels/>
> 9. **ALOHA**: <https://tonyzhaozh.github.io/aloha/>
> 10. **LucidSim**: <https://lucidsim.github.io/>
> 11. **RoPotter**: <https://robot-pottery.github.io/>
> 12. **HOVER**: <https://hover-versatile-humanoid.github.io/>
> 13. **DexMimicGen**: <https://dexmimicgen.github.io/>
> 14. **Eurekaverse**: <https://eureka-research.github.io/eurekaverse/>
> 15. **HIL-SERL**: <https://hil-serl.github.io/>
> 16. **OrbitGrasp**: <https://orbitgrasp.github.io/>
> 17. **3D-ViTac**: <https://binghao-huang.github.io/3D-ViTac/>
> 18. **Physical Intelligence**: <https://www.physicalintelligence.company/blog/pi0>
> 19. **HuDOR**: <https://object-rewards.github.io/>
> 20. **LAPA**: <https://latentactionpretraining.github.io/>
> 21. **ManipGen**: <https://mihdalal.github.io/manipgen/>
> 22. **Robots Pre-Train Robots**: <https://robots-pretrain-robots.github.io/>
> 23. **ARNOLD**: <https://arnold-benchmark.github.io/>
> 24. **GPT-4V(ision) for Robotics**: <https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/>
> 25. **VoxAct-B**: <https://voxact-b.github.io/>
> 26. **ARCap**: <https://stanford-tml.github.io/ARCap/>
> 27. **Harmon**: <https://ut-austin-rpl.github.io/Harmon/>
> 28. **Data Scaling Laws**: <https://data-scaling-laws.github.io/>
> 29. **Dynamic 3D Gaussian Tracking**: <https://gs-dynamics.github.io/>
> 30. **OKAMI**: <https://ut-austin-rpl.github.io/OKAMI/>
> 31. **UniHSI**: <https://xizaoqu.github.io/unihsi/>
> 32. **SDS**: <https://rpl-cs-ucl.github.io/SDSweb/>
> 33. **EgoAllo**: <https://egoallo.github.io/>
> 34. **DART**: <https://zkf1997.github.io/DART/>
> 35. **FrElise**: <https://for-elise.github.io/>
> 36. **PourIt**: <https://hetolin.github.io/PourIt/>
> 37. **Cherrybot**: <https://goodcherrybot.github.io/>
> 38. **AnyCar to Anywhere**: <https://lecar-lab.github.io/anycar/>
> 39. **Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies**: <https://lipschitz-constrained-policy.github.io/>
> 40. **HumanoidOlympics**: <https://humanoidolympics.github.io/>
> 41. **OmniH2O**: <https://omni.human2humanoid.com/>
> 42. **Continuously Improving Mobile Manipulation with Autonomous Real-World RL**: <https://continual-mobile-manip.github.io/>
> 43. **MotIF**: <https://motif-1k.github.io/>
> 44. **Helpful DoggyBot**: <https://helpful-doggybot.github.io/>
> 45. **Blox-Net**: <https://bloxnet.org/>
> 46. **GR-MG**: <https://gr-mg.github.io/>
> 47. **Real-World Cooking Robot System from Recipes**: <https://kanazawanaoaki.github.io/cook-from-recipe-pddl/>
> 48. **GR-1**: <https://gr1-manipulation.github.io/>
> 49. **GR-2**: <https://gr2-manipulation.github.io/>
> 50. **CLoSD**: <https://guytevet.github.io/CLoSD-page/>
> 51. **RDT-1B**: <https://rdt-robotics.github.io/rdt-robotics/>
> 52. **Agile Continuous Jumping in Discontinuous Terrains**: <https://yxyang.github.io/jumping_cod/>
> 53. **Humanoid Manipulation**: <https://humanoid-manipulation.github.io/>
> 54. **Diff-Control**: <https://diff-control.github.io/>
> 55. **Catch It**: <https://mobile-dex-catch.github.io/>
> 56. **Robot See Robot Do**: <https://robot-see-robot-do.github.io/>
> 57. **Gen2Act**: <https://homangab.github.io/gen2act/>
> 58. **Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing**: <https://lecar-lab.github.io/dial-mpc/>
> 59. **ReMEmbR**: <https://nvidia-ai-iot.github.io/remembr/>
> 60. **ReMEmbR**: <https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/>
> 61. **MaskedMimic**: <https://research.nvidia.com/labs/par/maskedmimic/>
> 62. **Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation**: <https://human2humanoid.com/>
> 63. **Theia**: <https://theia.theaiinstitute.com/>
> 64. **Robot Motion Diffusion Model: Motion Generation for Robotic Characters**: <https://la.disneyresearch.com/publication/robot-motion-diffusion-model-motion-generation-for-robotic-characters/>
> 65. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 66. **One-shot Video Imitation via Parameterized Symbolic Abstraction Graphs**: <https://www.jianrenw.com/PSAG/>
> 67. **RGBManip**: <https://rgbmanip.github.io/>
> 68. **ALOHA Unleashed**: <https://aloha-unleashed.github.io/>
> 69. **PianoMime**: <https://pianomime.github.io/>
> 70. **Robot Utility Models**: <https://robotutilitymodels.com/#>
> 71. **Polaris**: <https://star-uu-wang.github.io/Polaris/>
> 72. **RoboStudio**: <https://robostudioapp.com/>
> 73. **ICRT**: <https://icrt.dev/>
> 74. **SkillMimic**: <https://ingrid789.github.io/SkillMimic/>
> 75. **VoicePilot**: <https://sites.google.com/andrew.cmu.edu/voicepilot/>
> 76. **ReKep**: <https://rekep-robot.github.io/>
> 77. **DeformGS**: <https://deformgs.github.io/>
> 78. **ATM**: <https://xingyu-lin.github.io/atm/>
> 79. **Universal Manipulation Interface**: <https://umi-gripper.github.io/>
> 80. **ACE**: <https://ace-teleop.github.io/>
> 81. **Summarize the Past to Predict the Future**: <https://eth-ait.github.io/transfusion-proj/>
> 82. **TacSL**: <https://iakinola23.github.io/tacsl/>
> 83. **RoCo**: <https://project-roco.github.io/>
> 84. **UniT**: <https://zhengtongxu.github.io/unifiedtactile.github.io/>
> 85. **PhysHOI**: <https://wyhuai.github.io/physhoi-page/>
> 86. **UMI on Legs**: <https://umi-on-legs.github.io/>
> 87. **Lifelike Agility and Play in Quadrupedal Robots**: <https://tencent-roboticsx.github.io/lifelike-agility-and-play/>
> 88. **RoboCasa**: <https://robocasa.ai/>
> 89. **GET-Zero**: <https://get-zero-paper.github.io/>
> 90. **DextrAH-G**: <https://sites.google.com/view/dextrah-g>
> 91. **Surgical Robot Transformer**: <https://surgical-robot-transformer.github.io/>
> 92. **Grasping Diverse Objects with Simulated Humanoids**: <https://www.zhengyiluo.com/Omnigrasp-Site/>
> 93. **PoliFormer**: <https://poliformer.allen.ai/>
> 94. **This&That**: <https://cfeng16.github.io/this-and-that/>
> 95. **RoboCat**: <https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/>
> 96. **RoboGen**: <https://robogen-ai.github.io/>
> 97. **EquiBot**: <https://equi-bot.github.io/>
> 98. **Policy Composition From and For Heterogeneous Robot Learning**: <https://liruiw.github.io/policycomp/>
> 99. **GenSim**: <https://gen-sim.github.io/>
> 100. **Bunny-VisionPro**: <https://dingry.github.io/projects/bunny_visionpro.html>
> 101. **Open-TeleVision**: <https://robot-tv.github.io/>
> 102. **DexGraspNet**: <https://pku-epic.github.io/DexGraspNet/>
> 103. **Mobile ALOHA**: <https://mobile-aloha.github.io/>
> 104. **OpenVLA**: <https://openvla.github.io/>
> 105. **MS-Human-700**: <https://lnsgroup.cc/research/MS-Human-700>
> 106. **HumanPlus**: <https://humanoid-ai.github.io/>
> 107. **Octo**: <https://octo-models.github.io/>
> 108. **HOI-M3**: <https://juzezhang.github.io/HOIM3_ProjectPage/>
> 109. **DrEureka**: <https://eureka-research.github.io/dr-eureka/>
> 110. **FLD**: <https://sites.google.com/view/iclr2024-fld/home>
> 111. **SATO**: <https://sato-team.github.io/Stable-Text-to-Motion-Framework/>
> 112. **ViPlanner**: <https://leggedrobotics.github.io/viplanner.github.io/>
> 113. **HumanoidBench**: <https://humanoid-bench.github.io/>
> 114. **DexCap**: <https://dex-cap.github.io/>
> 115. **RT-Sketch**: <https://rt-sketch.github.io/>
> 116. **SARA**: <https://sites.google.com/view/rtsara/?pli=1>
> 117. **AutoRT**: <https://auto-rt.github.io/>
> 118. **RT-Trajectory**: <https://rt-trajectory.github.io/>
> 119. **iGibson**: <https://svl.stanford.edu/igibson/>
> 120. **GOAT**: <https://theophilegervet.github.io/projects/goat/>
> 121. **Dynamic Handover**: <https://binghao-huang.github.io/dynamic_handover/>
> 122. **Eureka**: <https://eureka-research.github.io/>
> 123. **Sequential Dexterity**: <https://sequential-dexterity.github.io/>
> 124. **From Text to Motion**: <https://tnoinkwms.github.io/ALTER-LLM/>
> 125. **MimicGen**: <https://mimicgen.github.io/>
> 126. **NOIR**: <https://noir-corl.github.io/>
> 127. **BC-Z**: <https://sites.google.com/view/bc-z/home>
> 128. **Open-World Object Manipulation using Pre-Trained Vision-Language Models**: <https://robot-moo.github.io/>
> 129. **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models**: <https://instructionaugmentation.github.io/>
> 130. **VIMA**: <https://vimalabs.github.io/>
> 131. **CLIPort**: <https://cliport.github.io/>
> 132. **EmbodiedGPT**: <https://embodiedgpt.github.io/>
> 133. **ASE**: <https://xbpeng.github.io/projects/ASE/index.html>
> 134. **RoboAgent**: <https://robopen.github.io/>
> 135. **RT-2**: <https://robotics-transformer2.github.io/>
> 136. **Do As I Can, Not As I Say**: <https://say-can.github.io/>
> 137. **Perceiver-Actor**: <https://peract.github.io/>
> 138. **VoxPoser**: <https://voxposer.github.io/>
> 139. **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**: <https://tonyzhaozh.github.io/aloha/>
> 140. **Soft Robotic Dynamic In-Hand Pen Spinning**: <https://soft-spin.github.io/>
> 141. **Bimanual Dexterity for Complex Tasks**: <https://bidex-teleop.github.io/>
> 142. **WildLMA**: <https://wildlma.github.io/>
> 143. **Learning Purely Tactile In-Hand Manipulation**: <https://aidx-lab.org/manipulation/humanoids24>
> 144. **FoAR**: <https://tonyfang.net/FoAR/>
> 145. **3D Diffusion Policy**: <https://3d-diffusion-policy.github.io/>
> 146. **Dex-Net**: <https://berkeleyautomation.github.io/dex-net/>
> 147. **See, Hear, Feel**: <https://ai.stanford.edu/~rhgao/see_hear_feel/>
> 148. **DenseMatcher**: <https://tea-lab.github.io/DenseMatcher/>
> 149. **Mobile-TeleVision**: <https://mobile-tv.github.io/>
> 150. **TidyBot++**: <https://tidybot2.github.io/>
> 151. **Planning-Guided Diffusion Policy Learning for Generalizable Contact-Rich Bimanual Manipulation**: <https://glide-manip.github.io/>
> 152. **You Only Plan Once**: <https://tju-air-lab.github.io/projects/YOPO/>
> 153. **Human-Object Interaction from Human-Level Instructions**: <https://hoifhli.github.io/>
> 154. **Video Prediction Policy**: <https://video-prediction-policy.github.io/>
> 155. **HYPERmotion**: <https://hy-motion.github.io/>
> 156. **RoboMIND**: <https://x-humanoid-robomind.github.io/>
> 157. **ForceMimic**: <https://forcemimic.github.io/>
> 158. **AgiBot World**: <https://agibot-world.com/>
> 159. **NaVILA**: <https://navila-bot.github.io/>
> 160. **Manipulate Anything**: <https://robot-ma.github.io/>
> 161. **Meta Motivo**: <https://metamotivo.metademolab.com/>
> 162. **EnerVerse**: <https://sites.google.com/view/enerverse>
> 163. **OmniManip**: <https://omnimanip.github.io/>
> 164. **MuJoCo Playground**: <https://playground.mujoco.org/>
> 165. **Embrace Collisions**: <https://project-instinct.github.io/>
> 166. **SPA**: <https://haoyizhu.github.io/spa/>
> 167. **ASAP**: <https://agile.human2humanoid.com/>
> 168. **Visuomotor Policies to Grasp Anything with Dexterous Hands**: <https://dextrah-rgb.github.io/>
> 169. **HAMSTER**: <https://hamster-robot.github.io/>
> 170. **DexVLA**: <https://dex-vla.github.io/>
> 171. **Humanoid-Getup**: <https://humanoid-getup.github.io/>
> 172. **RHINO**: <https://humanoid-interaction.github.io/>
> 173. **SoFar**: <https://qizekun.github.io/sofar/>
> 174. **BeamDojo**: <https://why618188.github.io/beamdojo/>
> 175. **Helix**: <https://www.figure.ai/news/helix>
> 176. **Reflective Planning**: <https://reflect-vlm.github.io/>
> 177. **AnyPlace**: <https://any-place.github.io/>
> 178. **AnyTop**: <https://anytop2025.github.io/Anytop-page/>
> 179. **Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning**: <https://whole-body-loco.github.io/>
> 180. **OpenRobotLab**: <https://grutopia.github.io/>
> 181. **Learning 3D Dynamic Scene Representations for Robot Manipulation**: <https://dsr-net.cs.columbia.edu/>
> 182. **Hybrid Internal Model for Legged Locomotion**: <https://junfeng-long.github.io/HIMLoco/>
> 183. **DexGraspVLA**: <https://dexgraspvla.github.io/>
> 184. **Discrete-Time Hybrid Automata Learning**: <https://umich-curly.github.io/DHAL/>
> 185. **Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids**: <https://toruowo.github.io/recipe/>
> 186. **Humanoid Parkour Learning**: <https://humanoid4parkour.github.io/>
> 187. **Robot Parkour Learning**: <https://robot-parkour.github.io/>
> 188. **InterMimic**: <https://sirui-xu.github.io/InterMimic/>
> 189. **Reactive Diffusion Policy**: <https://reactive-diffusion-policy.github.io/>
> 190. **BEHAVIOR Robot Suite**: <https://behavior-robot-suite.github.io/>
> 191. **Gemini Robotics**: <https://deepmind.google/technologies/gemini-robotics/>
> 192. **DemoGen**: <https://demo-generation.github.io/>
> 193. **Robo-ABC**: <https://tea-lab.github.io/Robo-ABC/>
> 194. **MimicPlay**: <https://mimic-play.github.io/>
> 195. **Soft and Compliant Contact-Rich Hair Manipulation and Care**: <https://moehair.github.io/>
> 196. **RT-1**: <https://robotics-transformer1.github.io/>
> 197. **Heterogeneous Pre-trained Transformers**: <https://liruiw.github.io/hpt/>
> 198. **GraspNet**: <https://graspnet.net/anygrasp.html>
> 199. **RoboDreamer**: <https://robovideo.github.io/>
> 200. **UniSim**: <https://universal-simulator.github.io/unisim/>
> 201. **Visuomotor Policy Learning via Action Diffusion**: <https://diffusion-policy.cs.columbia.edu/>
> 202. **RoboAgent**: <https://robopen.github.io/index.html>
> 203. **Learning to Act from Actionless Videos through Dense Correspondences**: <https://flow-diffusion.github.io/>
> 204. **OK-Robot**: <https://ok-robot.github.io/>
> 205. **Learning Universal Policies via Text-Guided Video Generation**: <https://universal-policy.github.io/unipi/>
> 206. **Helix**: <https://www.figure.ai/news/helix>
> 207. **Bi-DexHands**: <https://pku-marl.github.io/DexterousHands/>
> 208. **PhysTwin**: <https://jianghanxiao.github.io/phystwin-web/>
> 209. **RoboFactory**: <https://iranqin.github.io/robofactory/>
> 210. **TokenHSI**: <https://liangpan99.github.io/TokenHSI/>
> 211. **RoboBrain**: <https://superrobobrain.github.io/>
> 212. **NeuPAN**: <https://hanruihua.github.io/neupan_project/>
> 213. **Embodied Reasoner**: <https://embodied-reasoner.github.io/>
> 214. **ManipTrans**: <https://maniptrans.github.io/>
> 215. **ReBot**: <https://yuffish.github.io/rebot/>
> 216. **Scalable Real2Sim**: <https://scalable-real2sim.github.io/>
> 217. **RoboGSim**: <https://robogsim.github.io/>
> 218. **LEGATO**: <https://ut-hcrl.github.io/LEGATO/>
> 219. **RobustDexGrasp**: <https://zdchan.github.io/Robust_DexGrasp/>
> 220. **DexTrack**: <https://meowuu7.github.io/DexTrack/>
> 221. **Sequential Multi-Object Grasping with One Dexterous Hand**: <https://hesic73.github.io/SeqMultiGrasp/>
> 222. **Real-is-Sim**: <https://realissim.rai-inst.com/>
> 223. **AnyDexGrasp**: <https://graspnet.net/anydexgrasp/>
> 224. **AnyTouch**: <https://gewu-lab.github.io/AnyTouch/>
> 225. **RoboTwin**: <https://robotwin-benchmark.github.io/index.html>
> 226. **RoboSplat**: <https://yangsizhe.github.io/robosplat/>
> 227. **ZeroGrasp**: <https://sh8.io/#/zerograsp>
> 228. **Chain-of-Modality**: <https://chain-of-modality.github.io/>
> 229. **DexGraspNet 2.0**: <https://pku-epic.github.io/DexGraspNet2.0/>
> 230. **AffordDexGrasp**: <https://isee-laboratory.github.io/AffordDexGrasp/index.html>
> 231. **Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies**: <https://cxu-tri.github.io/FAIL-Detect-Website/>
> 232. **RoboVerse**: <https://roboverseorg.github.io/>
> 233. **ArticuBot**: <https://articubot.github.io/>
> 234. **Manual2Skill**: <https://owensun2004.github.io/Furniture-Assembly-Web/>
> 235. **SERL**: <https://serl-robot.github.io/>
> 236. **Limitations in Visuo-Motor Robot Learning**: <https://tsagkas.github.io/pvrobo/>
> 237. **DeepMimic**: <https://xbpeng.github.io/projects/DeepMimic/index.html>
> 238. **ET-SEED**: <https://et-seed.github.io/>
> 239. **Human2Sim2Robot**: <https://human2sim2robot.github.io/>
> 240. **TesserAct**: <https://tesseractworld.github.io/>
> 241. **Learning to Learn Faster from Human Feedback with Language Model Predictive Control**: <https://robot-teaching.github.io/>
> 242. **NVIDIA Robotics Research and Development Digest**: <https://developer.nvidia.com/blog/r2d2-advancing-robot-mobility-whole-body-control-with-ai-from-nvidia-research/>
> 243. **Adaptive Compliance Policy**: <https://adaptive-compliance.github.io/>
> 244. **DoughNet**: <https://dough-net.github.io/>
> 245. **ManiWAV**: <https://maniwav.github.io/>
> 246. **GROOT**: <https://ut-austin-rpl.github.io/GROOT/>
> 247. **Flow as the Cross-domain Manipulation Interface**: <https://im-flow-act.github.io/>
> 248. **Robotic Control via Embodied Chain-of-Thought Reasoning**: <https://embodied-cot.github.io/>
> 249. **Bridging Adaptivity and Safety**: <https://adaptive-safe-locomotion.github.io/>
> 250. **COMPASS**: <https://nvlabs.github.io/COMPASS/>
> 251. **TWIST**: <https://yanjieze.com/TWIST/>
> 252. **PARC**: <https://michaelx.io/parc/>
> 253. **AgiBot GO-1**: <https://agibot-world.com/blog/go1>
> 254. **TRILL**: <https://ut-austin-rpl.github.io/TRILL/>
> 255. **Learning Force Control for Legged Manipulation**: <https://tif-twirl-13.github.io/learning-compliance>
> 256. **HuB**: <https://hub-robot.github.io/>
> 257. **DexWild**: <https://dexwild.github.io/>
> 258. **Diffusion Policy Policy Optimization**: <https://diffusion-ppo.github.io/>
> 259. **DROID**: <https://droid-dataset.github.io/>
> 260. **Open X-Embodiment**: <https://robotics-transformer-x.github.io/>
> 261. **H3DP**: <https://lyy-iiis.github.io/h3dp/>
> 262. **UAD**: <https://unsup-affordance.github.io/>
> 263. **Real2Render2Real**: <https://real2render2real.com/>
> 264. **PTP**: <https://ptp-robot.github.io/>
> 265. **GarmentPile**: <https://garmentpile.github.io/>
> 266. **DreamGen**: <https://research.nvidia.com/labs/gear/dreamgen/>
> 267. **VideoMimic**: <https://www.videomimic.net/>
> 268. **AdaManip**: <https://adamanip.github.io/>
> 269. **D(R,O) Grasp**: <https://nus-lins-lab.github.io/drograspweb/>
> 270. **LaMMA-P**: <https://lamma-p.github.io/>
> 271. **RoBridge**: <https://abliao.github.io/RoBridge/>
> 272. **SPI-Active**: <https://lecar-lab.github.io/spi-active_/>
> 273. **ConRFT**: <https://cccedric.github.io/conrft/>
> 274. **FastTD3**: <https://younggyo.me/fast_td3/>
> 275. **DYNA-1**: <https://www.dyna.co/research>
> 276. **VLAs that Train Fast, Run Fast, and Generalize Better**: <https://www.pi.website/research/knowledge_insulation>
> 277. **HAND Me the Data**: <https://liralab.usc.edu/handretrieval/>
> 278. **LocoTouch**: <https://linchangyi1.github.io/LocoTouch/>
> 279. **HDC:Humanoid Diffusion Controller**: <https://humanoid-diffusion-controller.github.io/>
> 280. **AMOR**: <https://la.disneyresearch.com/publication/amor-adaptive-character-control-through-multi-objective-reinforcement-learning/>
> 281. **Agentic Robot**: <https://agentic-robot.github.io/>
> 282. **SAM2Act**: <https://sam2act.github.io/>
> 283. **Hold My Beer**: <https://lecar-lab.github.io/SoFTA/>
> 284. **DexMachina**: <https://project-dexmachina.github.io/>
> 285. **Feel The Force**: <https://feel-the-force-ftf.github.io/>
> 286. **TrackVLA**: <https://pku-epic.github.io/TrackVLA-web/>
> 287. **CrossFormer**: <https://crossformer-model.github.io/>
> 288. **Sim2Remote-Real**: <https://s2r2-ig.github.io/>
> 289. **CLONE**: <https://humanoid-clone.github.io/>
> 290. **HoMeR**: <https://homer-manip.github.io/>
> 291. **R2S2**: <https://zzk273.github.io/R2S2/>
> 292. **RoboRefer**: <https://zhoues.github.io/RoboRefer/>
> 293. **Real-Time Action Chunking with Large Models**: <https://www.physicalintelligence.company/research/real_time_chunking>
> 294. **Redwood AI**: <https://www.1x.tech/discover/redwood-ai>
> 295. **Mobi-**: <https://mobipi.github.io/>
> 296. **ViSA-Flow**: <https://visaflow-web.github.io/ViSAFLOW/>
> 297. **Fast-in-Slow**: <https://fast-in-slow.github.io/>
> 298. **CogACT**: <https://cogact.github.io/>
> 299. **3D Motion Field**: <https://zhaohengyin.github.io/3DMF/>
> 300. **ReLIC**: <https://relic-locoman.rai-inst.com/>
> 301. **BridgeVLA**: <https://bridgevla.github.io/home_page.html>
> 302. **RoboVLMs**: <https://robovlms.github.io/>
> 303. **Generalist**: <https://generalistai.com/>
> 304. **RH20T**: <https://rh20t.github.io/>
> 305. **AutoVLA**: <https://autovla.github.io/>
> 306. **Particle-Grid Neural Dynamics**: <https://kywind.github.io/pgnd>
> 307. **Vision in Action**: <https://vision-in-action.github.io/>
> 308. **AMPLIFY Robotics**: <https://amplify-robotics.github.io/>
> 309. **Embodied Web Agents**: <https://embodied-web-agent.github.io/>
> 310. **VR-Robo**: <https://vr-robo.github.io/>
> 311. **RoboTwin 2.0**: <https://robotwin-platform.github.io/>
> 312. **Forward Dynamics Model**: <https://leggedrobotics.github.io/fdm.github.io/>
> 313. **Dex1B**: <https://jiyuezh.github.io/research/dex1b>
> 314. **GaussianProperty**: <https://gaussian-property.github.io/>
> 315. **ManiBox**: <https://thkkk.github.io/manibox>
> 316. **EMOS**: <https://emos-project.github.io/>
> 317. **Tiny-VLA**: <https://tiny-vla.github.io/>
> 318. **CoVLA**: <https://turingmotors.github.io/covla-ad/>
> 319. **GMT**: <https://gmt-humanoid.github.io/>
> 320. **CLIP**: <https://openai.com/index/clip/>
> 321. **OpenVLA-OFT**: <https://openvla-oft.github.io/>
> 322. **ControlVLA**: <https://controlvla.github.io/>
> 323. **LeVERB**: <https://ember-lab-berkeley.github.io/LeVERB-Website/>
> 324. **Gemini Robotics On-Device**: <https://deepmind.google/discover/blog/gemini-robotics-on-device-brings-ai-to-local-robotic-devices/>
> 325. **An Affordance-Aware Hierarchical Model for General Robotic Manipulation**: <https://a-embodied.github.io/A0/>
> 326. **CUPID**: <https://cupid-curation.github.io/>
> 327. **DemoDiffusion**: <https://demodiffusion.github.io/>
> 328. **LAMARL**: <https://windylab.github.io/LAMARL/>
> 329. **AMP**: <https://xbpeng.github.io/projects/AMP/index.html>
> 330. **PHC**: <https://www.zhengyiluo.com/PHC-Site/>
> 331. **PULSE**: <https://www.zhengyiluo.com/PULSE-Site/>
> 332. **PDC**: <https://www.zhengyiluo.com/PDC-Site/>
> 333. **Hume**: <https://hume-vla.github.io/>
> 334. **FACTR**: <https://jasonjzliu.com/factr/>
> 335. **myolab**: <https://demo.myolab.ai/demo>
> 336. **VITAL**: <https://vitalprecise.github.io/>
> 337. **FEAST**: <https://emprise.cs.cornell.edu/feast/>
> 338. **Human2LocoMan**: <https://human2bots.github.io/>
> 339. **WSRL**: <https://zhouzypaul.github.io/wsrl/>
> 340. **SAPIEN**: <https://sapien.ucsd.edu/>
> 341. **RoboTransfer**: <https://horizonrobotics.github.io/robot_lab/robotransfer/>
> 342. **ForceVLA**: <https://sites.google.com/view/forcevla2025/>
> 343. **DexVLG**: <https://jiaweihe.com/dexvlg>
> 344. **TypeTele**: <https://isee-laboratory.github.io/TypeTele/>
> 345. **ArtVIP**: <https://x-humanoid-artvip.github.io/>
> 346. **ViTacFormer**: <https://roboverseorg.github.io/ViTacFormerPage/>
> 347. **Q-chunking**: <https://colinqiyangli.github.io/qc/>
> 348. **UniTac**: <https://ivl.cs.brown.edu/research/unitac>
> 349. **Beyond Robustness**: <https://leixinjonaschang.github.io/leggedloadadapt.github.io/>
> 350. **SimLingo**: <https://www.katrinrenz.de/simlingo/>
> 351. **FALCON**: <http://lecar-lab.github.io/falcon-huma>
> 352. **EmbodieDreamer**: <https://embodiedreamer.github.io/>
> 353. **EgoVLA**: <https://rchalyang.github.io/EgoVLA/>
> 354. **Touch in the Wild**: <https://binghao-huang.github.io/touch_in_the_wild/>
> 355. **ModSkill**: <https://yh2371.github.io/modskill/>
> 356. **Walk These Ways**: <https://gmargo11.github.io/walk-these-ways/>
> 357. **VidBot**: <https://hanzhic.github.io/vidbot-project/>
> 358. **What Matters in Learning from Large-Scale Datasets for Robot Manipulation**: <https://robo-mimiclabs.github.io/>
> 359. **RAMBO**: <https://jin-cheng.me/rambo.github.io/>
> 360. **GraspGen**: <https://graspgen.github.io/>
> 361. **TriVLA**: <https://zhenyangliu.github.io/TriVLA/>
> 362. **ThinkAct**: <https://jasper0314-huang.github.io/thinkact-vla/>
> 363. **BeyondMimic**: <https://beyondmimic.github.io/>
> 364. **GeoVLA**: <https://linsun449.github.io/GeoVLA/>
> 365. **Hand-Eye Autonomous Delivery**: <https://stanford-tml.github.io/HEAD/>
> 366. **Robot Trains Robot**: <https://robot-trains-robot.github.io/>
> 367. **HannesImitation**: <https://hsp-iit.github.io/HannesImitation/>
> 368. **MolmoAct**: <https://allenai.org/blog/molmoact>
> 369. **Evaluating Pi0 in the Wild**: <https://penn-pal-lab.github.io/Pi0-Experiment-in-the-Wild/>
> 370. **Large Behavior Models and Atlas Find New Footing**: <https://bostondynamics.com/blog/large-behavior-models-atlas-find-new-footing/>
> 371. **The Lazy Robot**: <https://levevictor.github.io/thelazyrobot/>
> 372. **ReWiND**: <https://rewind-reward.github.io/>
> 373. **RLDG**: <https://generalist-distillation.github.io/>
> 374. **RIPT-VLA**: <https://ariostgx.github.io/ript_vla/>
> 375. **ECoT-Lite**: <https://ecot-lite.github.io/>
> 376. **HITTER**: <https://humanoid-table-tennis.github.io/>
> 377. **Embodied-R1**: <https://embodied-r1.github.io/>
> 378. **MemoryVLA**: <https://shihao1895.github.io/MemoryVLA/>
> 379. **EMMA**: <https://ego-moma.github.io/>
> 380. **ManiFlow**: <https://maniflow-policy.github.io/>
> 381. **Robix**: <https://robix-seed.github.io/robix/>
> 382. **Deep Reactive Policy**: <https://deep-reactive-policy.com/>
> 383. **Gemini Robotics On-Device**: <https://deepmind.google/models/gemini-robotics/gemini-robotics-on-device/>
> 384. **Align-Then-stEer**: <https://align-then-steer.github.io/>
> 385. **MinD**: <https://manipulate-in-dream.github.io/>
> 386. **UnifoLM-WMA-0**: <https://unigen-x.github.io/unifolm-world-model-action.github.io/>
> 387. **StableMotion**: <https://yxmu.foo/stablemotion-page/>
> 388. **VideoMimic**: <https://videomimic.github.io/>
> 389. **Isaac Lab**: <https://isaac-sim.github.io/IsaacLab/main/index.html>
> 390. **Dexplore**: <https://sirui-xu.github.io/dexplore/>
> 391. **DreamControl**: <https://genrobo.github.io/DreamControl/>
> 392. **Project Go-Big**: <https://www.figure.ai/news/project-go-big>
> 393. **TrajBooster**: <https://jiachengliu3.github.io/TrajBooster/>
> 394. **HDMI**: <https://hdmi-humanoid.github.io/#/>
> 395. **MaskedManipulator**: <https://research.nvidia.com/labs/par/maskedmanipulator/>
> 396. **Behavior Foundation Model for Humanoid Robots**: <https://bfm4humanoid.github.io/>
> 397. **LightVLA**: <https://liauto-research.github.io/LightVLA/>
> 398. **RoboDexVLM**: <https://henryhcliu.github.io/robodexvlm/>
> 399. **Any2Track**: <https://zzk273.github.io/Any2Track/>
> 400. **Motion Priors Reimagined**: <https://anymalprior.github.io/>
> 401. **Residual Off-Policy RL for Finetuning Behavior Cloning Policies**: <https://residual-offpolicy-rl.github.io/>
> 402. **BumbleBee**: <https://beingbeyond.github.io/BumbleBee/>
> 403. **RDT2**: <https://rdt-robotics.github.io/rdt2/>
> 404. **Gemini Robotics 1.5**: <https://deepmind.google/models/gemini-robotics/>
> 405. **Expanding Our Data Engine for Physical AI**: <https://scale.com/blog/physical-ai>
> 406. **Omni-Bodied**: <https://www.skild.ai/blogs/omni-bodied>
> 407. **ImMimic**: <https://sites.google.com/view/immimic>
> 408. **UnifiedForce**: <https://unified-force.github.io/>
> 409. **DemoGrasp**: <https://beingbeyond.github.io/DemoGrasp/>
> 410. **Fabrica**: <https://fabrica.csail.mit.edu/>
> 411. **DSRL**: <https://diffusion-steering.github.io/>
> 412. **OmniRetarget**: <https://omniretarget.github.io/>
> 413. **SARM**: <https://qianzhong-chen.github.io/sarm.github.io/>
> 414. **VLM2VLA**: <https://vlm2vla.github.io/>
> 415. **ADD**: <https://add-moo.github.io/>
> 416. **CDF**: <https://sites.google.com/view/cdfmp/home>
> 417. **VLA-Adapter**: <https://vla-adapter.github.io/>
> 418. **ResMimic**: <https://resmimic.github.io/>
> 419. **EmbodiedCoder**: <https://anonymous.4open.science/w/Embodied-Coder/>
> 420. **Advantage-Weighted Regression**: <https://xbpeng.github.io/projects/AWR/index.html>
> 421. **VT-Refine**: <https://binghao-huang.github.io/vt_refine/>
> 422. **RL-augmented MPC**: <https://rl-augmented-mpc.github.io/rlaugmentedmpc/>
> 423. **DexNDM**: <https://meowuu7.github.io/DexNDM/>
> 424. **EgoBridge**: <https://ego-bridge.github.io/>
> 425. **MotionTrans**: <https://motiontrans.github.io/>
> 426. **FIPER**: <https://tum-lsy.github.io/fiper_website/>
> 427. **VTLA**: <https://sites.google.com/view/vtla>
> 428. **What Can RL Bring to VLA Generalization**: <https://rlvla.github.io/>
> 429. **TacRefineNet**: <https://sites.google.com/view/tacrefinenet>
> 430. **Learning to Ball**: <https://pei-xu.github.io/Basketball>
> 431. **AdaMimic**: <https://taohuang13.github.io/adamimic.github.io/>
> 432. **RL-100**: <https://lei-kun.github.io/RL-100/>
> 433. **DEAS**: <https://changyeon.site/deas/>
> 434. **Self-Improving Embodied Foundation Models**: <https://self-improving-efms.github.io/>
> 435. **Spatial Forcing**: <https://spatial-forcing.github.io/https://spatial-forcing.github.io/>
> 436. **Dexbotic**: <https://dexbotic.com/>
> 437. **SoftMimic**: <https://gmargo11.github.io/softmimic/>
> 438. **Ctrl-World**: <https://ctrl-world.github.io/>
> 439. **VLA**: <https://vla-2.github.io/>
> 440. **Multi-Modal Manipulation via Multi-Modal Policy Consensus**: <https://policyconsensus.github.io/>
> 441. **LIBERO-Plus**: <https://sylvestf.github.io/LIBERO-plus/>
> 442. **Probe, Learn, Distill: Self-Improving Vision-Language-Action Models with Data Generation via Residual RL**: <https://www.wenlixiao.com/self-improve-VLA-PLD>
> 443. **RISEOffline**: <https://uwrobotlearning.github.io/RISE-offline/>
> 444. **Mimic Robotics**: <https://www.mimicrobotics.com/>
> 445. **RoboOS**: <https://flagopen.github.io/RoboOS/>
> 446. **TWIST2**: <https://yanjieze.com/TWIST2/>
> 447. **BFM-Zero**: <https://lecar-lab.github.io/BFM-Zero/>
> 448. **Towards Versatile Humanoid Table Tennis**: <https://purdue-tracelab.github.io/ttrobot.github.io/>
> 449. **GentleHumanoid**: <https://gentle-humanoid.axell.top/#/>
> 450. **Ego-VCP**: <https://ego-vcp.github.io/>
> 451. **GEAR-SONIC**: <https://nvlabs.github.io/GEAR-SONIC/>
> 452. **NovaFlow**: <https://novaflow.lhy.xyz/>
> 453. **Learning a Thousand Tasks in a Day**: <https://www.robot-learning.uk/learning-1000-tasks>
> 454. **SPIDER**: <https://jc-bao.github.io/spider-project/>
> 455. **MoMaGen**: <https://momagen.github.io/>
> 456. **TextOp**: <https://text-op.github.io/>
> 457. **decPLM**: <https://decplm.github.io/>
> 458. **HMC**: <https://loco-hmc.github.io/>
> 459. **DemoHLM**: <https://beingbeyond.github.io/DemoHLM/>
> 460. **VPP-TC**: <https://vpp-tc.github.io/>
> 461. **Whole-Body Inverse Dynamics MPC for Legged Loco-Manipulation**: <https://lukasmolnar.github.io/wb-mpc-locoman/>
> 462. **ManipulationNet**: <https://www.manipulation-net.org/>
> 463. **CoDA**: <https://phj128.github.io/page/CoDA/index.html>
> 464. **AMS**: <https://opendrivelab.com/AMS/>
> 465. **DoorMan**: <https://doorman-humanoid.github.io/>
> 466. **BC-Z**: <https://sites.google.com/view/bc-z/>
> 467. **VIRAL**: <https://viral-humanoid.github.io/>
> 468. **EfficientFlow**: <https://efficientflow.github.io/>
> 469. **Robo2VLM**: <https://berkeleyautomation.github.io/robo2vlm/>
> 470. **cuMotion**: <https://nvidia-isaac.github.io/cumotion/index.html>
> 471. **Evaluating Gemini Robotics Policies in a Veo World Simulator**: <https://veo-robotics.github.io/>
> 472. **X-Humanoid**: <https://showlab.github.io/X-Humanoid/>
> 473. **PhysHMR**: <https://fengq1a0.github.io/projects/physhmr/>
> 474. **WholeBodyVLA**: <https://opendrivelab.com/WholeBodyVLA/>
> 475. **SAGA**: <https://robot-saga.github.io/>
> 476. **Human-To-Robot**: <https://www.pi.website/research/human_to_robot>
> 477. **PolaRiS**: <https://polaris-evals.github.io/>
> 478. **Vid2Robot**: <https://vid2robot.github.io/>
> 479. **CRISP**: <https://crisp-real2sim.github.io/CRISP-Real2Sim/>
> 480. **mimic-video**: <https://mimic-video.github.io/>
> 481. **kai0**: <https://mmlab.hk/research/kai0>
> 482. **Motus**: <https://motus-robotics.github.io/motus>
> 483. **Act2Goal**: <https://act2goal.github.io/>
> 484. **GenieReasoner**: <https://geniereasoner.github.io/GenieReasoner/>
> 485. **mjlab**: <https://mujocolab.github.io/mjlab/>
> 486. **SmolVLA**: <https://smolvla.net/>
> 487. **Locomotion Beyond Feet**: <https://locomotion-beyond-feet.github.io/>
> 488. **Learning by watching human videos**: <https://www.skild.ai/blogs/learning-by-watching>
> 489. **Being-H0**: <https://beingbeyond.github.io/Being-H0/>
> 490. **Walk the PLANC**: <https://caltech-amber.github.io/planc/>
> 491. **TeleOpBench**: <https://gorgeous2002.github.io/TeleOpBench/>
> 492. **EmbodiChain**: <https://dexforce.com/embodichain/index.html#/>
> 493. **SafeFall**: <https://safefall.github.io/>
> 494. **Click and Traverse**: <https://axian12138.github.io/CAT/>
> 495. **CEI**: <https://cross-embodiment-interface.github.io/>
> 496. **Closing the Reality Gap**: <https://dexmanip-seed.github.io/dexmanip/>
> 497. **EgoMimic**: <https://egomimic.github.io/>
> 498. **CLAP**: <https://lin-shan.com/CLAP/>
> 499. **NeoVerse**: <https://neoverse-4d.github.io/>
> 500. **PointWorld**: <https://point-world.github.io/>
> 501. **Cosmos Policy**: <https://research.nvidia.com/labs/dir/cosmos-policy/>
> 502. **Fast-ThinkAct**: <https://jasper0314-huang.github.io/fast-thinkact/>
> 503. **ReconVLA**: <https://zionchow.github.io/ReconVLA/>
> 504. **LingBot-VLA**: <https://technology.robbyant.com/lingbot-vla>
> 505. **NavDP**: <https://wzcai99.github.io/navigation-diffusion-policy.github.io/>
> 506. **LoGoPlanner**: <https://steinate.github.io/logoplanner.github.io/>
> 507. **EMPM**: <https://embodied-mpm.github.io/>
> 508. **Gemini Robotics-ER**: <https://deepmind.google/models/gemini-robotics/gemini-robotics-er/>
> 509. **Hi Robot**: <https://www.pi.website/research/hirobot>
> 510. **Pi0**: <https://www.pi.website/blog/pi0>
> 511. **Pi0.5**: <https://www.pi.website/blog/pi05>
> 512. **Pi0.6**: <https://www.pi.website/blog/pistar06>
> 513. **WoW**: <https://wow-world-model.github.io/>
> 514. **RoboStriker**: <https://yinkangning0124.github.io/RoboStriker/>
> 515. **FRoM-W1**: <https://openmoss.github.io/FRoM-W1/>
> 516. **HumanX**: <https://wyhuai.github.io/human-x/>
> 517. **Deep Whole-body Parkour**: <https://project-instinct.github.io/deep-whole-body-parkour/>
> 518. **SurfSplat**: <https://hebing-sjtu.github.io/SurfSplat-website/>
> 519. **EgoMI**: <https://egocentric-manipulation-interface.github.io/>
> 520. **LOVON**: <https://daojiepeng.github.io/LOVON/>
> 521. **DreamZero**: <https://dreamzero0.github.io/>
> 522. **GM-100**: <https://www.rhos.ai/research/gm-100/>
> 523. **HUSKY**: <https://husky-humanoid.github.io/>
> 524. **Eagle-WBC**: <https://eagle-wbc.github.io/>
> 525. **VB-Com**: <https://renjunli99.github.io/vbcom.github.io/>
> 526. **UnifoLM-VLA-0**: <https://unigen-x.github.io/unifolm-vla.github.io/>
> 527. **XHugWBC**: <https://xhugwbc.github.io/>
> 528. **FPO-Control**: <https://hongsukchoi.github.io/fpo-control/>
> 529. **InterPrior**: <https://sirui-xu.github.io/InterPrior/>
> 530. **RynnBrain**: <https://alibaba-damo-academy.github.io/RynnBrain.github.io/>
> 531. **HuMI**: <https://humanoid-manipulation-interface.github.io/#/>
> 532. **CHIP**: <https://nvlabs.github.io/CHIP/>
> 533. **HEX**: <https://hex-humanoid.github.io/>
> 534. **SceneSmith**: <https://scenesmith.github.io/>
> 535. **RISE**: <https://opendrivelab.com/kai0-rl/>
> 536. **SAGE**: <https://nvlabs.github.io/sage/>
> 537. **LaST0**: <https://vla-last0.github.io/>
> 538. **DreamDojo**: <https://dreamdojo-world.github.io/>
> 539. **ExtremControl**: <https://owenowl.github.io/extremcontrol/>
> 540. **GigaBrain-0.5M**: <https://gigabrain05m.github.io/>
> 541. **Xiaomi-Robotics-0**: <https://xiaomi-robotics-0.github.io/>
> 542. **MOSAIC**: <https://baai-humanoid.github.io/MOSAIC/>
> 543. **VideoWorld 2**: <https://maverickren.github.io/VideoWorld2.github.io/>
> 544. **DynamicVLA**: <https://www.infinitescript.com/project/dynamic-vla>
> 545. **DWM**: <https://snuvclab.github.io/dwm/>
> 546. **PHP**: <https://php-parkour.github.io/>
> 547. **RobotGen4D**: <https://robotgen4d.github.io/>
> 548. **MolmoSpaces**: <https://allenai.org/blog/molmospaces>
> 549. **MuJoCo WASM**: <https://dev.to/googleai/building-a-gemini-powered-robotics-simulator-in-the-browser-with-mujoco-wasm-hjj>
> 550. **HumanoidTerrain**: <https://humanoid-challenging-terrain.github.io/>
> 551. **AdaWorldPolicy**: <https://adaworldpolicy.github.io/>
> 552. **FeelAnyForce**: <https://prg.cs.umd.edu/FeelAnyForce>
> 553. **LAP**: <https://lap-vla.github.io/>
> 554. **RGMT**: <https://zeonsunlightyu.github.io/RGMT.github.io/>
> 555. **GMR**: <https://jaraujo98.github.io/retargeting_matters/>
> 556. **EgoScale**: <https://research.nvidia.com/labs/gear/egoscale/>
> 557. **FARM**: <https://tactile-farm.github.io/>
> 558. **Habilis**: <https://tommoro-ai.github.io/habilis-beta/>

## (2) Autonomous Driving

> 1. **X-Mobility**: <https://nvlabs.github.io/X-MOBILITY/>
> 2. **LagMemo**: <https://weekgoodday.github.io/lagmemo/>

## (3) Embodied Intelligence

> 1. **Gymnasium**: <https://gymnasium.farama.org/>

# Metaverse

## (1) Omniverse

## (2) Digital Twin

# Utilities

## (1) Computer Vision

> 1. **OpenCV**: <https://opencv.org/>
> 2. **roboflow**: <https://roboflow.com/>
> 3. **MoGe**: <https://wangrc.site/MoGePage/>
> 4. **Cloth-Splatting**: <https://kth-rpl.github.io/cloth-splatting/>
> 5. **SpectroMotion**: <https://cdfan0627.github.io/spectromotion/>
> 6. **SMITE**: <https://segment-me-in-time.github.io/>
> 7. **VistaDream**: <https://vistadream-project-page.github.io/index.html>
> 8. **InterMask**: <https://gohar-malik.github.io/intermask/>
> 9. **Dessie**: <https://celiali.github.io/Dessie/>
> 10. **CoTracker3**: <https://cotracker3.github.io/>
> 11. **PointCloud Conditioned Mesh Generation**: <https://research.nvidia.com/labs/dir/edgerunner/gallery/point_cond_4.html>
> 12. **Depth Any Video with Scalable Synthetic Data**: <https://depthanyvideo.github.io/>
> 13. **MonST3R**: <https://monst3r-project.github.io/>
> 14. **EVER**: <https://half-potato.gitlab.io/posts/ever/>
> 15. **CoTracker**: <https://co-tracker.github.io/>
> 16. **WiLoR**: <https://rolpotamias.github.io/WiLoR/>
> 17. **MIMO**: <https://menyifang.github.io/projects/MIMO/index.html>
> 18. **M2Mapping**: <https://jianhengliu.github.io/Projects/M2Mapping/>
> 19. **3D Gaussian Splatting for Real-Time Radiance Field Rendering**: <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>
> 20. **StableNormal**: <https://stable-x.github.io/StableNormal/>
> 21. **EmbodiedSAM**: <https://xuxw98.github.io/ESAM/>
> 22. **Large tendue 3D Holographic Display with Content-adpative Dynamic Fourier Modulation**: <https://bchao1.github.io/holo_dfm/>
> 23. **Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos**: <https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/>
> 24. **EgoHDM**: <https://handiyin.github.io/EgoHDM/>
> 25. **DepthCrafter**: <https://depthcrafter.github.io/>
> 26. **Spann3R**: <https://hengyiwang.github.io/projects/spanner>
> 27. **OpenIns3D**: <https://zheninghuang.github.io/OpenIns3D/>
> 28. **Bilateral Reference for High-Resolution Dichotomous Image Segmentation**: <https://www.birefnet.top/>
> 29. **ObjectCarver**: <https://objectcarver.github.io/>
> 30. **Improving 2D Feature Representations by 3D-Aware Fine-Tuning**: <https://ywyue.github.io/FiT3D/>
> 31. **Shape of Motion**: <https://shape-of-motion.github.io/>
> 32. **DINO-Tracker**: <https://dino-tracker.github.io/>
> 33. **DiffIR2VR-Zero**: <https://jimmycv07.github.io/DiffIR2VR_web/>
> 34. **Vidu4D**: <https://vidu4d-dgs.github.io/>
> 35. **RaDe-GS**: <https://baowenz.github.io/radegs/>
> 36. **Semantic Gaussians**: <https://sharinka0715.github.io/semantic-gaussians/>
> 37. **FoundationPose**: <https://nvlabs.github.io/FoundationPose/>
> 38. **InstantSplat**: <https://instantsplat.github.io/>
> 39. **GS-Pose**: <https://dingdingcai.github.io/gs-pose/>
> 40. **I'M HOI**: <https://afterjourney00.github.io/IM-HOI.github.io/>
> 41. **MeshLRM**: <https://sarahweiii.github.io/meshlrm/>
> 42. **LGM**: <https://me.kiui.moe/lgm/>
> 43. **Efficient LoFTR**: <https://zju3dv.github.io/efficientloftr/>
> 44. **VideoGigaGAN**: <https://videogigagan.github.io/>
> 45. **SpatialTracker**: <https://henry123-boy.github.io/SpaTracker/>
> 46. **Key2Mesh**: <https://key2mesh.github.io/>
> 47. **Ultralytics YOLO11**: <https://docs.ultralytics.com/>
> 48. **Neuralangelo**: <https://research.nvidia.com/labs/dir/neuralangelo/>
> 49. **SAMURAI**: <https://yangchris11.github.io/samurai/>
> 50. **YOLO**: <https://docs.ultralytics.com/zh>
> 51. **Buffer Anytime**: <https://bufferanytime.github.io/index.html>
> 52. **RollingDepth**: <https://rollingdepth.github.io/>
> 53. **Efficient Track Anything**: <https://yformer.github.io/efficient-track-anything/>
> 54. **MegaSaM**: <https://mega-sam.github.io/>
> 55. **DualPM**: <https://dualpm.github.io/>
> 56. **Align3R**: <https://igl-hkust.github.io/Align3R.github.io/>
> 57. **SAMa**: <https://mfischer-ucl.github.io/sama/>
> 58. **NLF:Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation**: <https://istvansarandi.com/nlf/>
> 59. **Using Diffusion Priors for Video Amodal Segmentation**: <https://diffusion-vas.github.io/>
> 60. **LongVolCap**: <https://zju3dv.github.io/longvolcap/>
> 61. **Multiview Scene Graph**: <https://ai4ce.github.io/MSG/>
> 62. **Prompting Depth Anything**: <https://promptda.github.io/>
> 63. **MVLift**: <https://lijiaman.github.io/projects/mvlift/>
> 64. **SAT-HMR**: <https://chisu001.github.io/SAT-HMR/>
> 65. **DEIM**: <https://www.shihuahuang.cn/DEIM/>
> 66. **PartGen**: <https://silent-chen.github.io/PartGen/>
> 67. **GenHMR**: <https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html>
> 68. **SuperGSeg**: <https://supergseg.github.io/>
> 69. **DUSt3R**: <https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/>
> 70. **Orient Anything**: <https://orient-anything.github.io/>
> 71. **Gaga**: <https://www.gaga.gallery/>
> 72. **ProTracker**: <https://michaelszj.github.io/protracker/>
> 73. **JOSH**: <https://genforce.github.io/JOSH/>
> 74. **Dyn-HaMR**: <https://dyn-hamr.github.io/>
> 75. **Depth Any Camera**: <https://yuliangguo.github.io/depth-any-camera/>
> 76. **DepthLab**: <https://johanan528.github.io/depthlab_web/>
> 77. **MatchAnything**: <https://zju3dv.github.io/MatchAnything/>
> 78. **OmniFusion**: <https://yuyanli0831.github.io/OmniFusion-Website/>
> 79. **Reconstructing People, Places, and Cameras**: <https://muelea.github.io/hsfm/>
> 80. **BioPose**: <https://m-usamasaleem.github.io/publication/BioPose/BioPose.html>
> 81. **SplatMAP**: <https://arxiv.org/html/2501.07015v1>
> 82. **FoundationStereo**: <https://nvlabs.github.io/FoundationStereo/>
> 83. **Video Depth Anything**: <https://videodepthanything.github.io/>
> 84. **Depth Anything V2**: <https://depth-anything-v2.github.io/>
> 85. **GausSurf**: <https://jiepengwang.github.io/GausSurf/>
> 86. **FLARE**: <https://zhanghe3z.github.io/FLARE/>
> 87. **Aether**: <https://aether-world.github.io/>
> 88. **Easi3R**: <https://easi3r.github.io/>
> 89. **SceneScript**: <https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/>
> 90. **HSMR**: <https://isshikihugh.github.io/HSMR/>
> 91. **Multi-view Reconstruction via SfM-guided Monocular Depth Estimation**: <https://zju3dv.github.io/murre/>
> 92. **NormalCrafter**: <https://normalcrafter.github.io/>
> 93. **FRAME**: <https://vcai.mpi-inf.mpg.de/projects/FRAME/>
> 94. **TAPIP3D**: <https://tapip3d.github.io/>
> 95. **ShowMak3r**: <https://nstar1125.github.io/showmak3r/>
> 96. **Matching 2D Images in 3D**: <https://nianticlabs.github.io/mickey/>
> 97. **Scene Coordinate Reconstruction**: <https://nianticlabs.github.io/acezero/>
> 98. **Clinica**: <https://www.clinica.run/>
> 99. **DenseFusion**: <https://sites.google.com/view/densefusion>
> 100. **PoseCNN**: <https://rse-lab.cs.washington.edu/projects/posecnn/>
> 101. **MP-SfM**: <https://opencv.org/blog/mp-sfm/>
> 102. **REWIND**: <https://jyunlee.github.io/projects/rewind/>
> 103. **GENMO**: <https://research.nvidia.com/labs/dair/genmo/>
> 104. **Matrix3D**: <https://nju-3dv.github.io/projects/matrix3d/>
> 105. **EVA**: <https://vcai.mpi-inf.mpg.de/projects/EVA/>
> 106. **PromptHMR**: <https://yufu-wang.github.io/phmr-page/>
> 107. **SceneTracker**: <https://mp.weixin.qq.com/s/XRo605YrAKbQPlQP2dELsg>
> 108. **UP-SLAM**: <https://aczheng-cai.github.io/up_slam.github.io/>
> 109. **OphNet-3D**: <https://ophnet-3d.github.io/>
> 110. **Hand-held Object Reconstruction from RGB Video with Dynamic Interaction**: <https://east-j.github.io/dynhor/>
> 111. **Embodied Scene-aware Human Pose Estimation**: <https://www.zhengyiluo.com/projects/embodied_pose/>
> 112. **PEVA**: <https://dannytran123.github.io/PEVA/>
> 113. **SpatialTracker**: <https://zju3dv.github.io/SpaTracker/>
> 114. **SpatialTrackerV2**: <https://spatialtracker.github.io/>
> 115. **LiteReality**: <https://litereality.github.io/>
> 116. **GaVS**: <https://sinoyou.github.io/gavs/>
> 117. **Cameras as Relative Positional Encoding**: <https://www.liruilong.cn/prope/>
> 118. **Real2Render2Real**: <https://real2render2real.com/>
> 119. **ViPE**: <https://research.nvidia.com/labs/toronto-ai/vipe/>
> 120. **OmniMap**: <https://omni-map.github.io/>
> 121. **TTT3R**: <https://rover-xingyu.github.io/TTT3R/>
> 122. **Instant4D**: <https://instant4d.github.io/>
> 123. **ARTDECO**: <https://city-super.github.io/artdeco/>
> 124. **Trace Anything**: <https://trace-anything.github.io/>
> 125. **NExF**: <https://m-niemeyer.github.io/nexf/index.html>
> 126. **PAGE-4D**: <https://page-4d.github.io/anonymous-submission/>
> 127. **VINGS-MONO**: <https://vings-mono.github.io/>
> 128. **CARI4D**: <https://nvlabs.github.io/CARI4D/>
> 129. **D4RT**: <https://deepmind.google/blog/d4rt-teaching-ai-to-see-the-world-in-four-dimensions/>
> 130. **EventNeuS**: <https://4dqv.mpi-inf.mpg.de/EventNeuS/>
> 131. **EVolSplat4D**: <https://xdimlab.github.io/EVolSplat4D/>
> 132. **MetricAnything**: <https://metric-anything.github.io/metric-anything-io/>
> 133. **Trackers**: <https://trackers.roboflow.com/latest/>

## (2) Tools

> 1. **JSON For You**: <https://json4u.cn/>
> 2. **Readdy**: <https://readdy.ai/>
> 3. **APILayer**: <https://apilayer.com/?utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo>
> 4. **Free for Developers**: <https://free-for.dev/#/>
> 5. **System for AI**: <https://microsoft.github.io/AI-System/>
> 6. **Prompt Optimizer**: <https://prompt.always200.com/>
> 7. **Dotfiles**: <https://dotfiles.github.io/>
> 8. **Hacker News**: <https://news.ycombinator.com/>
> 9. **NewsNow**: <https://newsnow.busiyi.world/>
> 10. **Inspira UI**: <https://inspira-ui.com/>
> 11. **kkFileView**: <https://kkview.cn/zh-cn/index.html>
> 12. **numpy-ml**: <https://numpy-ml.readthedocs.io/en/latest/>
> 13. **PDFMathTranslate**: <https://pdf2zh.com/>
> 14. **DeepChem**: <https://deepchem.io/>
> 15. **OpenAI Cookbook**: <https://cookbook.openai.com/>
> 16. **Nodezator**: <https://nodezator.com/>
> 17. **Micro**: <https://micro-editor.github.io/>
> 18. **Convertio**: <https://developers.convertio.co/>
> 19. **OpenRL**: <https://openrl-docs.readthedocs.io/en/latest/>
> 20. **ALLinSSL**: <https://allinssl.com/>
> 21. **Material for MkDocs**: <https://squidfunk.github.io/mkdocs-material/>
> 22. **Hoppscotch**: <https://hoppscotch.io/>
> 23. **SimpleMindMap**: <https://wanglin2.github.io/mind-map-docs/>
> 24. **GrapesJS**: <https://grapesjs.com/>
> 25. **Kuzu Graph Database**: <https://kuzudb.com/>
> 26. **nb**: <https://xwmx.github.io/nb/>
> 27. **Vosk**: <https://alphacephei.com/vosk/>
> 28. **Machinekit**: <https://machinekoder.com/machinekit-ros-industrial-robot/>
> 29. **MAZANOKE**: <https://mazanoke.com/>
> 30. **XFlow**: <https://xrender.fun/xflow>
> 31. **Viser**: <https://viser.studio/main/>
> 32. **MediaCrawler**: <https://nanmicoder.github.io/MediaCrawler/>
> 33. **Mechaverse**: <https://mechaverse.dev/>
> 34. **Zenoh**: <https://zenoh.io/>
> 35. **Rerun**: <https://rerun.io/>
> 36. **Foxglove**: <https://foxglove.dev/>
> 37. **MCAP**: <https://mcap.dev/>
> 38. **ZLMediaKit**: <https://docs.zlmediakit.com/>

Continue reading [Papers](Papers.md){:.heading.flip-title}
{:.read-more}
