---
layout: page
title: Projects
description: >
  Archive list for awesome projects.
# logo:
# theme_color:
# accent_color:
accent_image:
  background: url('/assets/images/resources/sidebar-resources.jpg') center/cover
  overlay: true
# image:
#   path:
#   srcset:
#     1024w:
#     512w:
#     256w:

# permalink: /contents/resources/
# show_collection: resources
# selected_projects:
# projects_page:
# selected_posts:
# posts_page:
# related_posts:
# redirect_from:
# excerpt_separator:
last_modified_at: 2024-11-19

hide_description: true
hide_image: false
hide_last_modified: false
invert_sidebar: false
cover: false
no_groups: true
no_link_title: false
no_excerpt: false
no_third_column: true
sitemap: true
comments: true
featured: false
---

- Table of Contents
{:toc}

# Artificial Intelligence

## (1) LMs

> 1. **The Rhythm In Anything**: <https://oreillyp.github.io/tria/>
> 2. **DriveDreamer4D**: <https://drivedreamer4d.github.io/>
> 3. **TokenFormer**: <https://haiyang-w.github.io/tokenformer.github.io/>
> 4. **hertz-dev**: <https://si.inc/hertz-dev/>
> 5. **PointLLM**: <https://runsenxu.com/projects/PointLLM/>
> 6. **Aria**: <https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model>
> 7. **SiT**: <https://scalable-interpolant.github.io/>
> 8. **Rhymes AI**: <https://rhymes.ai/>
> 9. **nemotron-4-340b-instruct**: <https://build.nvidia.com/nvidia/nemotron-4-340b-instruct>
> 10. **EgoLM**: <https://hongfz16.github.io/projects/EgoLM>
> 11. **Small Language Models (SLM)**: <https://www.jetson-ai-lab.com/tutorial_slm.html>
> 12. **MiniMind**: <https://jingyaogong.github.io/minimind/>
> 13. **ell**: <https://docs.ell.so/#>
> 14. **CogVLM2-Video**: <https://cogvlm2-video.github.io/>
> 15. **InternVL**: <https://internvl.github.io/>
> 16. **Chroma**: <https://generatebiomedicines.com/chroma>
> 17. **GameNGen**: <https://gamengen.github.io/>
> 18. **Husky**: <https://agent-husky.github.io/>
> 19. **Sapiens**: <https://about.meta.com/realitylabs/codecavatars/sapiens>
> 20. **MeshFormer**: <https://meshformer3d.github.io/>
> 21. **Genie**: <https://sites.google.com/view/genie-2024/home>
> 22. **Llama Tour**: <https://llamatutor.together.ai/>
> 23. **CogVideo**: <https://cogvideo.pka.moe/>
> 24. **Awesome ChatGPT Prompts**: <https://prompts.chat/>
> 25. **Opening up ChatGPT**: <https://opening-up-chatgpt.github.io/>
> 26. **AI Home Tab**: <https://aihometab.com/>
> 27. **Groq**: <https://groq.com/>
> 28. **NextChat**: <https://nextchat.dev/>
> 29. **MaxKB**: <https://maxkb.cn/>
> 30. **OpenDatalab**: <https://opendatalab.com/OpenSourceTools>
> 31. **Segment Anything Model 2 (SAM 2)**: <https://ai.meta.com/sam2/>
> 32. **LLaMA-Factory**: <https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html>
> 33. **FunAudioLLM**: <https://funaudiollm.github.io/>
> 34. **KLING**: <https://kling.kuaishou.com/>
> 35. **AlphaGeometry**: <https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/>
> 36. **Prompt Engineering Guide**: <https://www.promptingguide.ai/>
> 37. **AskManyAI**: <https://askmanyai.cn/login>
> 38. **Meet PathChat 2**: <https://www.modella.ai/intro.html>
> 39. **PaintsUndo**: <https://lllyasviel.github.io/pages/paints_undo/>
> 40. **Artificial Analysis**: <https://artificialanalysis.ai/>
> 41. **Transformer Explainer**: <https://poloclub.github.io/transformer-explainer/>
> 42. **Moshi**: <https://moshi.chat/?queue_id=talktomoshi>
> 43. **AI Graveyard**: <https://dang.ai/ai-graveyard>
> 44. **GraphRAG**: <https://microsoft.github.io/graphrag/>
> 45. **Luma Dream Machine**: <https://lumalabs.ai/dream-machine>
> 46. **Open-Sora**: <https://hpcaitech.github.io/Open-Sora/>
> 47. **Cambrian-1**: <https://cambrian-mllm.github.io/>
> 48. **PaLM-E**: <https://palm-e.github.io/>
> 49. **Meta Prompting for AI Systems**: <https://meta-prompting.github.io/>
> 50. **SITUATIONAL AWARENESS**: <https://situational-awareness.ai/>
> 51. **ChatTTS**: <https://chattts.com/>
> 52. **KAN**: <https://kindxiaoming.github.io/pykan/>
> 53. **LLM Visualization**: <https://bbycroft.net/llm>
> 54. **MLX**: <https://ml-explore.github.io/mlx/build/html/index.html>
> 55. **COSTAR Prompt Engineering**: <https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875>
> 56. **Cohere**: <https://cohere.com/>
> 57. **DeepSpeed**: <https://www.deepspeed.ai/>
> 58. **AI Mind**: <https://www.aimind.so/>
> 59. **flowith**: <https://flowith.io/conv/51c05bc8-92f3-4644-869d-35fd22dbfb71>
> 60. **LeanDojo**: <https://leandojo.org/>
> 61. **GPT4All**: <https://www.nomic.ai/gpt4all>
> 62. **MiniGPT4-Video**: <https://vision-cair.github.io/MiniGPT4-video/>
> 63. **Mira**: <https://mira-space.github.io/>
> 64. **ModelScope**: <https://www.modelscope.cn/home>
> 65. **Osmo**: <https://www.osmo.ai/>
> 66. **Hume**: <https://www.hume.ai/>
> 67. **Reka**: <https://www.reka.ai/>
> 68. **Suno**: <https://suno.com/>
> 69. **Kimi**: <https://kimi.moonshot.cn/>
> 70. **SeamlessM4T**: <https://ai.meta.com/blog/seamless-m4t/>
> 71. **ElevenLabs**: <https://elevenlabs.io/>
> 72. **ChatLaw**: <https://chatlaw.cloud/>
> 73. **Gemma**: <https://ai.google.dev/gemma?hl=zh-cn>
> 74. **Stable Diffusion Art**: <https://stable-diffusion-art.com/comfyui/>
> 75. **OpenRouter**: <https://openrouter.ai/>
> 76. **ChatRTX**: <https://www.nvidia.com/en-us/ai-on-rtx/chatrtx/>
> 77. **Runway AI**: <https://runwayml.com/>
> 78. **Stable Video**: <https://www.stablevideo.com/welcome>
> 79. **Stability AI**: <https://stability.ai/>
> 80. **Multi-Agent Transformer**: <https://sites.google.com/view/multi-agent-transformer>
> 81. **DreamerV3**: <https://danijar.com/project/dreamerv3/>
> 82. **Imagen 2**: <https://deepmind.google/technologies/imagen-2/>
> 83. **Gemini Models**: <https://deepmind.google/technologies/gemini/#introduction>
> 84. **Anthropic AI**: <https://www.anthropic.com/>
> 85. **Grok**: <https://x.ai/>
> 86. **LLaVA**: <https://llava-vl.github.io/>
> 87. **Speaking AI**: <https://speaking.ai/>
> 88. **GPT-4 Is Too Smart To Be Safe**: <https://llmcipherchat.github.io/>
> 89. **Character AI**: <https://character.ai/>

## (2) Agents

> 1. **UFO**: <https://github.com/microsoft/UFO>
> 2. **ell**: <https://github.com/MadcowD/ell>
> 3. **SWE-agent**: <https://github.com/princeton-nlp/SWE-agent>
> 4. **autogen**: <https://github.com/microsoft/autogen>
> 5. **CharacterGen**: <https://github.com/zjp-shadow/CharacterGen>
> 6. **Qwen2.5-Coder**: <https://qwenlm.github.io/zh/blog/qwen2.5-coder/>
> 7. **Skyvern**: <https://www.skyvern.com/>
> 8. **Ichigo**: <https://github.com/homebrewltd/ichigo>
> 9. **AI for Grant Writing**: <https://www.lizseckel.com/ai-for-grant-writing/>
> 10. **Large Language Model Agents**: <https://llmagents-learning.org/f24>
> 11. **OpenAGI**: <https://openagi.aiplanet.com/>
> 12. **HyperWrite**: <https://www.hyperwriteai.com/>
> 13. **FastGPT**: <https://tryfastgpt.ai/>
> 14. **ADAS**: <https://www.shengranhu.com/ADAS/>
> 15. **AI Town**: <https://www.convex.dev/ai-town>
> 16. **Comflowy**: <https://www.comflowy.com/>
> 17. **Altera**: <https://altera.al/>
> 18. **Firecrawl**: <https://www.firecrawl.dev/>
> 19. **Artificial LIfe ENvironment (ALIEN)**: <https://www.alien-project.org/index.html>
> 20. **AutoGen Studio 2.0**: <https://autogen-studio.com/>
> 21. **SuperCraft**: <https://supercraft.ai/>
> 22. **Pipecat**: <https://www.pipecat.ai/>
> 23. **Neo4j**: <https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/>
> 24. **Lumina**: <https://www.lumina.sh/c5bbe32b-4fb7-476a-81aa-fe269f67f283?ref=www.lumina-chat.com>
> 25. **RAGFlow**: <https://ragflow.io/>
> 26. **OmniParse**: <https://docs.cognitivelab.in/>
> 27. **Supermemory**: <https://supermemory.ai/>
> 28. **MindSearch**: <https://mindsearch.netlify.app/>
> 29. **ChatDev**: <https://chatdev.toscl.com/>
> 30. **LlamaCoder**: <https://llamacoder.together.ai/>
> 31. **GPTs Works**: <https://gpts.works/>
> 32. **V2A-Mapper**: <https://v2a-mapper.github.io/>
> 33. **MultiOn**: <https://www.multion.ai/>
> 34. **ThinkAny**: <https://thinkany.ai/zh>
> 35. **Mem0**: <https://docs.mem0.ai/overview>
> 36. **Cradle**: <https://baai-agents.github.io/Cradle/>
> 37. **Devv**: <https://devv.ai/zh>
> 38. **Co-STORM**: <https://storm.genie.stanford.edu/>
> 39. **Bubble**: <https://bubble.io/>
> 40. **Humanize AI text**: <https://www.humanizeai.pro/>
> 41. **Aider**: <https://aider.chat/>
> 42. **Agently AI**: <https://agently.tech/>
> 43. **DeepSeek Coder**: <https://deepseekcoder.github.io/>
> 44. **AgentScope**: <https://doc.agentscope.io/en/index.html>
> 45. **CrewAI**: <https://docs.crewai.com/introduction>
> 46. **Humaan AI**: <https://humaan.ai/>
> 47. **FlowiseAI**: <https://flowiseai.com/>
> 48. **Chainlit**: <https://docs.chainlit.io/get-started/overview>
> 49. **Phidata**: <https://docs.phidata.com/agents>
> 50. **Lepton AI**: <https://www.lepton.ai/>
> 51. **AutoGPT**: <https://agpt.co/>
> 52. **MetaGPT**: <https://www.deepwisdom.ai/>
> 53. **LangGraph**: <https://langchain-ai.github.io/langgraph/>
> 54. **HyperWrite**: <https://www.hyperwriteai.com/>
> 55. **QAnything**: <https://qanything.ai/>
> 56. **Synthflow AI Voice Assistants**: <https://synthflow.ai/>
> 57. **Tavily**: <https://tavily.com/>
> 58. **Dify**: <https://dify.ai/>
> 59. **LangChain**: <https://www.langchain.com/>
> 60. **LlamaIndex**: <https://www.llamaindex.ai/>
> 61. **SWE-agent**: <https://swe-agent.com/>
> 62. **MemGPT**: <https://memgpt.ai/>
> 63. **SIMA**: <https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/>
> 64. **Durable**: <https://durable.co/>
> 65. **Cognition**: <https://www.cognition.ai/>
> 66. **LTX Studio**: <https://ltx.studio/>
> 67. **vellum**: <https://www.vellum.ai/>
> 68. **DetectGPT**: <https://detectgpt.ai/index.html>
> 69. **Dora AI**: <https://www.dora.run/ai>
> 70. **An Embodied Generalist Agent in 3D World**: <https://embodied-generalist.github.io/>
> 71. **Voyager**: <https://voyager.minedojo.org/>
> 72. **XAgent**: <https://xagent-doc.readthedocs.io/en/latest/>

## (3) AIGC

> 1. **MikuDance**: <https://kebii.github.io/MikuDance/>
> 2. **DanceFusion**: <https://th-mlab.github.io/DanceFusion/>
> 3. **StdGEN**: <https://stdgen.github.io/>
> 4. **URAvatar**: <https://junxuan-li.github.io/urgca-website/>
> 5. **Towards High-fidelity Head Blending with Chroma Keying for Industrial Applications**: <https://hahminlew.github.io/changer/>
> 6. **ReCapture**: <https://generative-video-camera-controls.github.io/>
> 7. **DimensionX**: <https://chenshuo20.github.io/DimensionX/>
> 8. **Fashion-VDM**: <https://johannakarras.github.io/Fashion-VDM/>
> 9. **X-Portrait 2**: <https://byteaigc.github.io/X-Portrait2/>
> 10. **GameGen-X**: <https://gamegen-x.github.io/>
> 11. **HelloMeme**: <https://songkey.github.io/hellomeme/>
> 12. **DreamVideo-2**: <https://dreamvideo2.github.io/>
> 13. **VidPanos**: <https://vidpanos.github.io/>
> 14. **DAWN**: <https://hanbo-cheng.github.io/DAWN/>
> 15. **Interstice**: <https://www.interstice.cloud/>
> 16. **LongVU**: <https://vision-cair.github.io/LongVU/>
> 17. **DreamCraft3D++**: <https://dreamcraft3dplus.github.io/>
> 18. **Hallo2**: <https://fudan-generative-vision.github.io/hallo2/#/>
> 19. **MVideo**: <https://mvideo-v1.github.io/>
> 20. **UniMuMo**: <https://hanyangclarence.github.io/unimumo_demo/>
> 21. **TextToon**: <https://songluchuan.github.io/TextToon/>
> 22. **eye-contact-correction**: <https://www.sievedata.com/functions/sieve/eye-contact-correction>
> 23. **TANGO**: <https://pantomatrix.github.io/TANGO/>
> 24. **Animate-X**: <https://lucaria-academy.github.io/Animate-X/>
> 25. **ACE**: <https://ali-vilab.github.io/ace-page/>
> 26. **PhysGen**: <https://stevenlsw.github.io/physgen/>
> 27. **EdgeRunner**: <https://research.nvidia.com/labs/dir/edgerunner/>
> 28. **Movie Gen**: <https://ai.meta.com/research/movie-gen/>
> 29. **Inverse Painting**: <https://inversepainting.github.io/>
> 30. **Disco4D**: <https://disco-4d.github.io/>
> 31. **MimicTalk**: <https://mimictalk.github.io/>
> 32. **DIAMOND**: <https://diamond-wm.github.io/>
> 33. **JoyHallo**: <https://jdh-algo.github.io/JoyHallo/>
> 34. **SF3D**: <https://stable-fast-3d.github.io/>
> 35. **GaussianCube**: <https://gaussiancube.github.io/>
> 36. **Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing**: <https://pei-xu.github.io/guitar>
> 37. **SPARK**: <https://kelianb.github.io/SPARK/>
> 38. **LVCD**: <https://luckyhzt.github.io/lvcd>
> 39. **PortraitGen**: <https://ustc3dv.github.io/PortraitGen/>
> 40. **NeRF**: <https://www.matthewtancik.com/nerf>
> 41. **HIT**: <https://hit.is.tue.mpg.de/#video>
> 42. **3DTopia-XL**: <https://3dtopia.github.io/3DTopia-XL/>
> 43. **DrawingSpinUp**: <https://lordliang.github.io/DrawingSpinUp/>
> 44. **Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos**: <https://nowheretrix.github.io/DualGS/>
> 45. **PoseTalk**: <https://junleen.github.io/projects/posetalk/>
> 46. **GVHMR**: <https://zju3dv.github.io/gvhmr/>
> 47. **PersonaTalk**: <https://grisoon.github.io/PersonaTalk/>
> 48. **EscherNet**: <https://kxhit.github.io/EscherNet>
> 49. **Gaussian Garments**: <https://ribosome-rbx.github.io/Gaussian-Garments/>
> 50. **CyberHost**: <https://cyberhost.github.io/>
> 51. **Draw an Audio**: <https://yannqi.github.io/Draw-an-Audio/>
> 52. **3DGRT**: <https://gaussiantracer.github.io/>
> 53. **ViewCrafter**: <https://drexubery.github.io/ViewCrafter/>
> 54. **ReconX**: <https://liuff19.github.io/ReconX/>
> 55. **FaceSwap**: <https://faceswap.so/>
> 56. **Civitai**: <https://civitai.com/>
> 57. **Loopy**: <https://loopyavatar.github.io/>
> 58. **PhotoMaker**: <https://huggingface.co/spaces/TencentARC/PhotoMaker>
> 59. **InterTrack**: <https://virtualhumans.mpi-inf.mpg.de/InterTrack/>
> 60. **Build-A-Scene**: <https://abdo-eldesokey.github.io/build-a-scene/>
> 61. **MagicMan**: <https://thuhcsi.github.io/MagicMan/>
> 62. **LayerPano3D**: <https://ys-imtech.github.io/projects/LayerPano3D/>
> 63. **DreamCinema**: <https://liuff19.github.io/DreamCinema/>
> 64. **TurboEdit**: <https://betterze.github.io/TurboEdit/>
> 65. **Scaling Up Dynamic Human-Scene Interaction Modeling**: <https://jnnan.github.io/trumans/>
> 66. **DiPIR**: <https://research.nvidia.com/labs/toronto-ai/DiPIR/>
> 67. **TurboEdit**: <https://turboedit-paper.github.io/>
> 68. **DEGAS**: <https://initialneil.github.io/DEGAS>
> 69. **Audio Match Cutting**: <https://denfed.github.io/audiomatchcut/>
> 70. **Subsurface Scattering for Gaussian Splatting**: <https://sss.jdihlmann.com/>
> 71. **Tavus**: <https://www.tavus.io/>
> 72. **Media2Face**: <https://sites.google.com/view/media2face>
> 73. **FruitNeRF**: <https://meyerls.github.io/fruit_nerf/>
> 74. **Puppet-Master**: <https://vgg-puppetmaster.github.io/>
> 75. **ernerf**: <https://zhuanlan.zhihu.com/p/675131165>
> 76. **An Object is Worth 64x64 Pixels**: <https://omages.github.io/>
> 77. **VideoDoodles**: <https://em-yu.github.io/research/videodoodles/>
> 78. **ReSyncer**: <https://guanjz20.github.io/projects/ReSyncer/>
> 79. **KEEP**: <https://jnjaby.github.io/projects/KEEP/>
> 80. **MoMask**: <https://ericguo5513.github.io/momask/>
> 81. **Tora**: <https://ali-videoai.github.io/tora_video/>
> 82. **EmoTalk3D**: <https://nju-3dv.github.io/projects/EmoTalk3D/>
> 83. **Cycle3D**: <https://pku-yuangroup.github.io/Cycle3D/>
> 84. **Swapface**: <https://www.swapface.org/#/home>
> 85. **ExAvatar**: <https://mks0601.github.io/ExAvatar/>
> 86. **MotionClone**: <https://bujiazi.github.io/motionclone.github.io/>
> 87. **Outfit Anyone**: <https://humanaigc.github.io/outfit-anyone/>
> 88. **Vidu**: <https://www.vidu.studio/zh>
> 89. **Temporal Residual Jacobians for Rig-free Motion Transfer**: <https://temporaljacobians.github.io/>
> 90. **Cinemo**: <https://maxin-cn.github.io/cinemo_project/>
> 91. **HumanVid**: <https://humanvid.github.io/>
> 92. **Diffree**: <https://opengvlab.github.io/Diffree/>
> 93. **SMooDi**: <https://neu-vi.github.io/SMooDi/>
> 94. **Lite2Relight**: <https://vcai.mpi-inf.mpg.de/projects/Lite2Relight/>
> 95. ***Noise Calibration**: <https://yangqy1110.github.io/NC-SDEdit/>
> 96. **Diff-Foley**: <https://diff-foley.github.io/>
> 97. **Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity**: <https://maskvat.github.io/>
> 98. **vozo**: <https://www.vozo.ai/>
> 99. **MoA**: <https://snap-research.github.io/mixture-of-attention/>
> 100. **Magic Insert**: <https://magicinsert.github.io/>
> 101. **CharacterGen**: <https://charactergen.github.io/>
> 102. **Live2Diff**: <https://live2diff.github.io/>
> 103. **RodinHD**: <https://rodinhd.github.io/>
> 104. **StickerBaker**: <https://stickerbaker.com/>
> 105. **Still-Moving**: <https://still-moving.github.io/>
> 106. **Tripo3D**: <https://www.tripo3d.ai/>
> 107. **Hedra**: <https://www.hedra.com/>
> 108. **RenderNet**: <https://rendernet.ai/index.html>
> 109. **LivePortrait**: <https://liveportrait.github.io/>
> 110. **Image Conductor**: <https://liyaowei-stu.github.io/project/ImageConductor/>
> 111. **MOTIA**: <https://be-your-outpainter.github.io/>
> 112. **Meta 3D AssetGen**: <https://assetgen.github.io/>
> 113. **Portrait3D**: <https://jinkun-hao.github.io/Portrait3D/>
> 114. **GaussianDreamerPro**: <https://taoranyi.com/gaussiandreamerpro/>
> 115. **MimicMotion**: <https://tencent.github.io/MimicMotion/>
> 116. **Text-Animator**: <https://laulampaul.github.io/text-animator.html>
> 117. **YouDream**: <https://youdream3d.github.io/>
> 118. **FoleyCrafter**: <https://foleycrafter.github.io/>
> 119. **Wonder Studio**: <https://wonderdynamics.com/>
> 120. **TripoSR**: <https://stability.ai/news/triposr-3d-generation>
> 121. **MeshAnything**: <https://buaacyw.github.io/mesh-anything/>
> 122. **EvTexture**: <https://dachunkai.github.io/evtexture.github.io/>
> 123. **ScoreHypo**: <https://xy02-05.github.io/ScoreHypo/>
> 124. **AniFusion**: <https://anifusion.ai/>
> 125. **Style-NeRF2NeRF**: <https://haruolabs.github.io/style-n2n/>
> 126. **4K4DGen**: <https://4k4dgen.github.io/>
> 127. **ExVideo**: <https://ecnu-cilab.github.io/ExVideoProjectPage/>
> 128. **Diffutoon**: <https://ecnu-cilab.github.io/DiffutoonProjectPage/>
> 129. **Holistic-Motion2D**: <https://holistic-motion2d.github.io/>
> 130. **FaceFusion**: <https://docs.facefusion.io/>
> 131. **AnyFit**: <https://colorful-liyu.github.io/anyfit-page/>
> 132. **PuzzleFusion++**: <https://puzzlefusion-plusplus.github.io/>
> 133. **UniAnimate**: <https://unianimate.github.io/>
> 134. **ChronoDepth**: <https://jhaoshao.github.io/ChronoDepth/>
> 135. **Unique3D**: <https://wukailu.github.io/Unique3D/>
> 136. **GECO**: <https://cwchenwang.github.io/geco/>
> 137. **T2V-Turbo**: <https://t2v-turbo.github.io/>
> 138. **EasyAnimate**: <https://easyanimate.github.io/>
> 139. **ZeroSmooth**: <https://ssyang2020.github.io/zerosmooth.github.io/>
> 140. **MVSGaussian**: <https://mvsgaussian.github.io/>
> 141. **CityGaussian**: <https://dekuliutesla.github.io/citygs/>
> 142. **MOFA-Video**: <https://myniuuu.github.io/MOFA_Video/>
> 143. **VividDream**: <https://vivid-dream-4d.github.io/>
> 144. **Motion2VecSets**: <https://vveicao.github.io/projects/Motion2VecSets/>
> 145. **MultiPly**: <https://eth-ait.github.io/MultiPly/>
> 146. **Neural Gaffer**: <https://neural-gaffer.github.io/>
> 147. **I4VGen**: <https://xiefan-guo.github.io/i4vgen/>
> 148. **ToonCrafter**: <https://doubiiu.github.io/projects/ToonCrafter/>
> 149. **2DGS**: <https://surfsplatting.github.io/>
> 150. **Collaborative Video Diffusion**: <https://collaborativevideodiffusion.github.io/>
> 151. **Looking Backward**: <https://jeff-liangf.github.io/projects/streamv2v/>
> 152. **SadTalker**: <https://sadtalker.github.io/>
> 153. **VividTalk**: <https://humanaigc.github.io/vivid-talk/>
> 154. **I2VEdit**: <https://i2vedit.github.io/>
> 155. **MagicPose4D**: <https://boese0601.github.io/magicpose4d/>
> 156. **Generative Camera Dolly**: <https://gcd.cs.columbia.edu/>
> 157. **ReVideo**: <https://mc-e.github.io/project/ReVideo/>
> 158. **Text-to-Vector Generation with Neural Path Representation**: <https://intchous.github.io/T2V-NPR/>
> 159. **CAT3D**: <https://cat3d.github.io/>
> 160. **StructLDM**: <https://taohuumd.github.io/projects/StructLDM/>
> 161. **AniTalker**: <https://x-lance.github.io/AniTalker/>
> 162. **Dual3D**: <https://dual3d.github.io/>
> 163. **X-Oscar**: <https://xmu-xiaoma666.github.io/Projects/X-Oscar/>
> 164. **HiDiffusion**: <https://hidiffusion.github.io/>
> 165. **Tunnel Try-on**: <https://mengtingchen.github.io/tunnel-try-on-page/>
> 166. **STAG4D**: <https://nju-3dv.github.io/projects/STAG4D/>
> 167. **GS-LRM**: <https://sai-bi.github.io/project/gs-lrm/>
> 168. **GScream**: <https://w-ted.github.io/publications/gscream/>
> 169. **MotionMaster**: <https://sjtuplayer.github.io/projects/MotionMaster/>
> 170. **PhysDreamer**: <https://physdreamer.github.io/>
> 171. **SwapAnything**: <https://swap-anything.github.io/>
> 172. **MagicPose**: <https://boese0601.github.io/magicdance/>
> 173. **ZeST**: <https://ttchengab.github.io/zest/>
> 174. **EMOPortraits**: <https://neeek2303.github.io/EMOPortraits/>
> 175. **StoryDiffusion**: <https://storydiffusion.github.io/>
> 176. **Automatic Controllable Colorization via Imagination**: <https://xy-cong.github.io/imagine-colorization/>
> 177. **MaPa**: <https://zhanghe3z.github.io/MaPa/>
> 178. **EMO**: <https://humanaigc.github.io/emote-portrait-alive/>
> 179. **StreamingT2V**: <https://streamingt2v.github.io/>
> 180. **IDM-VTON**: <https://idm-vton.github.io/>
> 181. **IntrinsicAnything**: <https://zju3dv.github.io/IntrinsicAnything/>
> 182. **Interactive3D**: <https://interactive-3d.github.io/>
> 183. **in2IN**: <https://pabloruizponce.github.io/in2IN/>
> 184. **synthesia**: <https://www.synthesia.io/>
> 185. **DreamWalk**: <https://mshu1.github.io/dreamwalk.github.io/>
> 186. **MagicTime**: <https://pku-yuangroup.github.io/MagicTime/>
> 187. **Gaussian Head Avatar**: <https://yuelangx.github.io/gaussianheadavatar/>
> 188. **Champ**: <https://fudan-generative-vision.github.io/champ/#/>
> 189. **ObjectDrop**: <https://objectdrop.github.io/>
> 190. **HeyGen**: <https://www.heygen.com/>
> 191. **DomoAI**: <https://domoai.app/>
> 192. **Pebblely**: <https://pebblely.com/>
> 193. **Photorealistic Video Generation with Diffusion Models**: <https://walt-video-diffusion.github.io/>
> 194. **3D-GPT**: <https://chuny1.github.io/3DGPT/3dgpt.html>
> 195. **SynthID**: <https://deepmind.google/technologies/synthid/>
> 196. **Palette**: <https://palette.fm/>

# Robotics

## (1) Hardware

> 1. **Zeroth**: <https://docs.zeroth.bot/>
> 2. **Power-over-Skin**: <https://www.figlab.com/research/2024/poweroverskin>
> 3. **RoboDuet**: <https://locomanip-duet.github.io/>
> 4. **The snake that saves lives**: <https://ethz.ch/en/news-and-events/eth-news/news/2024/11/the-snake-that-saves-lives.html>
> 5. **XGO-Rider**: <https://www.kickstarter.com/projects/xgorobot/xgo-rider-desktop-two-wheel-legged-robot-with-ai>
> 6. **7X**: <https://7xr.tech/>
> 7. **Berkeley Humanoid**: <https://berkeley-humanoid.com/>
> 8. **Torobo**: <https://robotics.tokyo/products/torobo/>
> 9. **DexHand**: <https://www.dexhand.org/>
> 10. **NAVER LABS**: <https://www.naverlabs.com/>
> 11. **Surena Humanoid Robot**: <https://surenahumanoid.com/>

## (2) Software

> 1. **ROS2**: <https://docs.ros.org/en/jazzy/index.html>
> 2. **SAFER-Splat**: <https://chengine.github.io/safer-splat/>
> 3. **Neural MP**: <https://mihdalal.github.io/neuralmotionplanner/>
> 4. **AirSLAM**: <https://xukuanhit.github.io/airslam/>
> 5. **SimTK**: <https://simtk.org/>
> 6. **MyoSuite**: <https://sites.google.com/view/myosuite/myosuite>
> 7. **Hyfydy**: <https://hyfydy.com/>
> 8. **LVCP**: <https://sites.google.com/view/lvcp>
> 9. **Hello**: <https://www.hello-algo.com/>
> 10. **Skild AI**: <https://www.skild.ai/>
> 11. **NVIDIA Project GR00T**: <https://developer.nvidia.com/project-gr00t>
> 12. **OpenWorm**: <https://openworm.org/>
> 13. **NVIDIA Isaac ROS**: <https://nvidia-isaac-ros.github.io/>

# AIRobotics

## (1) Robot Learning

> 1. **UMI(Universal Manipulation Interface)**: <https://umi-gripper.github.io/>
> 2. **PSAG**: <https://www.jianrenw.com/PSAG/>
> 3. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 4. **RGBManip**: <https://rgbmanip.github.io/>
> 5. **RoboStudio**: <https://robostudioapp.com/>
> 6. **ReKep**: <https://rekep-robot.github.io/>
> 7. **DeformGS**: <https://deformgs.github.io/>
> 8. **NeuralFeels**: <https://suddhu.github.io/neural-feels/>
> 9. **ALOHA**: <https://tonyzhaozh.github.io/aloha/>
> 10. **LucidSim**: <https://lucidsim.github.io/>
> 11. **RoPotter**: <https://robot-pottery.github.io/>
> 12. **HOVER**: <https://hover-versatile-humanoid.github.io/>
> 13. **DexMimicGen**: <https://dexmimicgen.github.io/>
> 14. **Eurekaverse**: <https://eureka-research.github.io/eurekaverse/>
> 15. **HIL-SERL**: <https://hil-serl.github.io/>
> 16. **OrbitGrasp**: <https://orbitgrasp.github.io/>
> 17. **3D-ViTac**: <https://binghao-huang.github.io/3D-ViTac/>
> 18. **Physical Intelligence**: <https://www.physicalintelligence.company/blog/pi0>
> 19. **HuDOR**: <https://object-rewards.github.io/>
> 20. **LAPA**: <https://latentactionpretraining.github.io/>
> 21. **ManipGen**: <https://mihdalal.github.io/manipgen/>
> 22. **Robots Pre-Train Robots**: <https://robots-pretrain-robots.github.io/>
> 23. **ARNOLD**: <https://arnold-benchmark.github.io/>
> 24. **GPT-4V(ision) for Robotics**: <https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/>
> 25. **VoxAct-B**: <https://voxact-b.github.io/>
> 26. **ARCap**: <https://stanford-tml.github.io/ARCap/>
> 27. **Harmon**: <https://ut-austin-rpl.github.io/Harmon/>
> 28. **Data Scaling Laws**: <https://data-scaling-laws.github.io/>
> 29. **Dynamic 3D Gaussian Tracking**: <https://gs-dynamics.github.io/>
> 30. **OKAMI**: <https://ut-austin-rpl.github.io/OKAMI/>
> 31. **UniHSI**: <https://xizaoqu.github.io/unihsi/>
> 32. **SDS**: <https://rpl-cs-ucl.github.io/SDSweb/>
> 33. **EgoAllo**: <https://egoallo.github.io/>
> 34. **DART**: <https://zkf1997.github.io/DART/>
> 35. **FürElise**: <https://for-elise.github.io/>
> 36. **PourIt**: <https://hetolin.github.io/PourIt/>
> 37. **Cherrybot**: <https://goodcherrybot.github.io/>
> 38. **AnyCar to Anywhere**: <https://lecar-lab.github.io/anycar/>
> 39. **Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies**: <https://lipschitz-constrained-policy.github.io/>
> 40. **HumanoidOlympics**: <https://humanoidolympics.github.io/>
> 41. **OmniH2O**: <https://omni.human2humanoid.com/>
> 42. **Continuously Improving Mobile Manipulation with Autonomous Real-World RL**: <https://continual-mobile-manip.github.io/>
> 43. **MotIF**: <https://motif-1k.github.io/>
> 44. **Helpful DoggyBot**: <https://helpful-doggybot.github.io/>
> 45. **Blox-Net**: <https://bloxnet.org/>
> 46. **GR-MG**: <https://gr-mg.github.io/>
> 47. **Real-World Cooking Robot System from Recipes**: <https://kanazawanaoaki.github.io/cook-from-recipe-pddl/>
> 48. **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**: <https://gr1-manipulation.github.io/>
> 49. **GR-2**: <https://gr2-manipulation.github.io/>
> 50. **CLoSD**: <https://guytevet.github.io/CLoSD-page/>
> 51. **RDT-1B**: <https://rdt-robotics.github.io/rdt-robotics/>
> 52. **Agile Continuous Jumping in Discontinuous Terrains**: <https://yxyang.github.io/jumping_cod/>
> 53. **Humanoid Manipulation**: <https://humanoid-manipulation.github.io/>
> 54. **Diff-Control**: <https://diff-control.github.io/>
> 55. **Catch It**: <https://mobile-dex-catch.github.io/>
> 56. **Robot See Robot Do**: <https://robot-see-robot-do.github.io/>
> 57. **Gen2Act**: <https://homangab.github.io/gen2act/>
> 58. **Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing**: <https://lecar-lab.github.io/dial-mpc/>
> 59. **ReMEmbR**: <https://nvidia-ai-iot.github.io/remembr/>
> 60. **ReMEmbR**: <https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/>
> 61. **MaskedMimic**: <https://research.nvidia.com/labs/par/maskedmimic/>
> 62. **Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation**: <https://human2humanoid.com/>
> 63. **Theia**: <https://theia.theaiinstitute.com/>
> 64. **Robot Motion Diffusion Model: Motion Generation for Robotic Characters**: <https://la.disneyresearch.com/publication/robot-motion-diffusion-model-motion-generation-for-robotic-characters/>
> 65. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 66. **One-shot Video Imitation via Parameterized Symbolic Abstraction Graphs**: <https://www.jianrenw.com/PSAG/>
> 67. **RGBManip**: <https://rgbmanip.github.io/>
> 68. **ALOHA Unleashed**: <https://aloha-unleashed.github.io/>
> 69. **PianoMime**: <https://pianomime.github.io/>
> 70. **Robot Utility Models**: <https://robotutilitymodels.com/#>
> 71. **Polaris**: <https://star-uu-wang.github.io/Polaris/>
> 72. **RoboStudio**: <https://robostudioapp.com/>
> 73. **ICRT**: <https://icrt.dev/>
> 74. **SkillMimic**: <https://ingrid789.github.io/SkillMimic/>
> 75. **VoicePilot**: <https://sites.google.com/andrew.cmu.edu/voicepilot/>
> 76. **ReKep**: <https://rekep-robot.github.io/>
> 77. **DeformGS**: <https://deformgs.github.io/>
> 78. **ATM**: <https://xingyu-lin.github.io/atm/>
> 79. **Universal Manipulation Interface**: <https://umi-gripper.github.io/>
> 80. **ACE**: <https://ace-teleop.github.io/>
> 81. **Summarize the Past to Predict the Future**: <https://eth-ait.github.io/transfusion-proj/>
> 82. **TacSL**: <https://iakinola23.github.io/tacsl/>
> 83. **RoCo**: <https://project-roco.github.io/>
> 84. **UniT**: <https://zhengtongxu.github.io/unifiedtactile.github.io/>
> 85. **PhysHOI**: <https://wyhuai.github.io/physhoi-page/>
> 86. **UMI on Legs**: <https://umi-on-legs.github.io/>
> 87. **Lifelike Agility and Play in Quadrupedal Robots**: <https://tencent-roboticsx.github.io/lifelike-agility-and-play/>
> 88. **RoboCasa**: <https://robocasa.ai/>
> 89. **GET-Zero**: <https://get-zero-paper.github.io/>
> 90. **DextrAH-G**: <https://sites.google.com/view/dextrah-g>
> 91. **Surgical Robot Transformer**: <https://surgical-robot-transformer.github.io/>
> 92. **Grasping Diverse Objects with Simulated Humanoids**: <https://www.zhengyiluo.com/Omnigrasp-Site/>
> 93. **PoliFormer**: <https://poliformer.allen.ai/>
> 94. **This&That**: <https://cfeng16.github.io/this-and-that/>
> 95. **RoboCat**: <https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/>
> 96. **RoboGen**: <https://robogen-ai.github.io/>
> 97. **EquiBot**: <https://equi-bot.github.io/>
> 98. **Policy Composition From and For Heterogeneous Robot Learning**: <https://liruiw.github.io/policycomp/>
> 99. **GenSim**: <https://gen-sim.github.io/>
> 100. **Bunny-VisionPro**: <https://dingry.github.io/projects/bunny_visionpro.html>
> 101. **Open-TeleVision**: <https://robot-tv.github.io/>
> 102. **DexGraspNet**: <https://pku-epic.github.io/DexGraspNet/>
> 103. **Mobile ALOHA**: <https://mobile-aloha.github.io/>
> 104. **OpenVLA**: <https://openvla.github.io/>
> 105. **MS-Human-700**: <https://lnsgroup.cc/research/MS-Human-700>
> 106. **HumanPlus**: <https://humanoid-ai.github.io/>
> 107. **Octo**: <https://octo-models.github.io/>
> 108. **HOI-M3**: <https://juzezhang.github.io/HOIM3_ProjectPage/>
> 109. **DrEureka**: <https://eureka-research.github.io/dr-eureka/>
> 110. **FLD**: <https://sites.google.com/view/iclr2024-fld/home>
> 111. **SATO**: <https://sato-team.github.io/Stable-Text-to-Motion-Framework/>
> 112. **ViPlanner**: <https://leggedrobotics.github.io/viplanner.github.io/>
> 113. **HumanoidBench**: <https://humanoid-bench.github.io/>
> 114. **DexCap**: <https://dex-cap.github.io/>
> 115. **RT-Sketch**: <https://rt-sketch.github.io/>
> 116. **SARA**: <https://sites.google.com/view/rtsara/?pli=1>
> 117. **AutoRT**: <https://auto-rt.github.io/>
> 118. **RT-Trajectory**: <https://rt-trajectory.github.io/>
> 119. **iGibson**: <https://svl.stanford.edu/igibson/>
> 120. **GOAT**: <https://theophilegervet.github.io/projects/goat/>
> 121. **Dynamic Handover**: <https://binghao-huang.github.io/dynamic_handover/>
> 122. **Eureka**: <https://eureka-research.github.io/>
> 123. **Sequential Dexterity**: <https://sequential-dexterity.github.io/>
> 124. **From Text to Motion**: <https://tnoinkwms.github.io/ALTER-LLM/>
> 125. **MimicGen**: <https://mimicgen.github.io/>
> 126. **NOIR**: <https://noir-corl.github.io/>

## (2) Autonomous Driving

## (3) Embodied Intelligence

# Metaverse

## (1) Omniverse

## (2) Digital Twin

# Utilities

## (1) Computer Vision

> 1. **OpenCV**: <https://opencv.org/>
> 2. **roboflow**: <https://roboflow.com/>
> 3. **MoGe**: <https://wangrc.site/MoGePage/>
> 4. **Cloth-Splatting**: <https://kth-rpl.github.io/cloth-splatting/>
> 5. **SpectroMotion**: <https://cdfan0627.github.io/spectromotion/>
> 6. **SMITE**: <https://segment-me-in-time.github.io/>
> 7. **VistaDream**: <https://vistadream-project-page.github.io/index.html>
> 8. **InterMask**: <https://gohar-malik.github.io/intermask/>
> 9. **Dessie**: <https://celiali.github.io/Dessie/>
> 10. **CoTracker3**: <https://cotracker3.github.io/>
> 11. **PointCloud Conditioned Mesh Generation**: <https://research.nvidia.com/labs/dir/edgerunner/gallery/point_cond_4.html>
> 12. **Depth Any Video with Scalable Synthetic Data**: <https://depthanyvideo.github.io/>
> 13. **MonST3R**: <https://monst3r-project.github.io/>
> 14. **EVER**: <https://half-potato.gitlab.io/posts/ever/>
> 15. **CoTracker**: <https://co-tracker.github.io/>
> 16. **WiLoR**: <https://rolpotamias.github.io/WiLoR/>
> 17. **MIMO**: <https://menyifang.github.io/projects/MIMO/index.html>
> 18. **M2Mapping**: <https://jianhengliu.github.io/Projects/M2Mapping/>
> 19. **3D Gaussian Splatting for Real-Time Radiance Field Rendering**: <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>
> 20. **StableNormal**: <https://stable-x.github.io/StableNormal/>
> 21. **EmbodiedSAM**: <https://xuxw98.github.io/ESAM/>
> 22. **Large Étendue 3D Holographic Display with Content-adpative Dynamic Fourier Modulation**: <https://bchao1.github.io/holo_dfm/>
> 23. **Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos**: <https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/>
> 24. **EgoHDM**: <https://handiyin.github.io/EgoHDM/>
> 25. **DepthCrafter**: <https://depthcrafter.github.io/>
> 26. **Spann3R**: <https://hengyiwang.github.io/projects/spanner>
> 27. **OpenIns3D**: <https://zheninghuang.github.io/OpenIns3D/>
> 28. **Bilateral Reference for High-Resolution Dichotomous Image Segmentation**: <https://www.birefnet.top/>
> 29. **ObjectCarver**: <https://objectcarver.github.io/>
> 30. **Improving 2D Feature Representations by 3D-Aware Fine-Tuning**: <https://ywyue.github.io/FiT3D/>
> 31. **Shape of Motion**: <https://shape-of-motion.github.io/>
> 32. **DINO-Tracker**: <https://dino-tracker.github.io/>
> 33. **DiffIR2VR-Zero**: <https://jimmycv07.github.io/DiffIR2VR_web/>
> 34. **Vidu4D**: <https://vidu4d-dgs.github.io/>
> 35. **RaDe-GS**: <https://baowenz.github.io/radegs/>
> 36. **Semantic Gaussians**: <https://sharinka0715.github.io/semantic-gaussians/>
> 37. **FoundationPose**: <https://nvlabs.github.io/FoundationPose/>
> 38. **InstantSplat**: <https://instantsplat.github.io/>
> 39. **GS-Pose**: <https://dingdingcai.github.io/gs-pose/>
> 40. **I'M HOI**: <https://afterjourney00.github.io/IM-HOI.github.io/>
> 41. **MeshLRM**: <https://sarahweiii.github.io/meshlrm/>
> 42. **LGM**: <https://me.kiui.moe/lgm/>
> 43. **Efficient LoFTR**: <https://zju3dv.github.io/efficientloftr/>
> 44. **VideoGigaGAN**: <https://videogigagan.github.io/>
> 45. **SpatialTracker**: <https://henry123-boy.github.io/SpaTracker/>
> 46. **Key2Mesh**: <https://key2mesh.github.io/>
> 47. **Ultralytics YOLO11**: <https://docs.ultralytics.com/>

Continue reading [Papers](Papers.md){:.heading.flip-title}
{:.read-more}
