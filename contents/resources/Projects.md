---
layout: page
title: Projects
description: >
  Archive list for awesome projects.
# logo:
# theme_color:
# accent_color:
accent_image:
  background: url('/assets/images/resources/sidebar-resources.jpg') center/cover
  overlay: true
# image:
#   path:
#   srcset:
#     1024w:
#     512w:
#     256w:

# permalink: /contents/resources/
# show_collection: resources
# selected_projects:
# projects_page:
# selected_posts:
# posts_page:
# related_posts:
# redirect_from:
# excerpt_separator:
last_modified_at: 2025-05-04

hide_description: true
hide_image: false
hide_last_modified: false
invert_sidebar: false
cover: false
no_groups: true
no_link_title: false
no_excerpt: false
no_third_column: true
sitemap: true
comments: true
featured: false
---

- Table of Contents
{:toc}

# Artificial Intelligence

## (1) LMs

> 1. **The Rhythm In Anything**: <https://oreillyp.github.io/tria/>
> 2. **DriveDreamer4D**: <https://drivedreamer4d.github.io/>
> 3. **TokenFormer**: <https://haiyang-w.github.io/tokenformer.github.io/>
> 4. **hertz-dev**: <https://si.inc/hertz-dev/>
> 5. **PointLLM**: <https://runsenxu.com/projects/PointLLM/>
> 6. **Aria**: <https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model>
> 7. **SiT**: <https://scalable-interpolant.github.io/>
> 8. **Rhymes AI**: <https://rhymes.ai/>
> 9. **nemotron-4-340b-instruct**: <https://build.nvidia.com/nvidia/nemotron-4-340b-instruct>
> 10. **EgoLM**: <https://hongfz16.github.io/projects/EgoLM>
> 11. **Small Language Models (SLM)**: <https://www.jetson-ai-lab.com/tutorial_slm.html>
> 12. **MiniMind**: <https://jingyaogong.github.io/minimind/>
> 13. **ell**: <https://docs.ell.so/#>
> 14. **CogVLM2-Video**: <https://cogvlm2-video.github.io/>
> 15. **InternVL**: <https://internvl.github.io/>
> 16. **Chroma**: <https://generatebiomedicines.com/chroma>
> 17. **GameNGen**: <https://gamengen.github.io/>
> 18. **Husky**: <https://agent-husky.github.io/>
> 19. **Sapiens**: <https://about.meta.com/realitylabs/codecavatars/sapiens>
> 20. **MeshFormer**: <https://meshformer3d.github.io/>
> 21. **Genie**: <https://sites.google.com/view/genie-2024/home>
> 22. **Llama Tour**: <https://llamatutor.together.ai/>
> 23. **CogVideo**: <https://cogvideo.pka.moe/>
> 24. **Awesome ChatGPT Prompts**: <https://prompts.chat/>
> 25. **Opening up ChatGPT**: <https://opening-up-chatgpt.github.io/>
> 26. **AI Home Tab**: <https://aihometab.com/>
> 27. **Groq**: <https://groq.com/>
> 28. **NextChat**: <https://nextchat.dev/>
> 29. **MaxKB**: <https://maxkb.cn/>
> 30. **OpenDatalab**: <https://opendatalab.com/OpenSourceTools>
> 31. **Segment Anything Model 2 (SAM 2)**: <https://ai.meta.com/sam2/>
> 32. **LLaMA-Factory**: <https://qwen.readthedocs.io/en/latest/training/SFT/llama_factory.html>
> 33. **FunAudioLLM**: <https://funaudiollm.github.io/>
> 34. **KLING**: <https://kling.kuaishou.com/>
> 35. **AlphaGeometry**: <https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/>
> 36. **Prompt Engineering Guide**: <https://www.promptingguide.ai/>
> 37. **AskManyAI**: <https://askmanyai.cn/login>
> 38. **Meet PathChat 2**: <https://www.modella.ai/intro.html>
> 39. **PaintsUndo**: <https://lllyasviel.github.io/pages/paints_undo/>
> 40. **Artificial Analysis**: <https://artificialanalysis.ai/>
> 41. **Transformer Explainer**: <https://poloclub.github.io/transformer-explainer/>
> 42. **Moshi**: <https://moshi.chat/?queue_id=talktomoshi>
> 43. **AI Graveyard**: <https://dang.ai/ai-graveyard>
> 44. **GraphRAG**: <https://microsoft.github.io/graphrag/>
> 45. **Luma Dream Machine**: <https://lumalabs.ai/dream-machine>
> 46. **Open-Sora**: <https://hpcaitech.github.io/Open-Sora/>
> 47. **Cambrian-1**: <https://cambrian-mllm.github.io/>
> 48. **PaLM-E**: <https://palm-e.github.io/>
> 49. **Meta Prompting for AI Systems**: <https://meta-prompting.github.io/>
> 50. **SITUATIONAL AWARENESS**: <https://situational-awareness.ai/>
> 51. **ChatTTS**: <https://chattts.com/>
> 52. **KAN**: <https://kindxiaoming.github.io/pykan/>
> 53. **LLM Visualization**: <https://bbycroft.net/llm>
> 54. **MLX**: <https://ml-explore.github.io/mlx/build/html/index.html>
> 55. **COSTAR Prompt Engineering**: <https://medium.com/@frugalzentennial/unlocking-the-power-of-costar-prompt-engineering-a-guide-and-example-on-converting-goals-into-dc5751ce9875>
> 56. **Cohere**: <https://cohere.com/>
> 57. **DeepSpeed**: <https://www.deepspeed.ai/>
> 58. **AI Mind**: <https://www.aimind.so/>
> 59. **flowith**: <https://flowith.io/conv/51c05bc8-92f3-4644-869d-35fd22dbfb71>
> 60. **LeanDojo**: <https://leandojo.org/>
> 61. **GPT4All**: <https://www.nomic.ai/gpt4all>
> 62. **MiniGPT4-Video**: <https://vision-cair.github.io/MiniGPT4-video/>
> 63. **Mira**: <https://mira-space.github.io/>
> 64. **ModelScope**: <https://www.modelscope.cn/home>
> 65. **Osmo**: <https://www.osmo.ai/>
> 66. **Hume**: <https://www.hume.ai/>
> 67. **Reka**: <https://www.reka.ai/>
> 68. **Suno**: <https://suno.com/>
> 69. **Kimi**: <https://kimi.moonshot.cn/>
> 70. **SeamlessM4T**: <https://ai.meta.com/blog/seamless-m4t/>
> 71. **ElevenLabs**: <https://elevenlabs.io/>
> 72. **ChatLaw**: <https://chatlaw.cloud/>
> 73. **Gemma**: <https://ai.google.dev/gemma?hl=zh-cn>
> 74. **Stable Diffusion Art**: <https://stable-diffusion-art.com/comfyui/>
> 75. **OpenRouter**: <https://openrouter.ai/>
> 76. **ChatRTX**: <https://www.nvidia.com/en-us/ai-on-rtx/chatrtx/>
> 77. **Runway AI**: <https://runwayml.com/>
> 78. **Stable Video**: <https://www.stablevideo.com/welcome>
> 79. **Stability AI**: <https://stability.ai/>
> 80. **Multi-Agent Transformer**: <https://sites.google.com/view/multi-agent-transformer>
> 81. **DreamerV3**: <https://danijar.com/project/dreamerv3/>
> 82. **Imagen 2**: <https://deepmind.google/technologies/imagen-2/>
> 83. **Gemini Models**: <https://deepmind.google/technologies/gemini/#introduction>
> 84. **Anthropic AI**: <https://www.anthropic.com/>
> 85. **Grok**: <https://x.ai/>
> 86. **LLaVA**: <https://llava-vl.github.io/>
> 87. **Speaking AI**: <https://speaking.ai/>
> 88. **GPT-4 Is Too Smart To Be Safe**: <https://llmcipherchat.github.io/>
> 89. **Character AI**: <https://character.ai/>
> 90. **Whisper**: <https://openai.com/index/whisper/>
> 91. **Imagica AI**: <https://www.imagica.ai/>
> 92. **LangGPT**: <https://community.openai.com/t/langgpt-empowering-everyone-to-become-a-prompt-expert/207880>
> 93. **vLLM**: <https://docs.vllm.ai/en/latest/>
> 94. **DeepAI**: <https://deepai.org/>
> 95. **Midjourney**: <https://www.midjourney.com/home>
> 96. **Beautiful AI**: <https://www.beautiful.ai/>
> 97. **Qwen2.5-Turbo**: <https://qwen2.org/qwen2-5-turbo/>
> 98. **DINO-X**: <https://deepdataspace.com/home>
> 99. **ChatExcel**: <https://chatexcel.com/#/>
> 100. **World Labs**: <https://www.worldlabs.ai/blog>
> 101. **Prime Intellect**: <https://www.primeintellect.ai/>
> 102. **DeepSeek**: <https://www.deepseek.com/>
> 103. **HunyuanVideo**: <https://aivideo.hunyuan.tencent.com/>
> 104. **Navigation World Models**: <https://www.amirbar.net/nwm/>
> 105. **Polymathic**: <https://polymathic-ai.org/>
> 106. **Gemini 2.0**: <https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message>
> 107. **AlphaFold Server**: <https://alphafoldserver.com/welcome>
> 108. **Veo 2**: <https://deepmind.google/technologies/veo/veo-2/>
> 109. **Odyssey**: <https://odyssey.systems/introducing-explorer>
> 110. **Jina AI**: <https://jina.ai/reader/>
> 111. **Thinking in Space**: <https://vision-x-nyu.github.io/thinking-in-space.github.io/>
> 112. **Automating the Search for Artificial Life with Foundation Models**: <https://pub.sakana.ai/asal/>
> 113. **GPTsCopilot**: <https://chat.openai-now.com/zh>
> 114. **Unstract**: <https://unstract.com/>
> 115. **Decomposing Predictions by Modeling Model Computation**: <https://gradientscience.org/modelcomponents/>
> 116. **Hands-On Large Language Models**: <https://www.llm-book.com/>
> 117. **ZeroGPT**: <https://www.zerogpt.com/>
> 118. **SCENIC**: <https://virtualhumans.mpi-inf.mpg.de/scenic/>
> 119. **LobeChat**: <https://chat-preview.lobehub.com/chat>
> 120. **Farfalle**: <https://www.farfalle.dev/>
> 121. **TypingMind**: <https://www.typingmind.com/?ref=trackbes>
> 122. **LangGPT**: <https://langgptai.feishu.cn/wiki/RXdbwRyASiShtDky381ciwFEnpe>
> 123. **Sa2VA**: <https://lxtgh.github.io/project/sa2va/>
> 124. **Blackbox AI**: <https://www.blackbox.ai/>
> 125. **Sky-T1**: <https://novasky-ai.github.io/posts/sky-t1/>
> 126. **Instructor**: <https://useinstructor.com/>
> 127. **VILA**: <https://vila.mit.edu/>
> 128. **VideoLingo**: <https://videolingo.io/zh>
> 129. **Omni-RGPT**: <https://miranheo.github.io/omni-rgpt/>
> 130. **unwind ai**: <https://www.theunwindai.com/>
> 131. **ReaderLM v2**: <https://jina.ai/news/readerlm-v2-frontier-small-language-model-for-html-to-markdown-and-json/>
> 132. **cyanpuppets**: <https://cyanpuppets.com/>
> 133. **MatterGen**: <https://www.microsoft.com/en-us/research/articles/mattergen-a-generative-model-for-materials-design/>
> 134. **DrivingWorld**: <https://huxiaotaostasy.github.io/DrivingWorld/index.html>
> 135. **GAN Lab**: <https://poloclub.github.io/ganlab/>
> 136. **PoloClub**: <https://poloclub.github.io/>
> 137. **3D Shape Tokenization**: <https://machinelearning.apple.com/research/3d-shape-tokenization>
> 138. **Go-with-the-Flow**: <https://eyeline-research.github.io/Go-with-the-Flow/>
> 139. **unsloth**: <https://unsloth.ai/>
> 140. **Goku**: <https://saiyan-world.github.io/goku/>
> 141. **Inception**: <https://www.inceptionlabs.ai/>
> 142. **Magma**: <https://microsoft.github.io/Magma/>
> 143. **WorldModelBench**: <https://worldmodelbench-team.github.io/>
> 144. **OLMo Trace**: <https://allenai.org/>
> 145. **Model Context Protocol**: <https://www.anthropic.com/news/model-context-protocol>
> 146. **Awesome MCP Servers**: <https://mcpservers.org/>
> 147. **Glama**: <https://glama.ai/mcp/servers>
> 148. **MiniMax**:  <https://chat.minimaxi.com/>
> 149. **SpatialLM**: <https://manycore-research.github.io/SpatialLM/>
> 150. **Wanzhi**: <https://www.wanzhi.com/>
> 151. **Zapier MCP**: <https://zapier.com/mcp>
> 152. **MCP.so**: <https://mcp.so/zh>
> 153. **Qwen2.5 Omni**: <https://qwenlm.github.io/zh/blog/qwen2.5-omni/>
> 154. **Mureka**: <https://www.mureka.ai/home>
> 155. **Google AI Studio**: <https://aistudio.google.com/prompts/new_chat>
> 156. **MGX**: <https://mgx.dev/>
> 157. **Spark TTS**: <https://sparktts.io/zh>
> 158. **Llama Models**: <https://www.llama.com/docs/model-cards-and-prompt-formats/>
> 159. **WorldScore**: <https://haoyi-duan.github.io/WorldScore/>
> 160. **Agent2Agent Protocol**: <https://google.github.io/A2A/#/>
> 161. **Lyria**: <https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/>
> 162. **Magi-1**: <https://sand.ai/>
> 163. **InternVL**: <https://internvl.readthedocs.io/en/latest/>
> 164. **Niantic**: <https://www.nianticlabs.com/>
> 165. **Lemon Slice Live**: <https://lemonslice.com/live>

## (2) Agents

> 1. **UFO**: <https://github.com/microsoft/UFO>
> 2. **ell**: <https://github.com/MadcowD/ell>
> 3. **SWE-agent**: <https://github.com/princeton-nlp/SWE-agent>
> 4. **autogen**: <https://github.com/microsoft/autogen>
> 5. **CharacterGen**: <https://github.com/zjp-shadow/CharacterGen>
> 6. **Qwen2.5-Coder**: <https://qwenlm.github.io/zh/blog/qwen2.5-coder/>
> 7. **Skyvern**: <https://www.skyvern.com/>
> 8. **Ichigo**: <https://github.com/homebrewltd/ichigo>
> 9. **AI for Grant Writing**: <https://www.lizseckel.com/ai-for-grant-writing/>
> 10. **Large Language Model Agents**: <https://llmagents-learning.org/f24>
> 11. **OpenAGI**: <https://openagi.aiplanet.com/>
> 12. **HyperWrite**: <https://www.hyperwriteai.com/>
> 13. **FastGPT**: <https://tryfastgpt.ai/>
> 14. **ADAS**: <https://www.shengranhu.com/ADAS/>
> 15. **AI Town**: <https://www.convex.dev/ai-town>
> 16. **Comflowy**: <https://www.comflowy.com/>
> 17. **Altera**: <https://altera.al/>
> 18. **Firecrawl**: <https://www.firecrawl.dev/>
> 19. **Artificial LIfe ENvironment (ALIEN)**: <https://www.alien-project.org/index.html>
> 20. **AutoGen Studio 2.0**: <https://autogen-studio.com/>
> 21. **SuperCraft**: <https://supercraft.ai/>
> 22. **Pipecat**: <https://www.pipecat.ai/>
> 23. **Neo4j**: <https://neo4j.com/labs/genai-ecosystem/llm-graph-builder/>
> 24. **Lumina**: <https://www.lumina.sh/c5bbe32b-4fb7-476a-81aa-fe269f67f283?ref=www.lumina-chat.com>
> 25. **RAGFlow**: <https://ragflow.io/>
> 26. **OmniParse**: <https://docs.cognitivelab.in/>
> 27. **Supermemory**: <https://supermemory.ai/>
> 28. **MindSearch**: <https://mindsearch.netlify.app/>
> 29. **ChatDev**: <https://chatdev.toscl.com/>
> 30. **LlamaCoder**: <https://llamacoder.together.ai/>
> 31. **GPTs Works**: <https://gpts.works/>
> 32. **V2A-Mapper**: <https://v2a-mapper.github.io/>
> 33. **MultiOn**: <https://www.multion.ai/>
> 34. **ThinkAny**: <https://thinkany.ai/zh>
> 35. **Mem0**: <https://docs.mem0.ai/overview>
> 36. **Cradle**: <https://baai-agents.github.io/Cradle/>
> 37. **Devv**: <https://devv.ai/zh>
> 38. **Co-STORM**: <https://storm.genie.stanford.edu/>
> 39. **Bubble**: <https://bubble.io/>
> 40. **Humanize AI text**: <https://www.humanizeai.pro/>
> 41. **Aider**: <https://aider.chat/>
> 42. **Agently AI**: <https://agently.tech/>
> 43. **DeepSeek Coder**: <https://deepseekcoder.github.io/>
> 44. **AgentScope**: <https://doc.agentscope.io/en/index.html>
> 45. **CrewAI**: <https://docs.crewai.com/introduction>
> 46. **Humaan AI**: <https://humaan.ai/>
> 47. **FlowiseAI**: <https://flowiseai.com/>
> 48. **Chainlit**: <https://docs.chainlit.io/get-started/overview>
> 49. **Phidata**: <https://docs.phidata.com/agents>
> 50. **Lepton AI**: <https://www.lepton.ai/>
> 51. **AutoGPT**: <https://agpt.co/>
> 52. **MetaGPT**: <https://www.deepwisdom.ai/>
> 53. **LangGraph**: <https://langchain-ai.github.io/langgraph/>
> 54. **HyperWrite**: <https://www.hyperwriteai.com/>
> 55. **QAnything**: <https://qanything.ai/>
> 56. **Synthflow AI Voice Assistants**: <https://synthflow.ai/>
> 57. **Tavily**: <https://tavily.com/>
> 58. **Dify**: <https://dify.ai/>
> 59. **LangChain**: <https://www.langchain.com/>
> 60. **LlamaIndex**: <https://www.llamaindex.ai/>
> 61. **SWE-agent**: <https://swe-agent.com/>
> 62. **MemGPT**: <https://memgpt.ai/>
> 63. **SIMA**: <https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/>
> 64. **Durable**: <https://durable.co/>
> 65. **Cognition**: <https://www.cognition.ai/>
> 66. **LTX Studio**: <https://ltx.studio/>
> 67. **vellum**: <https://www.vellum.ai/>
> 68. **DetectGPT**: <https://detectgpt.ai/index.html>
> 69. **Dora AI**: <https://www.dora.run/ai>
> 70. **An Embodied Generalist Agent in 3D World**: <https://embodied-generalist.github.io/>
> 71. **Voyager**: <https://voyager.minedojo.org/>
> 72. **XAgent**: <https://xagent-doc.readthedocs.io/en/latest/>
> 73. **SuperAGI**: <https://superagi.com/>
> 74. **ima.copilot**: <https://ima.qq.com/>
> 75. **Supermaven**: <https://supermaven.com/>
> 76. **Accio**: <https://www.accio.com/>
> 77. **excalidraw**: <https://excalidraw.com/>
> 78. **Tencent Yuanqi**: <https://yuanqi.tencent.com/agent-shop>
> 79. **PicMenu**: <https://www.picmenu.co/>
> 80. **AgentBench**: <https://llmbench.ai/agent>
> 81. **SafetyBench**: <https://llmbench.ai/safety>
> 82. **AlignBench**: <https://llmbench.ai/align>
> 83. **AutoGLM**: <https://xiao9905.github.io/AutoGLM/>
> 84. **OmniParser**: <https://microsoft.github.io/OmniParser/>
> 85. **ResearchFlow**: <https://rflow.ai/>
> 86. **Semantic Scholar**: <https://www.semanticscholar.org/>
> 87. **Ant Design X**: <https://x.ant.design/index-cn>
> 88. **HAKE**: <http://hake-mvig.cn/home/>
> 89. **AIOS Foundation**: <https://aios.foundation/>
> 90. **CosmOS**: <https://humane.com/cosmos>
> 91. **letta**: <https://www.letta.com/>
> 92. **aisuite**: <https://github.com/andrewyng/aisuite>
> 93. **FlyFlow**: <https://www.flyflow.cc/>
> 94. **Building effective agents**: <https://www.anthropic.com/research/building-effective-agents>
> 95. **Automa**: <https://www.automa.site/>
> 96. **AG2**: <https://ag2.ai/>
> 97. **Running Llama on Windows 98**: <https://blog.exolabs.net/day-4/>
> 98. **n8n**: <https://n8n.io/>
> 99. **Project IDX**: <https://idx.dev/>
> 100. **Diagramming AI**: <https://diagrammingai.com/>
> 101. **Windsurf**: <https://windsurfai.org/zh>
> 102. **OpenSPG**: <https://spg.openkg.cn/>
> 103. **WrenAI**: <https://getwren.ai/oss>
> 104. **DB-GPT**: <http://docs.dbgpt.cn/docs/overview>
> 105. **Magentic-One**: <https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/>
> 106. **Micro Agent**: <https://www.builder.io/blog/micro-agent>
> 107. **Browserbase**: <https://www.browserbase.com/>
> 108. **Browser Use**: <https://browser-use.com/>
> 109. **TXYZ**: <https://app.txyz.ai/library>
> 110. **Agent Laboratory**: <https://agentlaboratory.github.io/>
> 111. **WhisperTyping**: <https://whispertyping.com/>
> 112. **Whisper Keyboard**: <https://wkd.market/>
> 113. **Eko**: <https://eko.fellou.ai/>
> 114. **ResearchFlow**: <https://rflow.ai/zh>
> 115. **PDF Guru Anki**: <https://guru.kevin2li.top/>
> 116. **RuleOS**: <https://ruleos.com/console/home>
> 117. **AI Agent**: <https://www.cnblogs.com/lusuo/p/18663007>
> 118. **Appwrite**: <https://appwrite.io/>
> 119. **Litmaps**: <https://www.litmaps.com/>
> 120. **PaSa**: <https://pasa-agent.ai/>
> 121. **TinyEngine**: <https://opentiny.design/tiny-engine#/home>
> 122. **AppAgent**: <https://appagent-official.github.io/>
> 123. **PC Agent**: <https://gair-nlp.github.io/PC-Agent/>
> 124. **ChatPaper**: <https://chatwithpaper.org/>
> 125. **DesktopGPT**: <https://desktopgpt.hix.ai/>
> 126. **Scholarcy**: <https://www.scholarcy.com/>
> 127. **Open Interpreter**: <https://www.openinterpreter.com/>
> 128. **Lumina**: <https://www.lumina.sh/c5bbe32b-4fb7-476a-81aa-fe269f67f283>
> 129. **Humata**: <https://app.humata.ai/login>
> 130. **Softgen**: <https://softgen.ai/>
> 131. **Talo**: <https://www.taloai.com/zh>
> 132. **Cofounder**: <https://cofounder.openinterface.ai/>
> 133. **Potpie AI**: <https://potpie.ai/>
> 134. **UndatasIO**: <https://undatas.io/>
> 135. **Sakana AI**: <https://sakana.ai/ai-cuda-engineer/>
> 136. **Docmost**: <https://docmost.com/>
> 137. **Convergence**: <https://convergence.ai/welcome>
> 138. **PyGPT**: <https://pygpt.net/>
> 139. **PySpur**: <https://www.pyspur.com/>
> 140. **Mobile-Agent-E**: <https://x-plug.github.io/MobileAgent/>
> 141. **OpenHands**: <https://www.all-hands.dev/>
> 142. **OctoTools**: <https://octotools.github.io/>
> 143. **Power Automate**: <https://www.microsoft.com/en-us/power-platform/products/power-automate>
> 144. **PySpur**: <https://www.pyspur.dev/>
> 145. **Manus**: <https://manus.monica.cn/>
> 146. **Glean**: <https://www.glean.com/>
> 147. **AppAgentX**: <https://appagentx.github.io/>
> 148. **LangBot**: <https://docs.langbot.app/insight/guide>
> 149. **TheySaid**: <https://www.theysaid.io/>
> 150. **Relume**: <https://www.relume.io/>
> 151. **ChatTTS**: <https://www.chattts.co/zh>
> 152. **Flourish**: <https://flourish.studio/>
> 153. **CopyWeb**: <https://copyweb.ai/>
> 154. ***AFFiNE**: <https://affine.pro/>
> 155. **Agent TARS**: <https://agent-tars.com/>
> 156. **TalkMe**: <https://www.talkme.ai/>
> 157. **Anysphere**: <https://anysphere.inc/>
> 158. **screenpipe**: <https://screenpi.pe/>
> 159. **Genspark**: <https://www.genspark.ai/>
> 160. **Agno**: <https://www.agno.com/>
> 161. **Khoj AI**: <https://khoj.dev/>
> 162. **Crawl4AI**: <https://docs.crawl4ai.com/>
> 163. **Drawatoon**: <https://drawatoon.com/>
> 164. **Cline**: <https://cline.bot/>
> 165. **Roo Code**: <https://roocode.com/>
> 166. **PandaAI**: <https://getpanda.ai/>
> 167. **Agent Development Kit**: <https://google.github.io/adk-docs/>
> 168. **Industrial Copilots**: <https://www.siemens.com/global/en/products/automation/topic-areas/industrial-ai/industrial-copilot.html#DiscoverourIndustrialCopilots>
> 169. **meetily**: <https://meetily.zackriya.com/>
> 170. **Liam ERD**: <https://liambx.com/>
> 171. **Droidrun**: <https://www.droidrun.ai/>
> 172. **AingDesk**: <https://www.aingdesk.com/>
> 173. **MindsDB**: <https://mindsdb.com/>
> 174. **Vanna.AI**: <https://vanna.ai/>
> 175. **Outrank**: <https://www.outrank.so/>
> 176. **Maxun**: <https://www.maxun.dev/>
> 177. **Simular AI**: <https://www.simular.ai/>
> 178. **FunASR**: <https://www.funasr.com/#/>
> 179. **Graphiti**: <https://help.getzep.com/graphiti/graphiti/overview>
> 180. **Suna**: <https://www.suna.so/>
> 181. **Chat2DB**: <https://chat2db-ai.com/>
> 182. **OpenBB**: <https://openbb.co/>
> 183. **SigLens**: <https://www.siglens.com/>
> 184. **Readdy**: <https://readdy.ai/>

## (3) AIGC

> 1. **MikuDance**: <https://kebii.github.io/MikuDance/>
> 2. **DanceFusion**: <https://th-mlab.github.io/DanceFusion/>
> 3. **StdGEN**: <https://stdgen.github.io/>
> 4. **URAvatar**: <https://junxuan-li.github.io/urgca-website/>
> 5. **Towards High-fidelity Head Blending with Chroma Keying for Industrial Applications**: <https://hahminlew.github.io/changer/>
> 6. **ReCapture**: <https://generative-video-camera-controls.github.io/>
> 7. **DimensionX**: <https://chenshuo20.github.io/DimensionX/>
> 8. **Fashion-VDM**: <https://johannakarras.github.io/Fashion-VDM/>
> 9. **X-Portrait 2**: <https://byteaigc.github.io/X-Portrait2/>
> 10. **GameGen-X**: <https://gamegen-x.github.io/>
> 11. **HelloMeme**: <https://songkey.github.io/hellomeme/>
> 12. **DreamVideo-2**: <https://dreamvideo2.github.io/>
> 13. **VidPanos**: <https://vidpanos.github.io/>
> 14. **DAWN**: <https://hanbo-cheng.github.io/DAWN/>
> 15. **Interstice**: <https://www.interstice.cloud/>
> 16. **LongVU**: <https://vision-cair.github.io/LongVU/>
> 17. **DreamCraft3D++**: <https://dreamcraft3dplus.github.io/>
> 18. **Hallo2**: <https://fudan-generative-vision.github.io/hallo2/#/>
> 19. **MVideo**: <https://mvideo-v1.github.io/>
> 20. **UniMuMo**: <https://hanyangclarence.github.io/unimumo_demo/>
> 21. **TextToon**: <https://songluchuan.github.io/TextToon/>
> 22. **eye-contact-correction**: <https://www.sievedata.com/functions/sieve/eye-contact-correction>
> 23. **TANGO**: <https://pantomatrix.github.io/TANGO/>
> 24. **Animate-X**: <https://lucaria-academy.github.io/Animate-X/>
> 25. **ACE**: <https://ali-vilab.github.io/ace-page/>
> 26. **PhysGen**: <https://stevenlsw.github.io/physgen/>
> 27. **EdgeRunner**: <https://research.nvidia.com/labs/dir/edgerunner/>
> 28. **Movie Gen**: <https://ai.meta.com/research/movie-gen/>
> 29. **Inverse Painting**: <https://inversepainting.github.io/>
> 30. **Disco4D**: <https://disco-4d.github.io/>
> 31. **MimicTalk**: <https://mimictalk.github.io/>
> 32. **DIAMOND**: <https://diamond-wm.github.io/>
> 33. **JoyHallo**: <https://jdh-algo.github.io/JoyHallo/>
> 34. **SF3D**: <https://stable-fast-3d.github.io/>
> 35. **GaussianCube**: <https://gaussiancube.github.io/>
> 36. **Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing**: <https://pei-xu.github.io/guitar>
> 37. **SPARK**: <https://kelianb.github.io/SPARK/>
> 38. **LVCD**: <https://luckyhzt.github.io/lvcd>
> 39. **PortraitGen**: <https://ustc3dv.github.io/PortraitGen/>
> 40. **NeRF**: <https://www.matthewtancik.com/nerf>
> 41. **HIT**: <https://hit.is.tue.mpg.de/#video>
> 42. **3DTopia-XL**: <https://3dtopia.github.io/3DTopia-XL/>
> 43. **DrawingSpinUp**: <https://lordliang.github.io/DrawingSpinUp/>
> 44. **Robust Dual Gaussian Splatting for Immersive Human-centric Volumetric Videos**: <https://nowheretrix.github.io/DualGS/>
> 45. **PoseTalk**: <https://junleen.github.io/projects/posetalk/>
> 46. **GVHMR**: <https://zju3dv.github.io/gvhmr/>
> 47. **PersonaTalk**: <https://grisoon.github.io/PersonaTalk/>
> 48. **EscherNet**: <https://kxhit.github.io/EscherNet>
> 49. **Gaussian Garments**: <https://ribosome-rbx.github.io/Gaussian-Garments/>
> 50. **CyberHost**: <https://cyberhost.github.io/>
> 51. **Draw an Audio**: <https://yannqi.github.io/Draw-an-Audio/>
> 52. **3DGRT**: <https://gaussiantracer.github.io/>
> 53. **ViewCrafter**: <https://drexubery.github.io/ViewCrafter/>
> 54. **ReconX**: <https://liuff19.github.io/ReconX/>
> 55. **FaceSwap**: <https://faceswap.so/>
> 56. **Civitai**: <https://civitai.com/>
> 57. **Loopy**: <https://loopyavatar.github.io/>
> 58. **PhotoMaker**: <https://huggingface.co/spaces/TencentARC/PhotoMaker>
> 59. **InterTrack**: <https://virtualhumans.mpi-inf.mpg.de/InterTrack/>
> 60. **Build-A-Scene**: <https://abdo-eldesokey.github.io/build-a-scene/>
> 61. **MagicMan**: <https://thuhcsi.github.io/MagicMan/>
> 62. **LayerPano3D**: <https://ys-imtech.github.io/projects/LayerPano3D/>
> 63. **DreamCinema**: <https://liuff19.github.io/DreamCinema/>
> 64. **TurboEdit**: <https://betterze.github.io/TurboEdit/>
> 65. **Scaling Up Dynamic Human-Scene Interaction Modeling**: <https://jnnan.github.io/trumans/>
> 66. **DiPIR**: <https://research.nvidia.com/labs/toronto-ai/DiPIR/>
> 67. **TurboEdit**: <https://turboedit-paper.github.io/>
> 68. **DEGAS**: <https://initialneil.github.io/DEGAS>
> 69. **Audio Match Cutting**: <https://denfed.github.io/audiomatchcut/>
> 70. **Subsurface Scattering for Gaussian Splatting**: <https://sss.jdihlmann.com/>
> 71. **Tavus**: <https://www.tavus.io/>
> 72. **Media2Face**: <https://sites.google.com/view/media2face>
> 73. **FruitNeRF**: <https://meyerls.github.io/fruit_nerf/>
> 74. **Puppet-Master**: <https://vgg-puppetmaster.github.io/>
> 75. **ernerf**: <https://zhuanlan.zhihu.com/p/675131165>
> 76. **An Object is Worth 64x64 Pixels**: <https://omages.github.io/>
> 77. **VideoDoodles**: <https://em-yu.github.io/research/videodoodles/>
> 78. **ReSyncer**: <https://guanjz20.github.io/projects/ReSyncer/>
> 79. **KEEP**: <https://jnjaby.github.io/projects/KEEP/>
> 80. **MoMask**: <https://ericguo5513.github.io/momask/>
> 81. **Tora**: <https://ali-videoai.github.io/tora_video/>
> 82. **EmoTalk3D**: <https://nju-3dv.github.io/projects/EmoTalk3D/>
> 83. **Cycle3D**: <https://pku-yuangroup.github.io/Cycle3D/>
> 84. **Swapface**: <https://www.swapface.org/#/home>
> 85. **ExAvatar**: <https://mks0601.github.io/ExAvatar/>
> 86. **MotionClone**: <https://bujiazi.github.io/motionclone.github.io/>
> 87. **Outfit Anyone**: <https://humanaigc.github.io/outfit-anyone/>
> 88. **Vidu**: <https://www.vidu.studio/zh>
> 89. **Temporal Residual Jacobians for Rig-free Motion Transfer**: <https://temporaljacobians.github.io/>
> 90. **Cinemo**: <https://maxin-cn.github.io/cinemo_project/>
> 91. **HumanVid**: <https://humanvid.github.io/>
> 92. **Diffree**: <https://opengvlab.github.io/Diffree/>
> 93. **SMooDi**: <https://neu-vi.github.io/SMooDi/>
> 94. **Lite2Relight**: <https://vcai.mpi-inf.mpg.de/projects/Lite2Relight/>
> 95. ***Noise Calibration**: <https://yangqy1110.github.io/NC-SDEdit/>
> 96. **Diff-Foley**: <https://diff-foley.github.io/>
> 97. **Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity**: <https://maskvat.github.io/>
> 98. **vozo**: <https://www.vozo.ai/>
> 99. **MoA**: <https://snap-research.github.io/mixture-of-attention/>
> 100. **Magic Insert**: <https://magicinsert.github.io/>
> 101. **CharacterGen**: <https://charactergen.github.io/>
> 102. **Live2Diff**: <https://live2diff.github.io/>
> 103. **RodinHD**: <https://rodinhd.github.io/>
> 104. **StickerBaker**: <https://stickerbaker.com/>
> 105. **Still-Moving**: <https://still-moving.github.io/>
> 106. **Tripo3D**: <https://www.tripo3d.ai/>
> 107. **Hedra**: <https://www.hedra.com/>
> 108. **RenderNet**: <https://rendernet.ai/index.html>
> 109. **LivePortrait**: <https://liveportrait.github.io/>
> 110. **Image Conductor**: <https://liyaowei-stu.github.io/project/ImageConductor/>
> 111. **MOTIA**: <https://be-your-outpainter.github.io/>
> 112. **Meta 3D AssetGen**: <https://assetgen.github.io/>
> 113. **Portrait3D**: <https://jinkun-hao.github.io/Portrait3D/>
> 114. **GaussianDreamerPro**: <https://taoranyi.com/gaussiandreamerpro/>
> 115. **MimicMotion**: <https://tencent.github.io/MimicMotion/>
> 116. **Text-Animator**: <https://laulampaul.github.io/text-animator.html>
> 117. **YouDream**: <https://youdream3d.github.io/>
> 118. **FoleyCrafter**: <https://foleycrafter.github.io/>
> 119. **Wonder Studio**: <https://wonderdynamics.com/>
> 120. **TripoSR**: <https://stability.ai/news/triposr-3d-generation>
> 121. **MeshAnything**: <https://buaacyw.github.io/mesh-anything/>
> 122. **EvTexture**: <https://dachunkai.github.io/evtexture.github.io/>
> 123. **ScoreHypo**: <https://xy02-05.github.io/ScoreHypo/>
> 124. **AniFusion**: <https://anifusion.ai/>
> 125. **Style-NeRF2NeRF**: <https://haruolabs.github.io/style-n2n/>
> 126. **4K4DGen**: <https://4k4dgen.github.io/>
> 127. **ExVideo**: <https://ecnu-cilab.github.io/ExVideoProjectPage/>
> 128. **Diffutoon**: <https://ecnu-cilab.github.io/DiffutoonProjectPage/>
> 129. **Holistic-Motion2D**: <https://holistic-motion2d.github.io/>
> 130. **FaceFusion**: <https://docs.facefusion.io/>
> 131. **AnyFit**: <https://colorful-liyu.github.io/anyfit-page/>
> 132. **PuzzleFusion++**: <https://puzzlefusion-plusplus.github.io/>
> 133. **UniAnimate**: <https://unianimate.github.io/>
> 134. **ChronoDepth**: <https://jhaoshao.github.io/ChronoDepth/>
> 135. **Unique3D**: <https://wukailu.github.io/Unique3D/>
> 136. **GECO**: <https://cwchenwang.github.io/geco/>
> 137. **T2V-Turbo**: <https://t2v-turbo.github.io/>
> 138. **EasyAnimate**: <https://easyanimate.github.io/>
> 139. **ZeroSmooth**: <https://ssyang2020.github.io/zerosmooth.github.io/>
> 140. **MVSGaussian**: <https://mvsgaussian.github.io/>
> 141. **CityGaussian**: <https://dekuliutesla.github.io/citygs/>
> 142. **MOFA-Video**: <https://myniuuu.github.io/MOFA_Video/>
> 143. **VividDream**: <https://vivid-dream-4d.github.io/>
> 144. **Motion2VecSets**: <https://vveicao.github.io/projects/Motion2VecSets/>
> 145. **MultiPly**: <https://eth-ait.github.io/MultiPly/>
> 146. **Neural Gaffer**: <https://neural-gaffer.github.io/>
> 147. **I4VGen**: <https://xiefan-guo.github.io/i4vgen/>
> 148. **ToonCrafter**: <https://doubiiu.github.io/projects/ToonCrafter/>
> 149. **2DGS**: <https://surfsplatting.github.io/>
> 150. **Collaborative Video Diffusion**: <https://collaborativevideodiffusion.github.io/>
> 151. **Looking Backward**: <https://jeff-liangf.github.io/projects/streamv2v/>
> 152. **SadTalker**: <https://sadtalker.github.io/>
> 153. **VividTalk**: <https://humanaigc.github.io/vivid-talk/>
> 154. **I2VEdit**: <https://i2vedit.github.io/>
> 155. **MagicPose4D**: <https://boese0601.github.io/magicpose4d/>
> 156. **Generative Camera Dolly**: <https://gcd.cs.columbia.edu/>
> 157. **ReVideo**: <https://mc-e.github.io/project/ReVideo/>
> 158. **Text-to-Vector Generation with Neural Path Representation**: <https://intchous.github.io/T2V-NPR/>
> 159. **CAT3D**: <https://cat3d.github.io/>
> 160. **StructLDM**: <https://taohuumd.github.io/projects/StructLDM/>
> 161. **AniTalker**: <https://x-lance.github.io/AniTalker/>
> 162. **Dual3D**: <https://dual3d.github.io/>
> 163. **X-Oscar**: <https://xmu-xiaoma666.github.io/Projects/X-Oscar/>
> 164. **HiDiffusion**: <https://hidiffusion.github.io/>
> 165. **Tunnel Try-on**: <https://mengtingchen.github.io/tunnel-try-on-page/>
> 166. **STAG4D**: <https://nju-3dv.github.io/projects/STAG4D/>
> 167. **GS-LRM**: <https://sai-bi.github.io/project/gs-lrm/>
> 168. **GScream**: <https://w-ted.github.io/publications/gscream/>
> 169. **MotionMaster**: <https://sjtuplayer.github.io/projects/MotionMaster/>
> 170. **PhysDreamer**: <https://physdreamer.github.io/>
> 171. **SwapAnything**: <https://swap-anything.github.io/>
> 172. **MagicPose**: <https://boese0601.github.io/magicdance/>
> 173. **ZeST**: <https://ttchengab.github.io/zest/>
> 174. **EMOPortraits**: <https://neeek2303.github.io/EMOPortraits/>
> 175. **StoryDiffusion**: <https://storydiffusion.github.io/>
> 176. **Automatic Controllable Colorization via Imagination**: <https://xy-cong.github.io/imagine-colorization/>
> 177. **MaPa**: <https://zhanghe3z.github.io/MaPa/>
> 178. **EMO**: <https://humanaigc.github.io/emote-portrait-alive/>
> 179. **StreamingT2V**: <https://streamingt2v.github.io/>
> 180. **IDM-VTON**: <https://idm-vton.github.io/>
> 181. **IntrinsicAnything**: <https://zju3dv.github.io/IntrinsicAnything/>
> 182. **Interactive3D**: <https://interactive-3d.github.io/>
> 183. **in2IN**: <https://pabloruizponce.github.io/in2IN/>
> 184. **synthesia**: <https://www.synthesia.io/>
> 185. **DreamWalk**: <https://mshu1.github.io/dreamwalk.github.io/>
> 186. **MagicTime**: <https://pku-yuangroup.github.io/MagicTime/>
> 187. **Gaussian Head Avatar**: <https://yuelangx.github.io/gaussianheadavatar/>
> 188. **Champ**: <https://fudan-generative-vision.github.io/champ/#/>
> 189. **ObjectDrop**: <https://objectdrop.github.io/>
> 190. **HeyGen**: <https://www.heygen.com/>
> 191. **DomoAI**: <https://domoai.app/>
> 192. **Pebblely**: <https://pebblely.com/>
> 193. **Photorealistic Video Generation with Diffusion Models**: <https://walt-video-diffusion.github.io/>
> 194. **3D-GPT**: <https://chuny1.github.io/3DGPT/3dgpt.html>
> 195. **SynthID**: <https://deepmind.google/technologies/synthid/>
> 196. **Palette**: <https://palette.fm/>
> 197. **Upscayl**: <https://upscayl.org/>
> 198. **CLIP-NeRF**: <https://cassiepython.github.io/clipnerf/>
> 199. **Scaling up GANs for Text-to-Image Synthesis**: <https://mingukkang.github.io/GigaGAN/>
> 200. **CoDeF**: <https://qiuyu96.github.io/CoDeF/>
> 201. **MVideo**: <https://mvideo-v1.github.io/>
> 202. **edify-3d**: <https://build.nvidia.com/shutterstock/edify-3d>
> 203. **JoyVASA**: <https://jdh-algo.github.io/JoyVASA/>
> 204. **EchoMimicV1**: <https://antgroup.github.io/ai/echomimic/>
> 205. **EchoMimicV2**: <https://antgroup.github.io/ai/echomimic_v2/>
> 206. **DiffusionGS**: <https://caiyuanhao1998.github.io/project/DiffusionGS/>
> 207. **AnchorCrafter**: <https://cangcz.github.io/Anchor-Crafter/>
> 208. **Generative Omnimatte**: <https://gen-omnimatte.github.io/>
> 209. **MultiFoley**: <https://ificl.github.io/MultiFoley/>
> 210. **ConsisID**: <https://pku-yuangroup.github.io/ConsisID/>
> 211. **CAT4D**: <https://cat-4d.github.io/>
> 212. **Sonic**: <https://jixiaozhong.github.io/Sonic/>
> 213. **MyTimeMachine**: <https://mytimemachine.github.io/>
> 214. **FLOAT**: <https://deepbrainai-research.github.io/float/>
> 215. **SelfSplat**: <https://gynjn.github.io/selfsplat/>
> 216. **I2VControl**: <https://wanquanf.github.io/I2VControl>
> 217. **One Shot, One Talk**: <https://ustc3dv.github.io/OneShotOneTalk/>
> 218. **MEMO**: <https://memoavatar.github.io/>
> 219. **MIDI**: <https://huanngzh.github.io/MIDI-Page/>
> 220. **AnyDressing**: <https://crayon-shinchan.github.io/AnyDressing/>
> 221. **StableAnimator**: <https://francis-rings.github.io/StableAnimator/>
> 222. **MotionShop**: <https://motionshop-diffusion.github.io/>
> 223. **You See it,You Got it**: <https://vision.baai.ac.cn/see3d>
> 224. **StyleMaster**: <https://zixuan-ye.github.io/stylemaster/>
> 225. **DisPose**: <https://lihxxx.github.io/DisPose/>
> 226. **ClearerVoice-Studio**: <https://stable-learn.com/en/clearvoice-studio-tutorial/>
> 227. **Video Seal**: <https://aidemos.meta.com/videoseal>
> 228. **Animated Drawings**: <https://fairanimateddrawings.com/site/home>
> 229. **DiffGS**: <https://junshengzhou.github.io/DiffGS/>
> 230. **FlipSketch**: <https://hmrishavbandy.github.io/flipsketch-web/>
> 231. **Motion Prompting**: <https://motion-prompting.github.io/>
> 232. **Wonderland**: <https://snap-research.github.io/wonderland/>
> 233. **BrushEdit**: <https://liyaowei-stu.github.io/project/BrushEdit/index.html>
> 234. **Whisk**: <https://labs.google/fx/zh/tools/whisk/unsupported-country>
> 235. **AniDoc**: <https://yihao-meng.github.io/AniDoc_demo/>
> 236. **Stag-1**: <https://wzzheng.net/Stag/>
> 237. **PanoDreamer**: <https://people.engr.tamu.edu/nimak/Papers/PanoDreamer/index.html>
> 238. **DI-PCG**: <https://thuzhaowang.github.io/projects/DI-PCG/>
> 239. **Sketch2Sound**: <https://hugofloresgarcia.art/sketch2sound/>
> 240. **INFP**: <https://grisoon.github.io/INFP/>
> 241. **Feat2GS**: <https://fanegg.github.io/Feat2GS/>
> 242. **AI Sound Effect Generator**: <https://www.sound-effect-generator.com/>
> 243. **ElevenLabs**: <https://elevenlabs.io/sound-effects>
> 244. **MotiF**: <https://wang-sj16.github.io/motif/>
> 245. **DrivingForward**: <https://fangzhou2000.github.io/projects/drivingforward/>
> 246. **Birth and Death of a Rose**: <https://chen-geng.com/rose4d/>
> 247. **ZeroHSI**: <https://awfuact.github.io/zerohsi/>
> 248. **Dora**: <https://www.dora.run/ai/>
> 249. **FaceLift**: <https://www.wlyu.me/FaceLift/>
> 250. **SynCamMaster**: <https://jianhongbai.github.io/SynCamMaster/>
> 251. **PDP**: <https://stanford-tml.github.io/PDP.github.io/>
> 252. **TEXT-TO-CAD**: <https://zoo.dev/text-to-cad>
> 253. **Story-Adapter**: <https://jwmao1.github.io/storyadapter/>
> 254. **PERSE**: <https://hyunsoocha.github.io/perse/>
> 255. **SeedVR**: <https://iceclear.github.io/projects/seedvr/>
> 256. **VideoAnydoor**: <https://videoanydoor.github.io/>
> 257. **STAR**: <https://nju-pcalab.github.io/projects/STAR/>
> 258. **3DEnhancer**: <https://yihangluo.com/projects/3DEnhancer/>
> 259. **Hallo3**: <https://fudan-generative-vision.github.io/hallo3/#/>
> 260. **TransPixar**: <https://wileewang.github.io/TransPixar/>
> 261. **DaS**: <https://igl-hkust.github.io/das/>
> 262. **Sana**: <https://nvlabs.github.io/Sana/>
> 263. **InfiniCube**: <https://research.nvidia.com/labs/toronto-ai/infinicube/>
> 264. **apob**: <https://apob.ai/>
> 265. **Diffusion as Shader**: <https://igl-hkust.github.io/das/>
> 266. **SVFR**: <https://wangzhiyaoo.github.io/SVFR/>
> 267. **EnergyMoGen**: <https://jiro-zhang.github.io/EnergyMoGen/>
> 268. **Labelme**: <https://labelme.io/>
> 269. **Magic Mirror**: <https://julianjuaner.github.io/projects/MagicMirror/>
> 270. **Video Alchemist**: <https://snap-research.github.io/open-set-video-personalization/>
> 271. **SPAR3D**: <https://spar3d.github.io/>
> 272. **ConceptMaster**: <https://yuzhou914.github.io/ConceptMaster/>
> 273. **TOPVIEW**: <https://www.topview.ai/>
> 274. **MangaNinja**: <https://johanan528.github.io/MangaNinjia/>
> 275. **LeviTor**: <https://ppetrichor.github.io/levitor.github.io/>
> 276. **Turbo-GS**: <https://ivl.cs.brown.edu/research/turbo-gs.html>
> 277. **RAIN**: <https://pscgylotti.github.io/pages/RAIN/>
> 278. **SynthLight**: <https://vrroom.github.io/synthlight/>
> 279. **Piktochart**: <https://piktochart.com/>
> 280. **Medio.Cool**: <https://www.medio.cool/en/>
> 281. **VTON**: <https://ningshuliang.github.io/2023/Arxiv/index.html>
> 282. **FlexiClip**: <https://creative-gen.github.io/flexiclip.github.io/>
> 283. **CaPa**: <https://ncsoft.github.io/CaPa/>
> 284. **AniGS**: <https://lingtengqiu.github.io/2024/AniGS/>
> 285. **Arc2Avatar**: <https://arc2avatar.github.io/>
> 286. **GaussianAvatar-Editor**: <https://xiangyueliu.github.io/GaussianAvatar-Editor/>
> 287. **MoDec-GS**: <https://kaist-viclab.github.io/MoDecGS-site/>
> 288. **X-Dyna**: <https://x-dyna.github.io/xdyna.github.io/>
> 289. **Textoon**: <https://human3daigc.github.io/Textoon_webpage/>
> 290. **EMO2**: <https://humanaigc.github.io/emote-portrait-alive-2/>
> 291. **MatAnyone**: <https://pq-yang.github.io/projects/MatAnyone/>
> 292. **OmniHuman-1**: <https://omnihuman-lab.github.io/>
> 293. **Scaling Up 3D Gaussian Splatting Training**: <https://daohanlu.github.io/scaling-up-3dgs/>
> 294. **DynVFX**: <https://dynvfx.github.io/>
> 295. **AuraFusion360**: <https://kkennethwu.github.io/aurafusion360/>
> 296. **JOGG AI**: <https://www.jogg.ai/community/>
> 297. **Animate Anyone 2**: <https://humanaigc.github.io/animate-anyone-2/>
> 298. **Light-A-Video**: <https://bujiazi.github.io/light-a-video.github.io/>
> 299. **CineMaster**: <https://cinemaster-dev.github.io/>
> 300. **HumanDiT**: <https://agnjason.github.io/HumanDiT-page/>
> 301. **Pippo**: <https://yashkant.github.io/pippo/>
> 302. **MetaHuman**: <https://www.unrealengine.com/zh-CN/metahuman>
> 303. **Phantom**: <https://phantom-video.github.io/Phantom/>
> 304. **SkyReels**: <https://www.skyreels.ai/home>
> 305. **Stockimg AI**: <https://stockimg.ai/>
> 306. **FantasyID**: <https://fantasy-amap.github.io/fantasy-id/>
> 307. **CAST**: <https://sites.google.com/view/cast4>
> 308. **EmbodiedScan**: <https://tai-wang.github.io/embodiedscan/>
> 309. **Kiss3DGen**: <https://ltt-o.github.io/Kiss3dgen.github.io/>
> 310. **Avat3r**: <https://tobias-kirschstein.github.io/avat3r/>
> 311. **Raphael AI**: <https://raphael.app/zh>
> 312. **UniScene**: <https://arlo0o.github.io/uniscene/>
> 313. **MeshPad**: <https://derkleineli.github.io/meshpad/>
> 314. **Vid2Avatar-Pro**: <https://moygcc.github.io/vid2avatar-pro/>
> 315. **Heygem**: <https://github.com/GuijiAI/HeyGem.ai>
> 316. **TrajectoryCrafter**: <https://trajectorycrafter.github.io/>
> 317. **DreamRelation**: <https://dreamrelation.github.io/>
> 318. **ObjectMover**: <https://xinyu-andy.github.io/ObjMover/>
> 319. **Motion Anything**: <https://steve-zeyu-zhang.github.io/MotionAnything/>
> 320. **NotaGen**: <https://electricalexis.github.io/notagen-demo/>
> 321. **Phase**: <https://www.phase.com/>
> 322. **VACE**: <https://ali-vilab.github.io/VACE-Page/>
> 323. **Flux AI**: <https://flux-ai.io/cn/>
> 324. **Klingai**: <https://app.klingai.com/cn/>
> 325. **miaohua**: <https://miaohua.sensetime.com/>
> 326. **PIKA AI**: <https://pikaai.org/cn/>
> 327. **EchoMimic**: <https://badtobest.github.io/echomimic.html>
> 328. **EchoMimicV2**: <https://antgroup.github.io/ai/echomimic_v2/>
> 329. **Uthana**: <https://uthana.com/>
> 330. **jimeng**: <https://jimeng.jianying.com/ai-tool/home>
> 331. **chanjing**: <https://www.chanjing.cc/>
> 332. **AIPPT**: <https://www.aippt.cn/>
> 333. **ETCH**: <https://boqian-li.github.io/ETCH/>
> 334. **Bolt3D**: <https://szymanowiczs.github.io/bolt3d>
> 335. **MTV-Inpaint**: <https://mtv-inpaint.github.io/>
> 336. **Hunyuan3D**: <https://www.hunyuan-3d.com/>
> 337. **Gamma**: <https://gamma.app/zh-cn>
> 338. **DeepMesh**: <https://zhaorw02.github.io/DeepMesh/>
> 339. **SynCity**: <https://research.paulengstler.com/syncity/>
> 340. **MagicMotion**: <https://quanhaol.github.io/magicmotion-site/?ref=aiartweekly>
> 341. **RASA**: <https://alice01010101.github.io/RASA/>
> 342. **A Recipe for Generating 3D Worlds From a Single Image**: <https://katjaschwarz.github.io/worlds/>
> 343. **TaoAvatar**: <https://pixelai-team.github.io/TaoAvatar/> or <https://taoavatar.org/>
> 344. **MoDGS**: <https://modgs.github.io/>
> 345. **MoCha**: <https://congwei1230.github.io/MoCha/>
> 346. **PhysGen3D**: <https://by-luckk.github.io/PhysGen3D/>
> 347. **GroomLight**: <https://syntec-research.github.io/GroomLight/>
> 348. **ChatAnyone**: <https://humanaigc.github.io/chat-anyone/>
> 349. **DreamActor-M1**: <https://grisoon.github.io/DreamActor-M1/>
> 350. **InfiniteYou**: <https://bytedance.github.io/InfiniteYou/>
> 351. **ACTalker**: <https://harlanhong.github.io/publications/actalker/index.html>
> 352. **LAM**: <https://aigc3d.github.io/projects/LAM/>
> 353. **One-Minute Video Generation with Test-Time Training**: <https://test-time-training.github.io/video-dit/>
> 354. **Comprehensive Relighting**: <https://junyingw.github.io/paper/relighting/>
> 355. **OmniSVG**: <https://omnisvg.github.io/>
> 356. **FantasyTalking**: <https://fantasy-amap.github.io/fantasy-talking/>
> 357. **lipsync2**: <https://taneemishere.github.io/projects/lipsync2.html>
> 358. **Move AI**: <https://www.move.ai/>
> 359. **Uni3C**: <https://ewrfcas.github.io/Uni3C/>
> 360. **Event-Enhanced Blurry Video Super-Resolution**: <https://dachunkai.github.io/ev-deblurvsr.github.io/>
> 361. **Infinite Mobility**: <https://infinite-mobility.github.io/>
> 362. **DeepCAD**: <http://www.cs.columbia.edu/cg/deepcad/>
> 363. **3DV-TON**: <https://2y7c3.github.io/3DV-TON/>
> 364. **Insert Anything**: <https://song-wensong.github.io/insert-anything/>
> 365. **Varjo**: <https://varjo.com/company-news/varjo-launches-teleport-2-0-a-generational-leap-in-photorealistic-3d-capture-for-creators/>
> 366. **MotionCritic**: <https://motioncritic.github.io/>
> 367. **Pixel3DMM**: <https://simongiebenhain.github.io/pixel3dmm/>

# Robotics

## (1) Hardware

> 1. **Zeroth**: <https://docs.zeroth.bot/>
> 2. **Power-over-Skin**: <https://www.figlab.com/research/2024/poweroverskin>
> 3. **RoboDuet**: <https://locomanip-duet.github.io/>
> 4. **The snake that saves lives**: <https://ethz.ch/en/news-and-events/eth-news/news/2024/11/the-snake-that-saves-lives.html>
> 5. **XGO-Rider**: <https://www.kickstarter.com/projects/xgorobot/xgo-rider-desktop-two-wheel-legged-robot-with-ai>
> 6. **7X**: <https://7xr.tech/>
> 7. **Berkeley Humanoid**: <https://berkeley-humanoid.com/>
> 8. **Torobo**: <https://robotics.tokyo/products/torobo/>
> 9. **DexHand**: <https://www.dexhand.org/>
> 10. **NAVER LABS**: <https://www.naverlabs.com/>
> 11. **Surena Humanoid Robot**: <https://surenahumanoid.com/>
> 12. **NEO Home Humanoid**: <https://www.1x.tech/>
> 13. **Digit - Dexterous Manipulation and Touch Perception**: <https://digit.ml/>
> 14. **TidyBot**: <https://tidybot.cs.princeton.edu/>
> 15. **EyeSight Hand**: <https://eyesighthand.github.io/>
> 16. **Carpentopod:A walking table project**: <https://www.decarpentier.nl/carpentopod>
> 17. **MouthPad**: <https://www.augmental.tech/>
> 18. **MiniRHex**: <https://robomechanics.github.io/MiniRHex/>
> 19. **DRAGON**: <https://interestingengineering.com/videos/ep-13-how-dragons-and-insects-are-inspiring-drones-of-the-future>
> 20. **Velociraptor Robot**: <https://indiannerve.com/korean-velociraptor-robot-is-the-fastest-biped-that-can-outrun-usain-bolt-top-speed-46-kmhr-15584/>
> 21. **RoboPanoptes**: <https://robopanoptes.github.io/>
> 22. **ToddlerBot**: <https://toddlerbot.github.io/>
> 23. **DOGlove**: <https://do-glove.github.io/>
> 24. **Torobo Hand**: <https://robotics.tokyo/products/hand/>
> 25. **COVVI Robotics**: <https://www.covvi-robotics.com/>
> 26. **Hannes Hand**: <https://rehab.iit.it/hannes>
> 27. **Vega**: <https://www.dexmate.ai/vega>
> 28. **FIRST Tech Challenge**: <https://ftc-docs.firstinspires.org/en/latest/index.html>
> 29. **Sanctuary AI**: <https://www.sanctuary.ai/>
> 30. **Linkerbot**: <https://www.linkerbot.cn/index>
> 31. **DexHand**: <https://www.dex-robot.com/productionDexhand>
> 32. **Booster T1**: <https://www.boosterobotics.com/zh/>
> 33. **OpenArm**: <https://open-arm.org/>
> 34. **XGO-Rider**: <https://wiki.elecfreaks.com/en/microbit/robot/xgo-rider-kit/introduction/>
> 35. **BamBot**: <https://bambot.org/>
> 36. **ORCA Hand**: <https://www.orcahand.com/>
> 37. **DG-5F Hand**: <https://en.tesollo.com/dg-5f/>
> 38. **Poppy Humanoid**: <https://www.poppy-project.org/en/robots/poppy-humanoid/>
> 39. **agibot**: <http://agibot.cn/>
> 40. **X1-PDG**: <https://www.zhiyuan-robot.com/DOCS/OS/X1-PDG>
> 41. **Fourier-GRX-N1**: <http://support.fftai.cn/main/en/concepts/overview/>
> 42. **Tactile-Readtive Roller Grasper**: <https://yuanshenli.com/tactile_reactive_roller_grasper.html>

## (2) Software

> 1. **ROS2**: <https://docs.ros.org/en/jazzy/index.html>
> 2. **SAFER-Splat**: <https://chengine.github.io/safer-splat/>
> 3. **Neural MP**: <https://mihdalal.github.io/neuralmotionplanner/>
> 4. **AirSLAM**: <https://xukuanhit.github.io/airslam/>
> 5. **SimTK**: <https://simtk.org/>
> 6. **MyoSuite**: <https://sites.google.com/view/myosuite/myosuite>
> 7. **Hyfydy**: <https://hyfydy.com/>
> 8. **LVCP**: <https://sites.google.com/view/lvcp>
> 9. **Hello**: <https://www.hello-algo.com/>
> 10. **Skild AI**: <https://www.skild.ai/>
> 11. **NVIDIA Project GR00T**: <https://developer.nvidia.com/project-gr00t>
> 12. **OpenWorm**: <https://openworm.org/>
> 13. **NVIDIA Isaac ROS**: <https://nvidia-isaac-ros.github.io/>
> 14. **VehicleSim**: <https://www.carsim.com/>
> 15. **GraspNet**: <https://graspnet.net/>
> 16. **Quad-SDK**: <https://robomechanics.github.io/quad-sdk/>
> 17. **MASt3R-SLAM**: <https://edexheim.github.io/mast3r-slam/>
> 18. **Genesis**: <https://genesis-embodied-ai.github.io/>
> 19. **Exbody2**: <https://exbody2.github.io/>
> 20. **HO-Cap**: <https://irvlutd.github.io/HOCap/>
> 21. **ARMOR**: <https://daehwakim.com/armor/>
> 22. **HugWBC**: <https://hugwbc.github.io/>
> 23. **Dexterity Gen**: <https://zhaohengyin.github.io/dexteritygen/>
> 24. **MJINX**: <https://based-robotics.github.io/mjinx/index.html>
> 25. **Newton**: <https://developer.nvidia.com/blog/announcing-newton-an-open-source-physics-engine-for-robotics-simulation>
> 26. **RUKA**: <https://ruka-hand.github.io/>
> 27. **Genie Sim Benchmark**: <https://agibot-world.com/sim-evaluation>
> 28. **Geometric Retargeting**: <https://zhaohengyin.github.io/geort/#>
> 29. **LangWBC**: <https://langwbc.github.io/>

# AIRobotics

## (1) Robot Learning

> 1. **UMI(Universal Manipulation Interface)**: <https://umi-gripper.github.io/>
> 2. **PSAG**: <https://www.jianrenw.com/PSAG/>
> 3. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 4. **RGBManip**: <https://rgbmanip.github.io/>
> 5. **RoboStudio**: <https://robostudioapp.com/>
> 6. **ReKep**: <https://rekep-robot.github.io/>
> 7. **DeformGS**: <https://deformgs.github.io/>
> 8. **NeuralFeels**: <https://suddhu.github.io/neural-feels/>
> 9. **ALOHA**: <https://tonyzhaozh.github.io/aloha/>
> 10. **LucidSim**: <https://lucidsim.github.io/>
> 11. **RoPotter**: <https://robot-pottery.github.io/>
> 12. **HOVER**: <https://hover-versatile-humanoid.github.io/>
> 13. **DexMimicGen**: <https://dexmimicgen.github.io/>
> 14. **Eurekaverse**: <https://eureka-research.github.io/eurekaverse/>
> 15. **HIL-SERL**: <https://hil-serl.github.io/>
> 16. **OrbitGrasp**: <https://orbitgrasp.github.io/>
> 17. **3D-ViTac**: <https://binghao-huang.github.io/3D-ViTac/>
> 18. **Physical Intelligence**: <https://www.physicalintelligence.company/blog/pi0>
> 19. **HuDOR**: <https://object-rewards.github.io/>
> 20. **LAPA**: <https://latentactionpretraining.github.io/>
> 21. **ManipGen**: <https://mihdalal.github.io/manipgen/>
> 22. **Robots Pre-Train Robots**: <https://robots-pretrain-robots.github.io/>
> 23. **ARNOLD**: <https://arnold-benchmark.github.io/>
> 24. **GPT-4V(ision) for Robotics**: <https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/>
> 25. **VoxAct-B**: <https://voxact-b.github.io/>
> 26. **ARCap**: <https://stanford-tml.github.io/ARCap/>
> 27. **Harmon**: <https://ut-austin-rpl.github.io/Harmon/>
> 28. **Data Scaling Laws**: <https://data-scaling-laws.github.io/>
> 29. **Dynamic 3D Gaussian Tracking**: <https://gs-dynamics.github.io/>
> 30. **OKAMI**: <https://ut-austin-rpl.github.io/OKAMI/>
> 31. **UniHSI**: <https://xizaoqu.github.io/unihsi/>
> 32. **SDS**: <https://rpl-cs-ucl.github.io/SDSweb/>
> 33. **EgoAllo**: <https://egoallo.github.io/>
> 34. **DART**: <https://zkf1997.github.io/DART/>
> 35. **FrElise**: <https://for-elise.github.io/>
> 36. **PourIt**: <https://hetolin.github.io/PourIt/>
> 37. **Cherrybot**: <https://goodcherrybot.github.io/>
> 38. **AnyCar to Anywhere**: <https://lecar-lab.github.io/anycar/>
> 39. **Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies**: <https://lipschitz-constrained-policy.github.io/>
> 40. **HumanoidOlympics**: <https://humanoidolympics.github.io/>
> 41. **OmniH2O**: <https://omni.human2humanoid.com/>
> 42. **Continuously Improving Mobile Manipulation with Autonomous Real-World RL**: <https://continual-mobile-manip.github.io/>
> 43. **MotIF**: <https://motif-1k.github.io/>
> 44. **Helpful DoggyBot**: <https://helpful-doggybot.github.io/>
> 45. **Blox-Net**: <https://bloxnet.org/>
> 46. **GR-MG**: <https://gr-mg.github.io/>
> 47. **Real-World Cooking Robot System from Recipes**: <https://kanazawanaoaki.github.io/cook-from-recipe-pddl/>
> 48. **Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**: <https://gr1-manipulation.github.io/>
> 49. **GR-2**: <https://gr2-manipulation.github.io/>
> 50. **CLoSD**: <https://guytevet.github.io/CLoSD-page/>
> 51. **RDT-1B**: <https://rdt-robotics.github.io/rdt-robotics/>
> 52. **Agile Continuous Jumping in Discontinuous Terrains**: <https://yxyang.github.io/jumping_cod/>
> 53. **Humanoid Manipulation**: <https://humanoid-manipulation.github.io/>
> 54. **Diff-Control**: <https://diff-control.github.io/>
> 55. **Catch It**: <https://mobile-dex-catch.github.io/>
> 56. **Robot See Robot Do**: <https://robot-see-robot-do.github.io/>
> 57. **Gen2Act**: <https://homangab.github.io/gen2act/>
> 58. **Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing**: <https://lecar-lab.github.io/dial-mpc/>
> 59. **ReMEmbR**: <https://nvidia-ai-iot.github.io/remembr/>
> 60. **ReMEmbR**: <https://developer.nvidia.com/blog/using-generative-ai-to-enable-robots-to-reason-and-act-with-remembr/>
> 61. **MaskedMimic**: <https://research.nvidia.com/labs/par/maskedmimic/>
> 62. **Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation**: <https://human2humanoid.com/>
> 63. **Theia**: <https://theia.theaiinstitute.com/>
> 64. **Robot Motion Diffusion Model: Motion Generation for Robotic Characters**: <https://la.disneyresearch.com/publication/robot-motion-diffusion-model-motion-generation-for-robotic-characters/>
> 65. **Identifying Terrain Physical Parameters from Vision**: <https://leggedrobotics.github.io/identifying_terrain_physical_parameters_webpage/>
> 66. **One-shot Video Imitation via Parameterized Symbolic Abstraction Graphs**: <https://www.jianrenw.com/PSAG/>
> 67. **RGBManip**: <https://rgbmanip.github.io/>
> 68. **ALOHA Unleashed**: <https://aloha-unleashed.github.io/>
> 69. **PianoMime**: <https://pianomime.github.io/>
> 70. **Robot Utility Models**: <https://robotutilitymodels.com/#>
> 71. **Polaris**: <https://star-uu-wang.github.io/Polaris/>
> 72. **RoboStudio**: <https://robostudioapp.com/>
> 73. **ICRT**: <https://icrt.dev/>
> 74. **SkillMimic**: <https://ingrid789.github.io/SkillMimic/>
> 75. **VoicePilot**: <https://sites.google.com/andrew.cmu.edu/voicepilot/>
> 76. **ReKep**: <https://rekep-robot.github.io/>
> 77. **DeformGS**: <https://deformgs.github.io/>
> 78. **ATM**: <https://xingyu-lin.github.io/atm/>
> 79. **Universal Manipulation Interface**: <https://umi-gripper.github.io/>
> 80. **ACE**: <https://ace-teleop.github.io/>
> 81. **Summarize the Past to Predict the Future**: <https://eth-ait.github.io/transfusion-proj/>
> 82. **TacSL**: <https://iakinola23.github.io/tacsl/>
> 83. **RoCo**: <https://project-roco.github.io/>
> 84. **UniT**: <https://zhengtongxu.github.io/unifiedtactile.github.io/>
> 85. **PhysHOI**: <https://wyhuai.github.io/physhoi-page/>
> 86. **UMI on Legs**: <https://umi-on-legs.github.io/>
> 87. **Lifelike Agility and Play in Quadrupedal Robots**: <https://tencent-roboticsx.github.io/lifelike-agility-and-play/>
> 88. **RoboCasa**: <https://robocasa.ai/>
> 89. **GET-Zero**: <https://get-zero-paper.github.io/>
> 90. **DextrAH-G**: <https://sites.google.com/view/dextrah-g>
> 91. **Surgical Robot Transformer**: <https://surgical-robot-transformer.github.io/>
> 92. **Grasping Diverse Objects with Simulated Humanoids**: <https://www.zhengyiluo.com/Omnigrasp-Site/>
> 93. **PoliFormer**: <https://poliformer.allen.ai/>
> 94. **This&That**: <https://cfeng16.github.io/this-and-that/>
> 95. **RoboCat**: <https://deepmind.google/discover/blog/robocat-a-self-improving-robotic-agent/>
> 96. **RoboGen**: <https://robogen-ai.github.io/>
> 97. **EquiBot**: <https://equi-bot.github.io/>
> 98. **Policy Composition From and For Heterogeneous Robot Learning**: <https://liruiw.github.io/policycomp/>
> 99. **GenSim**: <https://gen-sim.github.io/>
> 100. **Bunny-VisionPro**: <https://dingry.github.io/projects/bunny_visionpro.html>
> 101. **Open-TeleVision**: <https://robot-tv.github.io/>
> 102. **DexGraspNet**: <https://pku-epic.github.io/DexGraspNet/>
> 103. **Mobile ALOHA**: <https://mobile-aloha.github.io/>
> 104. **OpenVLA**: <https://openvla.github.io/>
> 105. **MS-Human-700**: <https://lnsgroup.cc/research/MS-Human-700>
> 106. **HumanPlus**: <https://humanoid-ai.github.io/>
> 107. **Octo**: <https://octo-models.github.io/>
> 108. **HOI-M3**: <https://juzezhang.github.io/HOIM3_ProjectPage/>
> 109. **DrEureka**: <https://eureka-research.github.io/dr-eureka/>
> 110. **FLD**: <https://sites.google.com/view/iclr2024-fld/home>
> 111. **SATO**: <https://sato-team.github.io/Stable-Text-to-Motion-Framework/>
> 112. **ViPlanner**: <https://leggedrobotics.github.io/viplanner.github.io/>
> 113. **HumanoidBench**: <https://humanoid-bench.github.io/>
> 114. **DexCap**: <https://dex-cap.github.io/>
> 115. **RT-Sketch**: <https://rt-sketch.github.io/>
> 116. **SARA**: <https://sites.google.com/view/rtsara/?pli=1>
> 117. **AutoRT**: <https://auto-rt.github.io/>
> 118. **RT-Trajectory**: <https://rt-trajectory.github.io/>
> 119. **iGibson**: <https://svl.stanford.edu/igibson/>
> 120. **GOAT**: <https://theophilegervet.github.io/projects/goat/>
> 121. **Dynamic Handover**: <https://binghao-huang.github.io/dynamic_handover/>
> 122. **Eureka**: <https://eureka-research.github.io/>
> 123. **Sequential Dexterity**: <https://sequential-dexterity.github.io/>
> 124. **From Text to Motion**: <https://tnoinkwms.github.io/ALTER-LLM/>
> 125. **MimicGen**: <https://mimicgen.github.io/>
> 126. **NOIR**: <https://noir-corl.github.io/>
> 127. **BC-Z**: <https://sites.google.com/view/bc-z/home>
> 128. **Open-World Object Manipulation using Pre-Trained Vision-Language Models**: <https://robot-moo.github.io/>
> 129. **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models**: <https://instructionaugmentation.github.io/>
> 130. **VIMA**: <https://vimalabs.github.io/>
> 131. **CLIPort**: <https://cliport.github.io/>
> 132. **EmbodiedGPT**: <https://embodiedgpt.github.io/>
> 133. **ASE**: <https://xbpeng.github.io/projects/ASE/index.html>
> 134. **RoboAgent**: <https://robopen.github.io/>
> 135. **RT-2**: <https://robotics-transformer2.github.io/>
> 136. **Do As I Can, Not As I Say**: <https://say-can.github.io/>
> 137. **Perceiver-Actor**: <https://peract.github.io/>
> 138. **VoxPoser**: <https://voxposer.github.io/>
> 139. **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**: <https://tonyzhaozh.github.io/aloha/>
> 140. **Soft Robotic Dynamic In-Hand Pen Spinning**: <https://soft-spin.github.io/>
> 141. **Bimanual Dexterity for Complex Tasks**: <https://bidex-teleop.github.io/>
> 142. **WildLMA**: <https://wildlma.github.io/>
> 143. **Learning Purely Tactile In-Hand Manipulation**: <https://aidx-lab.org/manipulation/humanoids24>
> 144. **FoAR**: <https://tonyfang.net/FoAR/>
> 145. **3D Diffusion Policy**: <https://3d-diffusion-policy.github.io/>
> 146. **Dex-Net**: <https://berkeleyautomation.github.io/dex-net/>
> 147. **See, Hear, Feel**: <https://ai.stanford.edu/~rhgao/see_hear_feel/>
> 148. **DenseMatcher**: <https://tea-lab.github.io/DenseMatcher/>
> 149. **Mobile-TeleVision**: <https://mobile-tv.github.io/>
> 150. **TidyBot++**: <https://tidybot2.github.io/>
> 151. **Planning-Guided Diffusion Policy Learning for Generalizable Contact-Rich Bimanual Manipulation**: <https://glide-manip.github.io/>
> 152. **You Only Plan Once**: <https://tju-air-lab.github.io/projects/YOPO/>
> 153. **Human-Object Interaction from Human-Level Instructions**: <https://hoifhli.github.io/>
> 154. **Video Prediction Policy**: <https://video-prediction-policy.github.io/>
> 155. **HYPERmotion**: <https://hy-motion.github.io/>
> 156. **RoboMIND**: <https://x-humanoid-robomind.github.io/>
> 157. **ForceMimic**: <https://forcemimic.github.io/>
> 158. **AgiBot World**: <https://agibot-world.com/>
> 159. **NaVILA**: <https://navila-bot.github.io/>
> 160. **Manipulate Anything**: <https://robot-ma.github.io/>
> 161. **Meta Motivo**: <https://metamotivo.metademolab.com/>
> 162. **EnerVerse**: <https://sites.google.com/view/enerverse>
> 163. **OmniManip**: <https://omnimanip.github.io/>
> 164. **MuJoCo Playground**: <https://playground.mujoco.org/>
> 165. **Embrace Collisions**: <https://project-instinct.github.io/>
> 166. **SPA**: <https://haoyizhu.github.io/spa/>
> 167. **ASAP**: <https://agile.human2humanoid.com/>
> 168. **Visuomotor Policies to Grasp Anything with Dexterous Hands**: <https://dextrah-rgb.github.io/>
> 169. **HAMSTER**: <https://hamster-robot.github.io/>
> 170. **DexVLA**: <https://dex-vla.github.io/>
> 171. **Humanoid-Getup**: <https://humanoid-getup.github.io/>
> 172. **RHINO**: <https://humanoid-interaction.github.io/>
> 173. **SoFar**: <https://qizekun.github.io/sofar/>
> 174. **BeamDojo**: <https://why618188.github.io/beamdojo/>
> 175. **Helix**: <https://www.figure.ai/news/helix>
> 176. **Reflective Planning**: <https://reflect-vlm.github.io/>
> 177. **AnyPlace**: <https://any-place.github.io/>
> 178. **AnyTop**: <https://anytop2025.github.io/Anytop-page/>
> 179. **Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning**: <https://whole-body-loco.github.io/>
> 180. **OpenRobotLab**: <https://grutopia.github.io/>
> 181. **Learning 3D Dynamic Scene Representations for Robot Manipulation**: <https://dsr-net.cs.columbia.edu/>
> 182. **Hybrid Internal Model for Legged Locomotion**: <https://junfeng-long.github.io/HIMLoco/>
> 183. **DexGraspVLA**: <https://dexgraspvla.github.io/>
> 184. **Discrete-Time Hybrid Automata Learning**: <https://umich-curly.github.io/DHAL/>
> 185. **Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids**: <https://toruowo.github.io/recipe/>
> 186. **Humanoid Parkour Learning**: <https://humanoid4parkour.github.io/>
> 187. **Robot Parkour Learning**: <https://robot-parkour.github.io/>
> 188. **InterMimic**: <https://sirui-xu.github.io/InterMimic/>
> 189. **Reactive Diffusion Policy**: <https://reactive-diffusion-policy.github.io/>
> 190. **BEHAVIOR Robot Suite**: <https://behavior-robot-suite.github.io/>
> 191. **Gemini Robotics**: <https://deepmind.google/technologies/gemini-robotics/>
> 192. **DemoGen**: <https://demo-generation.github.io/>
> 193. **Robo-ABC**: <https://tea-lab.github.io/Robo-ABC/>
> 194. **MimicPlay**: <https://mimic-play.github.io/>
> 195. **Soft and Compliant Contact-Rich Hair Manipulation and Care**: <https://moehair.github.io/>
> 196. **RT-1**: <https://robotics-transformer1.github.io/>
> 197. **Heterogeneous Pre-trained Transformers**: <https://liruiw.github.io/hpt/>
> 198. **GraspNet**: <https://graspnet.net/anygrasp.html>
> 199. **RoboDreamer**: <https://robovideo.github.io/>
> 200. **UniSim**: <https://universal-simulator.github.io/unisim/>
> 201. **Visuomotor Policy Learning via Action Diffusion**: <https://diffusion-policy.cs.columbia.edu/>
> 202. **RoboAgent**: <https://robopen.github.io/index.html>
> 203. **Learning to Act from Actionless Videos through Dense Correspondences**: <https://flow-diffusion.github.io/>
> 204. **OK-Robot**: <https://ok-robot.github.io/>
> 205. **Learning Universal Policies via Text-Guided Video Generation**: <https://universal-policy.github.io/unipi/>
> 206. **Helix**: <https://www.figure.ai/news/helix>
> 207. **Bi-DexHands**: <https://pku-marl.github.io/DexterousHands/>
> 208. **PhysTwin**: <https://jianghanxiao.github.io/phystwin-web/>
> 209. **RoboFactory**: <https://iranqin.github.io/robofactory/>
> 210. **TokenHSI**: <https://liangpan99.github.io/TokenHSI/>
> 211. **RoboBrain**: <https://superrobobrain.github.io/>
> 212. **NeuPAN**: <https://hanruihua.github.io/neupan_project/>
> 213. **Embodied Reasoner**: <https://embodied-reasoner.github.io/>
> 214. **ManipTrans**: <https://maniptrans.github.io/>
> 215. **ReBot**: <https://yuffish.github.io/rebot/>
> 216. **Scalable Real2Sim**: <https://scalable-real2sim.github.io/>
> 217. **RoboGSim**: <https://robogsim.github.io/>
> 218. **LEGATO**: <https://ut-hcrl.github.io/LEGATO/>
> 219. **RobustDexGrasp**: <https://zdchan.github.io/Robust_DexGrasp/>
> 220. **DexTrack**: <https://meowuu7.github.io/DexTrack/>
> 221. **Sequential Multi-Object Grasping with One Dexterous Hand**: <https://hesic73.github.io/SeqMultiGrasp/>
> 222. **Real-is-Sim**: <https://realissim.rai-inst.com/>
> 223. **AnyDexGrasp**: <https://graspnet.net/anydexgrasp/>
> 224. **AnyTouch**: <https://gewu-lab.github.io/AnyTouch/>
> 225. **RoboTwin**: <https://robotwin-benchmark.github.io/index.html>
> 226. **RoboSplat**: <https://yangsizhe.github.io/robosplat/>
> 227. **ZeroGrasp**: <https://sh8.io/#/zerograsp>
> 228. **Chain-of-Modality**: <https://chain-of-modality.github.io/>
> 229. **DexGraspNet 2.0**: <https://pku-epic.github.io/DexGraspNet2.0/>
> 230. **AffordDexGrasp**: <https://isee-laboratory.github.io/AffordDexGrasp/index.html>
> 231. **Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies**: <https://cxu-tri.github.io/FAIL-Detect-Website/>
> 232. **RoboVerse**: <https://roboverseorg.github.io/>
> 233. **ArticuBot**: <https://articubot.github.io/>
> 234. **Manual2Skill**: <https://owensun2004.github.io/Furniture-Assembly-Web/>
> 235. **SERL**: <https://serl-robot.github.io/>
> 236. **Limitations in Visuo-Motor Robot Learning**: <https://tsagkas.github.io/pvrobo/>
> 237. **DeepMimic**: <https://xbpeng.github.io/projects/DeepMimic/index.html>
> 238. **ET-SEED**: <https://et-seed.github.io/>
> 239. **Human2Sim2Robot**: <https://human2sim2robot.github.io/>
> 240. **TesserAct**: <https://tesseractworld.github.io/>
> 241. **Learning to Learn Faster from Human Feedback with Language Model Predictive Control**: <https://robot-teaching.github.io/>

## (2) Autonomous Driving

## (3) Embodied Intelligence

# Metaverse

## (1) Omniverse

## (2) Digital Twin

# Utilities

## (1) Computer Vision

> 1. **OpenCV**: <https://opencv.org/>
> 2. **roboflow**: <https://roboflow.com/>
> 3. **MoGe**: <https://wangrc.site/MoGePage/>
> 4. **Cloth-Splatting**: <https://kth-rpl.github.io/cloth-splatting/>
> 5. **SpectroMotion**: <https://cdfan0627.github.io/spectromotion/>
> 6. **SMITE**: <https://segment-me-in-time.github.io/>
> 7. **VistaDream**: <https://vistadream-project-page.github.io/index.html>
> 8. **InterMask**: <https://gohar-malik.github.io/intermask/>
> 9. **Dessie**: <https://celiali.github.io/Dessie/>
> 10. **CoTracker3**: <https://cotracker3.github.io/>
> 11. **PointCloud Conditioned Mesh Generation**: <https://research.nvidia.com/labs/dir/edgerunner/gallery/point_cond_4.html>
> 12. **Depth Any Video with Scalable Synthetic Data**: <https://depthanyvideo.github.io/>
> 13. **MonST3R**: <https://monst3r-project.github.io/>
> 14. **EVER**: <https://half-potato.gitlab.io/posts/ever/>
> 15. **CoTracker**: <https://co-tracker.github.io/>
> 16. **WiLoR**: <https://rolpotamias.github.io/WiLoR/>
> 17. **MIMO**: <https://menyifang.github.io/projects/MIMO/index.html>
> 18. **M2Mapping**: <https://jianhengliu.github.io/Projects/M2Mapping/>
> 19. **3D Gaussian Splatting for Real-Time Radiance Field Rendering**: <https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/>
> 20. **StableNormal**: <https://stable-x.github.io/StableNormal/>
> 21. **EmbodiedSAM**: <https://xuxw98.github.io/ESAM/>
> 22. **Large tendue 3D Holographic Display with Content-adpative Dynamic Fourier Modulation**: <https://bchao1.github.io/holo_dfm/>
> 23. **Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos**: <https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/>
> 24. **EgoHDM**: <https://handiyin.github.io/EgoHDM/>
> 25. **DepthCrafter**: <https://depthcrafter.github.io/>
> 26. **Spann3R**: <https://hengyiwang.github.io/projects/spanner>
> 27. **OpenIns3D**: <https://zheninghuang.github.io/OpenIns3D/>
> 28. **Bilateral Reference for High-Resolution Dichotomous Image Segmentation**: <https://www.birefnet.top/>
> 29. **ObjectCarver**: <https://objectcarver.github.io/>
> 30. **Improving 2D Feature Representations by 3D-Aware Fine-Tuning**: <https://ywyue.github.io/FiT3D/>
> 31. **Shape of Motion**: <https://shape-of-motion.github.io/>
> 32. **DINO-Tracker**: <https://dino-tracker.github.io/>
> 33. **DiffIR2VR-Zero**: <https://jimmycv07.github.io/DiffIR2VR_web/>
> 34. **Vidu4D**: <https://vidu4d-dgs.github.io/>
> 35. **RaDe-GS**: <https://baowenz.github.io/radegs/>
> 36. **Semantic Gaussians**: <https://sharinka0715.github.io/semantic-gaussians/>
> 37. **FoundationPose**: <https://nvlabs.github.io/FoundationPose/>
> 38. **InstantSplat**: <https://instantsplat.github.io/>
> 39. **GS-Pose**: <https://dingdingcai.github.io/gs-pose/>
> 40. **I'M HOI**: <https://afterjourney00.github.io/IM-HOI.github.io/>
> 41. **MeshLRM**: <https://sarahweiii.github.io/meshlrm/>
> 42. **LGM**: <https://me.kiui.moe/lgm/>
> 43. **Efficient LoFTR**: <https://zju3dv.github.io/efficientloftr/>
> 44. **VideoGigaGAN**: <https://videogigagan.github.io/>
> 45. **SpatialTracker**: <https://henry123-boy.github.io/SpaTracker/>
> 46. **Key2Mesh**: <https://key2mesh.github.io/>
> 47. **Ultralytics YOLO11**: <https://docs.ultralytics.com/>
> 48. **Neuralangelo**: <https://research.nvidia.com/labs/dir/neuralangelo/>
> 49. **SAMURAI**: <https://yangchris11.github.io/samurai/>
> 50. **YOLO**: <https://docs.ultralytics.com/zh>
> 51. **Buffer Anytime**: <https://bufferanytime.github.io/index.html>
> 52. **RollingDepth**: <https://rollingdepth.github.io/>
> 53. **Efficient Track Anything**: <https://yformer.github.io/efficient-track-anything/>
> 54. **MegaSaM**: <https://mega-sam.github.io/>
> 55. **DualPM**: <https://dualpm.github.io/>
> 56. **Align3R**: <https://igl-hkust.github.io/Align3R.github.io/>
> 57. **SAMa**: <https://mfischer-ucl.github.io/sama/>
> 58. **NLF:Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation**: <https://istvansarandi.com/nlf/>
> 59. **Using Diffusion Priors for Video Amodal Segmentation**: <https://diffusion-vas.github.io/>
> 60. **LongVolCap**: <https://zju3dv.github.io/longvolcap/>
> 61. **Multiview Scene Graph**: <https://ai4ce.github.io/MSG/>
> 62. **Prompting Depth Anything**: <https://promptda.github.io/>
> 63. **MVLift**: <https://lijiaman.github.io/projects/mvlift/>
> 64. **SAT-HMR**: <https://chisu001.github.io/SAT-HMR/>
> 65. **DEIM**: <https://www.shihuahuang.cn/DEIM/>
> 66. **PartGen**: <https://silent-chen.github.io/PartGen/>
> 67. **GenHMR**: <https://m-usamasaleem.github.io/publication/GenHMR/GenHMR.html>
> 68. **SuperGSeg**: <https://supergseg.github.io/>
> 69. **DUSt3R**: <https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/>
> 70. **Orient Anything**: <https://orient-anything.github.io/>
> 71. **Gaga**: <https://www.gaga.gallery/>
> 72. **ProTracker**: <https://michaelszj.github.io/protracker/>
> 73. **JOSH**: <https://genforce.github.io/JOSH/>
> 74. **Dyn-HaMR**: <https://dyn-hamr.github.io/>
> 75. **Depth Any Camera**: <https://yuliangguo.github.io/depth-any-camera/>
> 76. **DepthLab**: <https://johanan528.github.io/depthlab_web/>
> 77. **MatchAnything**: <https://zju3dv.github.io/MatchAnything/>
> 78. **OmniFusion**: <https://yuyanli0831.github.io/OmniFusion-Website/>
> 79. **Reconstructing People, Places, and Cameras**: <https://muelea.github.io/hsfm/>
> 80. **BioPose**: <https://m-usamasaleem.github.io/publication/BioPose/BioPose.html>
> 81. **SplatMAP**: <https://arxiv.org/html/2501.07015v1>
> 82. **FoundationStereo**: <https://nvlabs.github.io/FoundationStereo/>
> 83. **Video Depth Anything**: <https://videodepthanything.github.io/>
> 84. **Depth Anything V2**: <https://depth-anything-v2.github.io/>
> 85. **GausSurf**: <https://jiepengwang.github.io/GausSurf/>
> 86. **FLARE**: <https://zhanghe3z.github.io/FLARE/>
> 87. **Aether**: <https://aether-world.github.io/>
> 88. **Easi3R**: <https://easi3r.github.io/>
> 89. **SceneScript**: <https://ai.meta.com/blog/scenescript-3d-scene-reconstruction-reality-labs-research/>
> 90. **HSMR**: <https://isshikihugh.github.io/HSMR/>
> 91. **Multi-view Reconstruction via SfM-guided Monocular Depth Estimation**: <https://zju3dv.github.io/murre/>
> 92. **NormalCrafter**: <https://normalcrafter.github.io/>
> 93. **FRAME**: <https://vcai.mpi-inf.mpg.de/projects/FRAME/>
> 94. **TAPIP3D**: <https://tapip3d.github.io/>
> 95. **ShowMak3r**: <https://nstar1125.github.io/showmak3r/>
> 96. **Matching 2D Images in 3D**: <https://nianticlabs.github.io/mickey/>
> 97. **Scene Coordinate Reconstruction**: <https://nianticlabs.github.io/acezero/>
> 98. **Clinica**: <https://www.clinica.run/>

## (2) Tools

> 1. **JSON For You**: <https://json4u.cn/>
> 2. **Readdy**: <https://readdy.ai/>
> 3. **APILayer**: <https://apilayer.com/?utm_source=Github&utm_medium=Referral&utm_campaign=Public-apis-repo>
> 4. **Free for Developers**: <https://free-for.dev/#/>
> 5. **System for AI**: <https://microsoft.github.io/AI-System/>
> 6. **Prompt Optimizer**: <https://prompt.always200.com/>
> 7. **Dotfiles**: <https://dotfiles.github.io/>
> 8. **Hacker News**: <https://news.ycombinator.com/>
> 9. **NewsNow**: <https://newsnow.busiyi.world/>
> 10. **Inspira UI**: <https://inspira-ui.com/>
> 11. **kkFileView**: <https://kkview.cn/zh-cn/index.html>
> 12. **numpy-ml**: <https://numpy-ml.readthedocs.io/en/latest/>
> 13. **PDFMathTranslate**: <https://pdf2zh.com/>
> 14. **DeepChem**: <https://deepchem.io/>
> 15. **OpenAI Cookbook**: <https://cookbook.openai.com/>
> 16. **Nodezator**: <https://nodezator.com/>
> 17. **Micro**: <https://micro-editor.github.io/>

Continue reading [Papers](Papers.md){:.heading.flip-title}
{:.read-more}
